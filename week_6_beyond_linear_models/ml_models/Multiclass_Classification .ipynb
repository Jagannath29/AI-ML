{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBVz1RS9IB3M"
      },
      "source": [
        "# Multiclass Classification\n",
        "\n",
        "In the previous units, we have discussed  logistic regression, and how the logistic regression model is perfect for the classification task of  discrete instances belonging to two classes.\n",
        "\n",
        "What happens if we need to classify into more than two classes? For instance classification of handwritten digits (MNIST dataset) into 10 classes, classification of Iris dataset into three classes.\n",
        "\n",
        "Let's consider the following cases for classification.\n",
        "<!--\n",
        "|![Binary classification](https://drive.google.com/uc?id=1A2sOZ3TIZ0eQV0QDbjlVTZn2Iify-48n)|![multiclass](https://drive.google.com/uc?id=1T0kDZqp622AJeANHPmAZSYnwevmuhqhp)|\n",
        "|-|-| -->\n",
        "\n",
        "|![Binary classification](https://i.postimg.cc/TPPXxRK4/image.png)|![multiclass](https://drive.google.com/uc?id=1T0kDZqp622AJeANHPmAZSYnwevmuhqhp)|\n",
        "|-|-|\n",
        "\n",
        "<center><figcaption>Figure 1: Binary class and Multiclass</figcaption></center>\n",
        "\n",
        "In the first figure, there are two classes, thus logistic regression that can be used for binary classification. On the other hand, the second figure consists of three classes. How can we find the decision boundaries that effectively divide it into three classes?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f9Y2StbIFF4"
      },
      "source": [
        "## One-vs-All ( One-vs-Rest)\n",
        "\n",
        "The first method that we are going to discuss is called the One-vs-all method (sometimes also referred to as one-vs-rest or OvR). It tries to classify samples into different classes by splitting the original problem into multiple binary classification problems. Then we employ logistic regression for each subset of the original problem. For a particular class, it estimates the probability of whether the sample belongs to the class or not. We make a prediction based on the class that maximizes the prediction.\n",
        "\n",
        "Consider the following classification problem depicted as\n",
        "\n",
        "<div>\n",
        "<center>\n",
        "<!-- <img src='https://drive.google.com/uc?id=1T0kDZqp622AJeANHPmAZSYnwevmuhqhp' width=\"700px\"/> -->\n",
        "<img src='https://i.postimg.cc/2Syhx1j2/image.png' width=\"700px\"/>\n",
        "<figcaption>Figure 2: Multiclass Classification</figcaption>\n",
        "</center>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "We can observe that there are three classes. Let A, B, and C be the classes, then we need to\n",
        "\n",
        "- train a logistic regression classifier $h_w^{(i)}(x) $ for each class $i$ to predict the probability that $y=i$.\n",
        "\n",
        "- On a new input $x$, to make a prediction, pick the class $i$ that maximizes\n",
        "$$\n",
        "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
        " \\max_i h_w^{(i)}(x) $$\n",
        "\n",
        "Let's split the problem into three binary classification problems.\n",
        "For class A,\n",
        "<div>\n",
        "<center>\n",
        "<!-- <img src='https://drive.google.com/uc?id=1RBY_alSnriTd76ZZTSaRkAj_obW7bJtI' width=\"700px\"/> -->\n",
        "<img src='https://i.postimg.cc/2Syhx1j2/image.png' width=\"700px\"/>\n",
        "<figcaption> Figure 3: Class A vs Not Class A </figcaption>\n",
        "</center>\n",
        "</div>\n",
        "\n",
        "The probability of an instance that belongs to class A is given as $$\\mathcal{P}(A|x) = h_w^{(1)}(x) $$\n",
        "\n",
        "Similarly for class B,\n",
        "<div>\n",
        "<center>\n",
        "<!-- <img src='https://drive.google.com/uc?id=1t7FFHEBfwQlZZdetr-8BrKbLrAo8AjNH' width=\"700px\"/> -->\n",
        "<img src=\"https://i.postimg.cc/LsyvdSPv/image.png\" width= \"700px\"/>\n",
        "<figcaption> Figure 4: Class B vs Not Class B </figcaption>\n",
        "</center>\n",
        "</div>\n",
        "\n",
        "The probability of an instance that belongs to class B is given as $$\\mathcal{P}(B|x) = h_w^{(2)}(x) $$\n",
        "\n",
        "And lastly for class C,\n",
        "<div>\n",
        "<center>\n",
        "<!-- <img src='https://drive.google.com/uc?id=1SFlD6aro4g-oTWRUQ8iA0l8A0O0gw7G0' width=\"700px\"/> -->\n",
        "<img src=\"https://i.postimg.cc/1tfk2J8n/image.png\" width=\"700px\"/>\n",
        "<figcaption> Figure 5: Class C vs Not Class C </figcaption>\n",
        "</center>\n",
        "</div>\n",
        "\n",
        "The probability of an instance that belongs to class B is given as $$\\mathcal{P}(A|x) = h_w^{(3)}(x) $$\n",
        "\n",
        "Now for any new input $x$, we will evaluate all three probabilities and choose the class that gives the maximum probability for that particular input.\n",
        "\n",
        "$$\n",
        "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
        " \\max_i h_w^{(i)}(x) $$\n",
        "\n",
        "\n",
        "<div>\n",
        "<center>\n",
        "<!-- <img src='https://drive.google.com/uc?id=1wWDELsZFfyEdDk8bT0Wzk5A9CjAwFlMZ' width=\"700px\"/> -->\n",
        "<img src=\"https://i.postimg.cc/fRRB8ktk/image.png\" width=\"700px\"/>\n",
        "<figcaption> Figure 6: Combined Classifiers  </figcaption>\n",
        "</center>\n",
        "</div>\n",
        "\n",
        "The issue with this method is that even if the training examples have an even class distribution, the binary classifiers will see an unbalanced distribution as the set of negative examples are much more than the set of positive examples for any particular class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obaB6JssIK8e"
      },
      "source": [
        "## One-vs-One (OvO)\n",
        "\n",
        "Another strategy for multinomial classification using logistic regression is to compute the binary classification between each of the available classes. Thus if the problem has $N$ classes, we will require to train $$\\frac{N(N-1)}{N}$$ binary classifiers. This problem is similar to the total handshakes in a room or total matches in a group of football teams.\n",
        "\n",
        "During prediction, for an unseen sample all $N(N-1)/2$ classifiers are applied and a voting scheme will be used. The class that got the highest number of positive predictions (votes) will be predicted by the combined classifier.\n",
        "\n",
        "This strategy fails when there are equal numbers of votes for more than one class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqiqH7T9IOMQ"
      },
      "source": [
        "## Softmax Function\n",
        "\n",
        "The Softmax regression, also known as the multinomial logistic regression, is a direct approach to classifying a given sample among more than two classes without the need to use multiple binary classifiers unlike One-vs-rest or one-vs-one methods.\n",
        "\n",
        "For each class $k$, the Softmax regression model computes a score $s_k(\\boldsymbol x)$ for every given instance of $\\boldsymbol x$. Then softmax function (also called the _normalized exponential_) is applied to the scores to estimate the probability of each class. We compute the scores using the following expression:\n",
        "$$ s_k(\\boldsymbol x) = \\boldsymbol w^T_k \\boldsymbol x $$\n",
        "\n",
        "$\\boldsymbol w^{(k)}$ is a parameter vector of each class. All these vectors are stored as rows in a parameter matrix $\\boldsymbol W$.\n",
        "\n",
        "The estimated probability $\\mathcal{\\hat{P}}_k$ that an instance $\\boldsymbol x$ belongs to class $k$ given the scores of every class for that instance is computed as\n",
        "\n",
        "$$ \\mathcal{\\hat{P}}_k = softmax(\\boldsymbol s(\\boldsymbol x))_k = \\frac{\\exp(s_k(\\boldsymbol x))} {\\sum_{i=1}^K \\exp(s_j(\\boldsymbol x))} \\tag{Equation 2}\n",
        "$$\n",
        "\n",
        "\n",
        "Equation 2 is known as the softmax function, here\n",
        "- $K$ is the number of classes\n",
        "- $\\boldsymbol s(\\boldsymbol x)$ is a vector containing the scores of each class for the instance $\\boldsymbol x$\n",
        "- the exponent maps each score to a positive interval as the score might be negative.\n",
        "- the denominator term normalizes the score, such that the sum of all probability equals 1.  \n",
        "\n",
        "Similar to the binary classifier, the multinomial regression classifier predicts the class with the highest estimated probability (class with the highest score).\n",
        "\n",
        "$$\n",
        "\\DeclareMathOperator*{\\argmax}{argmax}\n",
        "\\hat{y} = \\argmax_k  (softmax(\\boldsymbol s(\\boldsymbol x))_k ) = \\argmax_k s_k(\\boldsymbol x) = \\argmax_k \\left(\\boldsymbol w_k^T \\boldsymbol x \\right) \\tag{Equation 3}\n",
        "$$\n",
        "\n",
        "The  $argmax$ operator in Equation 3 returns the value of $k$ that maximizes the estimated probability $softmax(\\boldsymbol s(\\boldsymbol x))_k $.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvL15_v2IRJz"
      },
      "source": [
        "# Cost Function\n",
        "\n",
        "Our training objective for the softmax regression model is to estimate a high probability for the target class while simultaneously estimating low probabilities for the remaining $K-1$ classes. We need a cost function such that there is a low penalty for correct prediction and a high penalty for the  wrong prediction. Since the premise is similar to the binary logistic regression, we can generalize the cost function for the binary classifier from 2 to $K$ classes.\n",
        "\n",
        "The binary cross-entropy gives the cost function for the binary logistic regression and has the following form:\n",
        "\n",
        "\n",
        "$$ \\text{Cost}(\\hat{y}, y) = -y\\log(\\hat{y}) - (1-y)\\log(1-\\hat{y}) \\tag{Equation 4}$$\n",
        "\n",
        "We can generalize the two terms in Equation 4 to $K$ terms. In multinomial regression, we represent both the true label ($\\boldsymbol y$) and predicted label ($ \\hat{\\boldsymbol y}$) as vectors with $K$ elements. The true label $\\boldsymbol y$ is a one-hot vector of length $K$ with $y_c=1$ for correct class $c$ and the remaining elements being 0. Whereas ($ \\hat{\\boldsymbol y}$) is the estimated vector with $K$ elements, with each element ($ \\hat{\\boldsymbol y}_k$) representing probability estimates  given by our classifier.\n",
        "\n",
        "The generalized version of binary cross-entropy, binary logistic regression cost function, for multiple classes is known as **cross-entropy**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izusjtIkIjvI"
      },
      "source": [
        "## Cross Entropy\n",
        "The cost function for multinomial logistic regression, Cross entropy for a single example $\\boldsymbol x$ is expressed as:\n",
        "$$\n",
        " \\text{Cost}(\\hat{\\boldsymbol y}, \\boldsymbol y) = - \\sum_{k=1}^K \\boldsymbol y_k \\log (\\hat{\\boldsymbol y}_k) \\tag{Equation 5}\n",
        "$$\n",
        "\n",
        "Equation 5  is the sum of logs of the $K$ output classes, each weighted by their probability $\\boldsymbol y_k$. Since $\\boldsymbol y_k$ is one-hot vector with $y_c = 1$ for correct class $c$ and the remaining elements being $0$.\n",
        "The cost function will result in the negative loss probability of the correct class $c$.\n",
        "\\begin{align*}\n",
        "  \\text{Cost}(\\hat{\\boldsymbol y}, \\boldsymbol y) &= - \\sum_{k=1}^K \\boldsymbol y_k \\log (\\hat{\\boldsymbol y}_k) \\\\\n",
        "  &= -\\log(\\hat{\\boldsymbol y}_c) \\tag{where $c$ is the correct class}\n",
        "\\end{align*}\n",
        "\n",
        "The cost function for $m$ examples will then be given as:\n",
        "$$\n",
        "\\mathcal{J}(\\boldsymbol W) = -\\frac{1}{m}\\sum_{i=1}^m \\sum_{k=1}^K \\boldsymbol y_k^{(i)} \\log \\left(\\hat{\\boldsymbol y}_k^{(i)}\\right) \\tag{Equation 6}\n",
        "$$\n",
        "\n",
        "Here, $y_k^{(i)}$ represents the labeled one-hot probability vector that the $i^{th}$ example belongs to class $k$.\n",
        "\n",
        "When there are only two classes ($K=2$), Equation 6 is equivalent to the binary cross-entropy.\n",
        "\n",
        "More details on [Cross Entropy](https://youtu.be/ErfnhcEV1O8)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT6A53SZIviG"
      },
      "source": [
        "# Optimizing Parameters using Gradient Descent\n",
        "\n",
        "Cross-entropy is a convex function so taking the gradient descent on the cost function will find the parameter matrix $\\boldsymbol W$ that minimizes the cost function.\n",
        "\n",
        "Computing the gradient vector for each class of Equation 6 with regard to $\\boldsymbol w^{(k)}$, we get\n",
        "(_Refer to previous unit notebook for the computation of partial derivative for cross-entropy._)\n",
        "$$\n",
        "\\nabla_{\\boldsymbol w^{(k)}} \\mathcal{J}(\\boldsymbol W) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{\\boldsymbol y}_k^{(i)} - \\boldsymbol y_k^{(i)} \\right) \\boldsymbol x^{(i)} \\tag{Equation 7}\n",
        "$$\n",
        "\n",
        "Equation 7 shows that the gradient vector for every class is the difference between the true value (one-hot vector) and the probability the classifier outputs for that class, weighted by the value of the input $x$ corresponding to the $i^{th}$ element of the weight vector for that class.\n",
        "\n",
        "Now we can use the following update equation for  Gradient descent to optimize the $w$ parameters\n",
        "\n",
        "$$\n",
        " \\boldsymbol w_{j+1} = \\boldsymbol w_j - \\alpha  \\nabla_{\\boldsymbol w^{(k)}} \\mathcal{J}(\\boldsymbol W)\\tag{Equation 8}\n",
        "$$\n",
        "\n",
        "The amount of the movement in the gradient descent is given by the slope of the cost function weighted by a learning rate $\\alpha$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lensCJAJUwbX"
      },
      "source": [
        "# Key Takeaways\n",
        "- Binary classifiers can be used for multiclass classification in one-vs-rest and one-vs-one methods.\n",
        "- Softmax function can estimate probabilities for multiclass problem.\n",
        "- Cross entropy is a generalized binary cross-entropy cost function for the multi-class classification.\n",
        "- The gradient vector for every class is the difference between the true value and the probability the classifier outputs for that class, weighted by the value of the input $x$ corresponding to the $i^{th}$ element of the weight vector for that class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
