{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "873def5d",
   "metadata": {
    "id": "873def5d"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "To start with Decision Tree, you must have an understanding of:\n",
    "- Probability\n",
    "\n",
    "- Differences between classification and regression\n",
    "- Evaluation metrics for classification and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab9ae60",
   "metadata": {
    "id": "dab9ae60"
   },
   "source": [
    "# Decision Tree\n",
    "\n",
    "A **Decision Tree** is a supervised machine learning model used for both **classification** and **regression tasks**. It works by learning a set of decision rules from data and organizing them into a **tree-like structure**.\n",
    "\n",
    "Think of a decision tree like a flowchart of yes/no questions you ask to make a decision.\n",
    "- Each if-else is like a branch of the tree.\n",
    "\n",
    "- The final True/False at the bottom is the leaf node (the decision)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9fc891",
   "metadata": {
    "id": "ba9fc891"
   },
   "source": [
    "## Intuition with Breast Cancer Classification Example\n",
    "\n",
    "Let's take a look at a medical example illustrating if **Breast Cancer** is present or not by predicting whether a **breast mass** is **benign** (not cancerous) or **malignant** (cancerous).\n",
    "\n",
    "<div align=\"center\">\n",
    "    <figure>\n",
    "     <!-- <img src=\"https://doc.google.com/a/fusemachines.com/uc?id=1_Y_Y8eG5epFOsrvoRJlZy56EGxCCvuvg\"> -->\n",
    "     <img src = \"https://i.postimg.cc/kMfTP7vK/Breast-Cancer-Classification.png\" width=80%>\n",
    "     <figcaption>Figure: Breast Cancer Classification with Decision Tree </figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "- **Root Node:** The top-most node; the starting point.\n",
    "\n",
    "- **Internal Node:** A decision point based on a feature.\n",
    "\n",
    "- **Leaf Node:** The terminal node that gives a prediction.\n",
    "\n",
    "- **Branch:** A connection between nodes that represents a decision path.\n",
    "\n",
    "- **Depth:** The number of layers from root to leaf.\n",
    "\n",
    "</br>\n",
    "\n",
    "We classify a breast mass as **benign** or **malignant** following these steps.\n",
    "\n",
    "**Step 1: Test → Concave points_mean <= 0.051**\n",
    "\n",
    "* If `concave points_mean` is **less than or equal to 0.051**: Go **left**.\n",
    "    * Proceed to next test (on left).\n",
    "* If `concave points_mean` is **greater than 0.051**: Go **right**.\n",
    "    * Proceed to next test (on right).\n",
    "\n",
    "**Step 2a (Left Branch): Test → radius_mean <= 14.98**\n",
    "\n",
    "* If `radius_mean` is **less than or equal to 14.98**: Go **left**.\n",
    "    * **Predict -> benign**.\n",
    "* If `radius_mean` is **greater than 14.98**: Go **right**.\n",
    "    * **Predict -> malignant**.\n",
    "\n",
    "**Step 2b (Right Branch): Test → radius_mean <= 11.345**\n",
    "\n",
    "* If `radius_mean` is **less than or equal to 11.345**: Go **left**.\n",
    "    * **Predict -> benign**.\n",
    "* If `radius_mean` is **greater than 11.345**: Go **right**.\n",
    "    * **Predict -> malignant**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bh7nviBumAZw",
   "metadata": {
    "id": "bh7nviBumAZw"
   },
   "source": [
    "## Popular Decision Tree Alorithms\n",
    "\n",
    "* **ID3 (Iterative Dichotomiser 3):** This algorithm leverages the concepts of **entropy** and **information gain** to determine the best splits. It is particularly well-suited for datasets with **categorical features**.\n",
    "\n",
    "* **C4.5:** As a successor to ID3, C4.5 extends its capabilities by supporting **continuous attributes** and effectively handling **missing values** in the data.\n",
    "\n",
    "* **CART (Classification and Regression Trees):** This versatile algorithm, notably implemented in the popular `scikit-learn` library, can be used for both **classification** and **regression** tasks. For classification, it commonly employs the **Gini impurity** criterion, while for regression, it typically uses **Mean Squared Error (MSE)** to guide the splitting process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c764aa",
   "metadata": {
    "id": "11c764aa"
   },
   "source": [
    "## Impurity Metrics and Selection of the Best Attribute\n",
    "\n",
    "The decision tree we just examined used two attributes: **Concave points_mean** and **radius_mean** as test/split functions.\n",
    "\n",
    "In real life, we may not always be lucky to have such a clear-cut set of attributes for every decision. Most real datasets have multiple attributes, thus leading to confusion about which attribute to choose as a test/split function at each node. So, to identify the **best attribute for splitting** at each node, we use metrics like:\n",
    "- **Gini impurity**,\n",
    "- **Entropy**, and\n",
    "- **Mean Squared Error (MSE)**.\n",
    "\n",
    "These metrics are called **impurity metrics**.\n",
    "\n",
    "Let us try to understand the impurity metrics and how they help to determine which attribute to use to split by using a toy classification problem with three classes and two features shown below.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <figure>\n",
    "        <!-- <img src=\"https://doc.google.com/a/fusemachines.com/uc?id=1fEPv2lkdw6azEvjW7AS-saHZwRaBdBgO\"> -->\n",
    "        <img src=\"https://i.postimg.cc/vTxZpmvY/image.png\">\n",
    "        <figcaption> <br> Figure 2: Test attributes and their associated gini values.  a) Dataset at the parent node before split. b) After a vertical split. c) After a horizontal split.</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "__Data Before Split__\n",
    "\n",
    "* The **leftmost figure** shows a scatter plot and class histogram of training samples in a **parent node**.\n",
    "* Goal: Select the attribute ($x₀$ or $x₁$) that best separates the classes.\n",
    "\n",
    "__Splitting by $x₀$ (Vertical Split)__\n",
    "\n",
    "* Creates two sets:\n",
    "\n",
    "  * **Right set**: contains samples from **one class only** → **pure**.\n",
    "  * **Left set**: contains samples from **two classes** → **impure**.\n",
    "\n",
    "__Splitting by $x₁$ (Horizontal Split)__\n",
    "\n",
    "* Creates:\n",
    "\n",
    "  * **Top set**: samples from **one class only** → **pure**.\n",
    "  * **Bottom set**: samples from **all three classes** → **highly impure**.\n",
    "\n",
    "### Gini Impurity\n",
    "\n",
    "* Shown at the top of the split plots.\n",
    "\n",
    "* **Lower Gini value = better split**.\n",
    "* In this case, **$x₀$ (vertical split)** has **lower impurity** than $x₁$, so it's selected for the split.\n",
    "\n",
    "Let's go back to our **Breast Cancer classification** example. Can you verify that using the **radius_mean** test in the root node would have reduced the uncertainty in prediction better than the attribute **Concave points_mean**? If one attribute is sufficient to make a good split, the decision tree above might have only one node! Such a tree with a single node is called a **decision stump**.\n",
    "\n",
    "Now we know about the impurity metrics, let's delve into them. Throughout this reading material, we will be talking about the gini impurity. Mathematically, gini is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{Gini} = 1 - \\sum_{i=1}^{n} P_{i}^2\n",
    "\\end{equation}\n",
    "\n",
    "Where,$P_i$ is the probability of an object being classified to a class $i$.\n",
    "\n",
    "Now, let's write our function to compute the gini impurity using `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74656385",
   "metadata": {
    "id": "74656385"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_gini(class_frequencies):\n",
    "    probabilities = class_frequencies / np.sum(class_frequencies)\n",
    "    gini = 1 - np.sum(probabilities ** 2)\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ca272",
   "metadata": {
    "id": "619ca272"
   },
   "source": [
    "Let's suppose that we have four different samples of data for the **Breast classification** example above. We will use the function `compute_gini()` to compute the gini impurities.\n",
    "\n",
    "<!-- https://drive.google.com/open?id=1mcrYoGeTUz7ToLp03iHzUCN11i2k5oZt -->\n",
    "\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Benign</th>\n",
    "      <th>Malignant</th>\n",
    "      <th>Remarks</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>150</td>\n",
    "      <td>0</td>\n",
    "      <td>Pure Data</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>10</td>\n",
    "      <td>90</td>\n",
    "      <td>Unevenly Distributed Impure Data</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>60</td>\n",
    "      <td>40</td>\n",
    "      <td>Unevenly Distributed Impure Data</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>50</td>\n",
    "      <td>50</td>\n",
    "      <td>Evenly Distributed Impure Data</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a33ada",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1748351019558,
     "user": {
      "displayName": "Shashin Maharjan",
      "userId": "18244015372746364813"
     },
     "user_tz": -345
    },
    "id": "80a33ada",
    "outputId": "c4b8d37a-1e49-40c5-bb01-ab40ea70e1f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00\n",
      "0.18\n",
      "0.48\n",
      "0.50\n"
     ]
    }
   ],
   "source": [
    "# Computing gini impurities for different proportions of data shown in the table above\n",
    "class_dist_matrix = np.array([\n",
    "    [150, 0],\n",
    "    [10, 90],\n",
    "    [60, 40],\n",
    "    [50, 50]\n",
    "])\n",
    "\n",
    "for num_cases in range(len(class_dist_matrix)):\n",
    "    class_dist = class_dist_matrix[num_cases,:]\n",
    "    gini_impurity = compute_gini(class_dist)\n",
    "    print(\"{:.2f}\".format(gini_impurity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca1405",
   "metadata": {
    "id": "dfca1405"
   },
   "source": [
    "From the computations above, we can see that a pure set (samples from one class only) has zero impurity.\n",
    "\n",
    "Impurity (uncertainty of prediction) becomes maximum when a set has an equal number of samples from each class. Just like gini and entropy, MSE also measures impurity in data. It is used in regression trees. Attribute that yields split with lower MSE are preferred over a split with higher MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6e8f46",
   "metadata": {
    "id": "4b6e8f46"
   },
   "source": [
    "\n",
    "### Decision Tree Theory\n",
    "* Pang-Ning Tan, Michael Steinbach, Anuj Karpatne, Vipin Kumar, [Introduction to Data Mining](https://www-users.cs.umn.edu/~kumar001/dmbook/index.php), 2nd Edition\n",
    "   * Check unit 3.3.1, 3.3.2, and 3.3.3 page 119 to understand how decision tree work and how to build them\n",
    "\n",
    "\n",
    "* A. Criminisi and J. Shotton, Decision Forests for Computer Vision and Medical Image Analysis\n",
    "   * Check chapter 3 page 7 to get a basic understanding of tree$-$data structure and decision tree."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
