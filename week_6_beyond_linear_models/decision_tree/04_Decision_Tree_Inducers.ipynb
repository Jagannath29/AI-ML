{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJ4IEG_cWNQm"
   },
   "source": [
    "# Decision Tree Inducers\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6hk9GHrM8yy"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before proceeding through this notebook, students should have completed\n",
    "* Decision Tree basics \n",
    "* Impurity Metrics\n",
    "\n",
    "## Learning Objectives\n",
    "After going through this notebook, students should be able to:  \n",
    "* List popular decision tree inducers.\n",
    "* List differences between ID3, C4.5, and CART.  \n",
    "* Use algorithms discussed in this chapter and python to create decision tree from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLdzO3lxM4iE"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYMAqvBWM5-Z"
   },
   "source": [
    "\n",
    "In previous notebooks we studied about impurity metrics and pruning methods. In this notebook we will see how different impurity metrics and pruning methods are used by decision tree inducers. \n",
    "\n",
    "Decision Tree inducers are algorithms used to create decision tree from the training data. Some popular decision tree inducers are:  \n",
    "\n",
    "* ID3\n",
    "\n",
    "* C4.5\n",
    "* CART  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_hSTdkeYCwk"
   },
   "source": [
    "## ID3\n",
    "\n",
    "ID3 stands for Iterative Dichotomiser 3. 'Iterate' means to perform repeatedly, and 'dichotomize' means to represent as divided. ID3 repeatedly divides data at each node and hence the name. It was developed by Ross Quinlan and is used to solve classification problems. It creates multiway splits. For an *n*-valued categorical attribute, ID3 makes *n* splits. For example, if an attribute **outlook** has three values *sunny, foggy, and rainy*, then ID3 makes three splits, one for each distinct value of **outlook**. \n",
    "\n",
    "ID3 uses **information gain to determine the best split**. The tree growing process stops when all instances are assigned a class label or when maximum information gain is not greater than 0. ID3 **doesn't use any tree pruning method**.\n",
    "\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. For all instances in a dataset $D$:\n",
    "\n",
    "   * If all instances belong to the same class $C$, or stopping criteria are met (e.g., pure instances, maximum tree depth):\n",
    "\n",
    "     * Create a **leaf node** labeled with class $C$ and stop.\n",
    "   * Else:\n",
    "\n",
    "     * Compute **Information Gain** for all previously unselected attributes.\n",
    "     * Select an attribute (say $f$) that yields the **maximum Information Gain**.\n",
    "     * If $\\max(\\text{Information Gain}) \\leq 0$:\n",
    "\n",
    "       * Create a **leaf node** with the **majority class label** and stop.\n",
    "     * Else:\n",
    "\n",
    "       * Split the dataset into subsets according to the value of attribute $f$.\n",
    "\n",
    "2. Apply the algorithm **recursively** from Step 1 for each of the subsets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Disadvantage of ID3\n",
    "- ID3 is prone to overfitting because it doesn't use pruning method. \n",
    "- Another is that it can only handle categorical attributes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhDg4XIhYC3V"
   },
   "source": [
    "## C4.5\n",
    "C4.5, also developed by Ross Quinlan, is an improved version of ID3. Like ID3, C4.5 creates multiway splits for a categorical attribute and is used to solve classification problems. \n",
    "\n",
    "Unlike ID3, C4.5 removed the restriction that attributes must be categorical. For continuous attributes, C4.5 creates a threshold and then splits the data into two parts. The first part contains all instances for which the attribute value is less than or equal to the threshold. The second part contains all instances for which the attribute value is greater than the threshold. It uses **gain ratio as splitting** criteria. The tree growing process stops when the number of instances to be split is below a certain threshold. For pruning, C4.5 **uses Error-based pruning** to solve the overfitting problem. Our previous notebook on pruning methods doesn't discuss error-based pruning, but you may refer to the additional resources section if you want to know more about error-based pruning.\n",
    "\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "<!-- ```\n",
    "1. For all instances in a dataset:\n",
    "        i. If all instances in D belong to the same class, C or other stopping criteria are met:\n",
    "            a. Create a leaf node and stop.\n",
    "        ii. Else\n",
    "            a. Compute gain ratio for all attributes.\n",
    "            b. Select an attribute(say f) that yields the greatest gain ratio.\n",
    "            c. Split the data into subsets according to value of attribute f.\n",
    "2. Apply the algorithm recursively from step-1 for each of the subset.\n",
    "3. Prune tree using error based pruning.\n",
    "``` -->\n",
    "\n",
    "1. For all instances in a dataset $D$:\n",
    "\n",
    "   * If all instances belong to the same class $C$, or other stopping criteria are met (e.g., pure instances, maximum tree depth):\n",
    "\n",
    "     * Create a **leaf node** labeled with class $C$ and **stop**.\n",
    "   * Else:\n",
    "\n",
    "     * Compute the **Gain Ratio** for all unused attributes.\n",
    "     * Select the attribute $f$ with the **highest Gain Ratio**.\n",
    "     * Split the dataset into subsets based on the values of attribute $f$.\n",
    "\n",
    "2. Apply the algorithm **recursively** from Step 1 to each subset.\n",
    "\n",
    "3. After the full tree is constructed, **prune the tree using Error-Based Pruning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUyIICf0i04k"
   },
   "source": [
    "## CART\n",
    "\n",
    "CART stands for **Classification and Regression Trees**. CART constructs binary trees. Each  split generates maximum of two subsets. For example, if an attribute **outlook** has 3 values *sunny, foggy, and rainy* then one subset may contain all instances in which **outlook** has value *foggy*. The other subset may contain instances in which **outlook** has value *sunny* and *rainy*.  The splits are selected using **information gain** as the impurity metric and the tree is pruned using **cost-complexity pruning**.  \n",
    "\n",
    "An advantage of CART over ID3 and C4.5 is that it can be used for both regression and classification.\n",
    "\n",
    "\n",
    "\n",
    "### CART Algorithm (Classification)\n",
    "\n",
    "1. For all instances in a dataset $D$:\n",
    "\n",
    "   * If all instances belong to the same class $C$, or other stopping criteria are met (e.g., pure instances, maximum tree depth):\n",
    "\n",
    "     * Create a **leaf node** and **stop**.\n",
    "   * Else:\n",
    "\n",
    "     * Compute **Information Gain** for all attributes.\n",
    "     * Select the attribute $f$ that yields the **greatest Information Gain**.\n",
    "     * Split the data into subsets according to the value of attribute $f$.\n",
    "\n",
    "2. Apply the algorithm **recursively** from Step 1 to each subset.\n",
    "\n",
    "3. **Prune the tree** using **Cost Complexity Pruning**.\n",
    "\n",
    "> The algorithm uses **Information Gain** as the impurity metric, but you could also use **Twoing Criterion** (refer to the *Additional Resources* section for more details).\n",
    "\n",
    "\n",
    "### CART Algorithm (Regression)\n",
    "\n",
    "The above algorithm describes how to construct a decision tree for a **classification problem** using CART. Since **CART supports both classification and regression**, here are the steps for constructing **regression trees**:\n",
    "\n",
    "1. For all instances in a dataset $D$:\n",
    "\n",
    "   * If all target values are approximately the same, or other stopping criteria are met (e.g., Mean Squared Error below a threshold):\n",
    "\n",
    "     * Create a **leaf node** and **stop**.\n",
    "   * Else:\n",
    "\n",
    "     * Compute the **Mean Squared Error (MSE)** for all attributes.\n",
    "     * Select the attribute $f$ that yields the **least MSE**.\n",
    "     * Split the data into subsets according to the value of attribute $f$.\n",
    "\n",
    "2. Apply the algorithm **recursively** from Step 1 to each subset.\n",
    "\n",
    "3. **Prune the tree** using **Cost Complexity Pruning**.\n",
    "\n",
    "> The algorithm uses **MSE** as the impurity metric, but **Variance** or **Residual Sum of Squares (RSS)** can also be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W9CBj_zz_51"
   },
   "source": [
    "Now that we have studied decision tree inducers, we can conclude that among ID3, C4.5, and CART the easiest to implement is ID3. However, since CART can be used for both regression and classification, it is the most versatile. Due to this reason CART is used by `Scikit-learn` to create `DecisionTreeClassifier` and `DecisionTreeRegressor`.   \n",
    "\n",
    "The following table summarizes above algorithms:  \n",
    "<!-- https://drive.google.com/file/d/1jPFzaYgTYyfNvYF23yYBlDTY6G0h0oLz/view?usp=sharing -->\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    " <figure>\n",
    " <!-- <img src=\"https://doc.google.com/a/fusemachines.com/uc?id=1jPFzaYgTYyfNvYF23yYBlDTY6G0h0oLz\"> -->\n",
    " <img src=\"https://i.postimg.cc/NMHSVX69/image.png\">\n",
    " \n",
    " <!-- <figcaption>Figure 1: Impurity v/s probability of class A for a binary classification(class A and class B) problem.</figcaption> -->\n",
    " </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gChPt0bmi078"
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "### Decision Tree Inducers:\n",
    "* Lior Rokach, and Oded Maimon, [Decision Trees](http://www.ise.bgu.ac.il/faculty/liorr/hbchap9.pdf)\n",
    "  * See Section 8 page 181 to read about Decision Tree Inducers\n",
    "\n",
    "\n",
    "### Pruning Method:\n",
    "* Lior Rokach, and Oded Maimon, [Decision Trees](http://www.ise.bgu.ac.il/faculty/liorr/hbchap9.pdf)\n",
    "  * See Section 3.11 page 172 to read about Twoing Criteria\n",
    "  * See Section 6.4 page 176 to read about Minimum Error Pruning\n",
    "\n",
    "* John Mingers, [An Empirical Comparison of Pruning Methods for Decision Tree Induction](https://link.springer.com/article/10.1023/A:1022604100933)  \n",
    "  * See section 2.2.3 to read about Minimum Error Pruning\n",
    "  * See section 2.2.4 to read about Reduced Error Pruning\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Read 1:5.1 Decision Tree Inducers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
