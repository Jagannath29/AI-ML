{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRQ1275DdTj1"
   },
   "source": [
    "## SVM Types\n",
    "1. The main types of SVM we encounter frequently are:\n",
    "* C-SVM or $\\epsilon$-SVM\n",
    "* NuSVM ($\\nu$SVM)\n",
    "\n",
    "### C-SVM or $\\epsilon$-SVM\n",
    "\n",
    "*C-SVM* is named after its regularization parameter $C$ (hyperparameter) that is used. The C parameter must be greater than zero and can extend upto infinity. It is the most commonly used SVM in practice and includes slack vairables to allow misclassifications for non-linearly separable data.\n",
    "\n",
    "**Interpretation of C**\n",
    "- Must be strictly greater than 0\n",
    "- **High ùê∂**: Penalizes misclassification heavily, aims for fewer training errors and has a high risk of overfitting.\n",
    "- **Low ùê∂**: Tolerates more margin violations, leads to a wider margin and potentially better generalization and poses risks of underfitting.\n",
    "\n",
    "### NuSVM or $\\nu$SVM\n",
    "\n",
    "NuSVM offers more intuitive control over the number of support vectors and margin errors. It replaces the parameter C with ŒΩ, which consists a value between 0 and 1.\n",
    "\n",
    "$\\nu$ is interpreted as an upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors.\n",
    "\n",
    "*Benefits of $\\nu$SVM*\n",
    "- Better control over model complexity\n",
    "- Useful when the number of support vectors or training errors needs to be explicitly bounded.\n",
    "- Works well in imbalanced or noisy datasets\n",
    "\n",
    "Although both variants of SVM‚ÄîC-SVM and Nu-SVM‚Äîoften produce similar results, parameter ùúà directly relates to the ratio of support vectors and the ratio of training errors, providing more interpretable and intuitive control over the model's complexity and tolerance for misclassification.\n",
    "\n",
    "If you wish to learn more on NuSVM, please consult [this link](http://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/pdf3353.pdf).\n",
    "\n",
    "We will be using the C-SVM variant for demonstrating the applications of SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpkza4qOM0_X"
   },
   "source": [
    "## SVM as Classifier\n",
    "\n",
    "So far we have studied SVM as a model for classification of data in binary context, i.e. only into two categories. However, many real life scenarios may require us to classify the data points into three or more categories. Hence, SVM classifiers are divided into two types:\n",
    "- Binary Classifier\n",
    "- Multiclass Classifier\n",
    "\n",
    "### Binary Classifier\n",
    "\n",
    "Here's a quick recap of what we have learnt so far.\n",
    "\n",
    "The Support Vector Machine classifies the data points either linearly or non-linearly using hyperplanes, Kernels and slack variables. These examples of classifications classify the data only into two categories, which is called binary classification.\n",
    "\n",
    "In binary classification, we simply assign a class as $1$ and another class as $0$. To understand it simply, consider this. For lets say cats and dogs classification, we assign cats $0$ and dogs as $1$.\n",
    "\n",
    "Suppose, $W$ is our learned weight and we are given data $X$ for the prediction then our prediction would be as follows:\n",
    "$$\n",
    "\\begin{cases}1,&  \\text{ if } ~~ w^TX \\geq 0\\\\\n",
    "0,  & \\text{ Otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "### MultiClass Classifier\n",
    "\n",
    "Multi class classification is the method of classifying or identifying objects among a number of different crowds.\n",
    "\n",
    "For example: Classification between cats and dogs is a binary classification problem. Classification between cats, dogs and mice is a multiclass classification problem.\n",
    "\n",
    "There are a number of methods which we can apply for the multi class classification:\n",
    "* One vs. All for multi class classification\n",
    "* One vs. One for multi class classification\n",
    "\n",
    "a. **One vs. All Multi Class Classification:**  \n",
    " One vs All Multi Class Classification or One vs. Rest Multi Class Classification is a multi class classification technique in which we split the multi class dataset into multiple binary classification problem.\n",
    "\n",
    "Consider we have to do classification on three labels: **Cats**, **Dogs** and **Humans**, then we will have three models that does classification as:\n",
    " * Model 1: Cats vs. [Dogs, Humans]  \n",
    " * Model 2: Dogs vs. [Cats, Humans]  \n",
    " * Model 3: Humans vs. [Dogs, Cats]  \n",
    "Here, *Model 1* classifies Cats as positive sample and the rest(Dogs and Humans) as negative. A similar case is seen for model 2 and model 3.  \n",
    "The main idea behind this is that we create one model for each label which is really good at recognisizing only one object. Here the number of model required is equal to the number of classes.\n",
    "\n",
    "b. **One vs. One Multi Class Classification:**  \n",
    "Similar to the \"*One vs. All*\" method we make multiple binary classifier in *One vs. One Multi Class Classification* but the difference is that in the one vs one, the model splits the dataset into one dataset for each class verus every other class.  \n",
    "To make thing clear, consider the case as before. We have to do classification on cats dogs and humans, here we divide into the binary classification as:\n",
    " * Model 1: Cats vs. Dogs\n",
    " * Model 2: Cats vs. Humans\n",
    " * Model 3: Dogs vs. Humans  \n",
    "The formula to calculate the number of binary datasets, and in turn models required for the one vs. one  multi class classification is given as:\n",
    "$$\\frac{Class ~ Number \\times (Class ~ Number - 1)}{2}$$\n",
    "In the above example, we had $3$ classes. Hence:\n",
    "$\\frac{3 \\times (3-1)}{2} = 3$ models and binary datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5TfQGxrLkZ9"
   },
   "source": [
    "## One Class SVM:\n",
    "One class SVM is the most common type of SVM used for anomaly detection. It can be considered as a type of binary classification that classifies data points either into *outliers* or *inliers*.\n",
    "\n",
    "If the prediction from the algorithm is *positive*, the data is normal; if the the prediction is *negative*, then the data is an outlier or anomaly. i.e.  \n",
    "\n",
    "**Positive Case**: *Normal Data*  \n",
    "**Negative Case**: *Outlier/Anomaly Data*\n",
    "\n",
    "In One-Class SVM, the model is trained using only the \"normal\" data, without any class labels. Because it doesn't rely on labeled anomalies, this makes One-Class SVM an unsupervised learning algorithm.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "- The model learns the boundary that encloses the majority of the normal data points in the feature space.\n",
    "- At prediction time, it checks whether a new data point falls inside or outside this learned boundary.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWLhDT8ux6G-"
   },
   "source": [
    "## SVM as a Regression Algorithm:\n",
    "So far we have learnt SVM as a classification model for categorization of datapoints using hyperplanes.\n",
    "In this section, we shall discuss how SVM can be used for regression problem as well (SVR).\n",
    "\n",
    "The principle in suport vector machine regression is same as in support vector machine classifiers. However, instead of predicting class labels, SVR predicts continuous numerical values.\n",
    "\n",
    "For the hyper plane and the margins, our best fit is the hyperplane that has maximum number of points.\n",
    "\n",
    "Given a training vectors $X$ and the result $Y$ ; $x_i \\in X, ~ y_i \\in Y$\n",
    "and $w$ be the learned weight. Then the prediction for  support vector regression can be formulated as:\n",
    "\n",
    "$$\\min_ {w, b} \\frac{1}{2} w^T w + C \\sum_{i=1}\\max(0, |y_i - (w^T \\phi(x_i) + b)| )$$\n",
    " here. $C$ is th regularization parameter\n",
    "and $b$ is bias.\n",
    "\n",
    "Hence, the objective of SVM regression training is to maximize the margin and minimize the error.\n",
    "\n",
    "**Goal of SVR:**\n",
    "- Minimize model complexity (flatness of the function)\n",
    "- Ignore small errors (within ùúñ)\n",
    "- Penalize larger errors, scaled by ùê∂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmN7Abs3Dtgl"
   },
   "source": [
    "## SVM Under the Hood\n",
    "\n",
    "The Support Vector Machine (SVM) models we use in scikit-learn are powered by two popular open-source libraries: *libsvm* and *liblinear*. Each is used under the hood for different SVM variants and optimizes the model using different algorithms.  \n",
    "\n",
    "The support vector machine model(*SVM*) implemented on sklearn uses *libsvm* where as the model *LinearSVC* uses *liblinear*.\n",
    "- SVC is ideal for non-linear classification and supports kernel functions.\n",
    "- LinearSVC is optimized for large-scale linear problems and does not support kernels.\n",
    "\n",
    "At its core, SVM solves an optimization problem to find the best decision boundary that separates classes with the maximum margin. In its dual form, the SVM optimization problem is:\n",
    "$$\\max_\\alpha \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n y_i y_j K(x_i, x_j) \\alpha_i \\alpha_j$$\n",
    "subject to:\n",
    "$$0 \\leq \\alpha_i \\leq C  for i = 1, 2, \\cdots,n$$  \n",
    "$$\\sum _{i=1} ^n y_i \\alpha_i = 0$$\n",
    "\n",
    "where,\n",
    "$Œ±_i$: Lagrange multipliers\n",
    "\n",
    "ùê∂: Regularization parameter (controls soft margin)\n",
    "\n",
    "ùêæ($ùë•_ùëñ$,$ùë•_ùëó$): Kernel function (e.g., linear, polynomial, RBF)\n",
    "\n",
    "For the optimization, *libsvm* uses *Sequential Minimal Optimization (SMO)* whereas *liblinear* uses [Coordinate Descent](https://en.wikipedia.org/wiki/Coordinate_descent) algorithm.\n",
    "\n",
    "<!-- To put things simply, SMO algorithm is dedicated to heuristics for choosing which $\\alpha_i$ and $\\alpha_j$ to\n",
    "optimize so as to maximize the objective function as much as possible. For large data sets, this\n",
    "is critical for the speed of the algorithm, since there are $m(m ‚àí 1)$ possible choices for $\\alpha_i$ and\n",
    "$\\alpha_j$ , and some will result in much less improvement than others.\n",
    "However, for our simplified version of SMO, we employ a much simpler heuristic. We simply\n",
    "iterate over all $\\alpha_i$, $i = 1, \\cdots m$. If $\\alpha_i$ does not fulfill the KKT conditions to within some numerical\n",
    "tolerance, we select $\\alpha_j$ at random from the remaining $m ‚àí 1$ $\\alpha$‚Äôs and attempt to jointly optimize\n",
    "$\\alpha_i$ and $\\alpha_j$ . If none of the $\\alpha$‚Äôs are changed after a few iteration over all the $\\alpha_i$‚Äôs, then the algorithm\n",
    "terminates. It is important to realize that by employing this simplification, the algorithm is no\n",
    "longer guaranteed to converge to the global optimum (since we are not attempting to optimize\n",
    "all possible $\\alpha_i$\n",
    ", $\\alpha_j$ pairs, there exists the possibility that some pair could be optimized which we\n",
    "do not consider) -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsXzsXNgR8a2"
   },
   "source": [
    "## SMO: Sequential Minimal Optimization\n",
    "\n",
    "*Sequential Minimal Optimization* is an iterative algorithm for optimization. The algorithm is used by libsvm and solves this optimization problem efficiently by breaking it down into smaller problems.\n",
    "\n",
    "*Working Algorithm*\n",
    "\n",
    "1. Select one Lagrange multiplier $ùõº_1$ that violates the KKT conditions (i.e., it does not satisfy the optimality constraints).\n",
    "\n",
    "2. Select a second multiplier $ùõº_2$ and solve the subproblem involving only $ùõº_1$ and $ùõº_2$ while keeping others fixed.\n",
    "\n",
    "3. Update the model and repeat the process until all multipliers satisfy the KKT conditions.\n",
    "\n",
    "The algorithm converges when all the Lagrange multipliers satisfy the KKT condition.\n",
    "\n",
    "This pairwise approach avoids complex numerical solvers and converges efficiently, even on large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMbMYJ8g0iyc"
   },
   "source": [
    "## Application of SVM\n",
    "Support Vector Machines (SVMs) are powerful and versatile models used across various domains for both classification and regression tasks due to ability to handle high-dimensional data and non-linear decision boundaries (via kernels).\n",
    "\n",
    "1. **Text Classification & Natural Language Processing (NLP)**: Spam detection (email classification), Sentiment analysis (positive vs negative reviews), Topic classification (news categorization)\n",
    "\n",
    "2. **Image Classification**: Handwritten digit recognition (e.g., MNIST), Face detection and recognition, Object detection\n",
    "\n",
    "3. **Bioinformatics**: Gene expression classification, Protein structure prediction, Disease diagnosis (e.g., cancer detection using microarray data)\n",
    "\n",
    "4. **Financial Applications**: Credit risk assessment, Stock price prediction (in limited, feature-engineered scenarios), Fraud detection\n",
    "\n",
    "5. **Anomaly Detection**: Network intrusion detection, Fraudulent transaction detection, Industrial defect detection\n",
    "\n",
    "6. **Computer Vision in Real-Time Systems**: License plate recognition, Gesture recognition, Medical imaging classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GE4bdsxi3gky"
   },
   "source": [
    "## Advantages and disadvantages of SVM\n",
    "### Advantages:\n",
    " - **Effective in high-dimensional spaces:**\tSVM works well when the number of features is very large, such as in text classification or gene expression data.\n",
    " - **Works well with a clear margin of separation**:\tSVM is ideal when the classes are clearly separable with a large margin.\n",
    " - **Memory efficient**:\tOnly the support vectors are used in the decision function, which reduces the model size.\n",
    " - **Versatile with kernels**:\tThrough the kernel trick, SVM can model complex non-linear decision boundaries without explicitly transforming the data.\n",
    " - **Robust to overfitting (with the right C value)**:\tEspecially in high-dimensional spaces and with appropriate regularization, SVMs generalize well.\n",
    "\n",
    "### Disadvantages\n",
    " - **Not suitable for large datasets**:\tTraining time and memory usage scale poorly with the number of samples ‚Äî especially in non-linear SVMs.\n",
    " - **Less effective when classes overlap**:\tSVM performs poorly when there is significant noise and class overlap, unless carefully tuned.\n",
    " - **Difficult to tune**:\tChoosing the right kernel, regularization parameter ùê∂, and kernel-specific parameters (like ùõæ in RBF) can be complex and time-consuming.\n",
    " - **No direct probabilistic output**:\tUnlike models like logistic regression, SVM doesn‚Äôt naturally provide class probabilities (though they can be approximated).\n",
    " - **Performance degrades with imbalanced data**:\tSVM assumes balanced class distribution; it may bias toward the majority class without adjustment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbAoKubSj_BV"
   },
   "source": [
    "# Tips on Practical Use\n",
    "### 1. Kernel cache size:\n",
    "For SVC, SVR, NuSVC and NuSVR, the size of the kernel cache has a strong impact on run times for larger problems. If you have enough RAM available, it is recommended to set cache_size to a higher value than the default of 200(MB), such as 500(MB) or 1000(MB).\n",
    "\n",
    "### 2. Randomness of the underlying implementations:\n",
    "The underlying implementations of SVC and NuSVC use a random number generator only to shuffle the data for probability estimation (when probability is set to True). This randomness can be controlled with the random_state parameter. If probability is set to False these estimators are not random and random_state has no effect on the results. The underlying OneClassSVM implementation is similar to the ones of SVC and NuSVC. As no probability estimation is provided for OneClassSVM, it is not random.\n",
    "\n",
    "The underlying LinearSVC implementation uses a random number generator to select features when fitting the model with a dual coordinate descent (i.e. when dual is set to True). It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller tol parameter. This randomness can also be controlled with the random_state parameter. When dual is set to False the underlying implementation of LinearSVC is not random and random_state has no effect on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdqXXc6skxJv"
   },
   "source": [
    "3. Parameter nu in NuSVC/OneClassSVM/NuSVR approximates the fraction of training errors and support vectors.\n",
    "\n",
    "4. In SVC, if the data is unbalanced (e.g. many positive and few negative), set class_weight='balanced' and/or try different penalty parameters C.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6b5JikGsCI3P"
   },
   "source": [
    "<!-- ## Implementation of SVM in sklearn\n",
    "\n",
    "We may take sklearn for granted. Sklearn library is what we use to make our prediction for our machine learnign model. For the implementation of the SVM, sklearn uses two library to handle all the computations. they are **libsvm** and **liblinear**. Internally both the library(libsvm and liblinear) uses *C* and *Cython* for the computation. Let's discuss **libsvm** and **liblinear** briefly.\n",
    "\n",
    "**LIBSVM** is a library explicitly designed for the support vecor machine. It is capable fo performing classificationa nd regression task. in addition to that, LIBSVM is also capable of other variants of the svm such as nuSVM, linear svm etc.  \n",
    "The fact that makes LIBSVM so popular and versatile is that it makes the use of **SMO** algorithm -->\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1wYJlMaCq7_XuOhZqiIeAh6QEDglAo80m",
     "timestamp": 1747994017337
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
