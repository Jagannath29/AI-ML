{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x-mV2ZQem9x"
      },
      "source": [
        "# Regression using K-NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfozZVJtem9z"
      },
      "source": [
        "## Learning Objective\n",
        "\n",
        "1. Distinguish working of K-NN regression in comparision to K-NN classifier.\n",
        "2. Build K-NN regression using Numpy module.\n",
        "3. Contrast KNN regression with linear regression model.\n",
        "4. Point out few applications of K-NN regression.\n",
        "5. List the pros and cons of K-NN algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl8rtDPPem90"
      },
      "source": [
        "Previously, you studied K-NN based classifier. This time we are learning K-NN based regression algorithm, comparing it with linear regression model, and finally discussing the pros and cons of K-NN algorithm. Let's get started."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv4GVgWmem91"
      },
      "source": [
        "## K-NN regression\n",
        "\n",
        "K-NN regression's training phase is exactly same as K-NN classifier, where the dataset is stored either directly or using advanced data structure. However, K-NN regression uses __aggregation scheme__ for prediction of real-valued target of unseen sample. After setting the hyperparameters and traing phase, the prediction phase for K-NN regression looks as:\n",
        "\n",
        "- Prediction phase\n",
        "\n",
        "  Given a test sample $\\mathbf{x}_t$,\n",
        "  \n",
        "  1. Compute its \"K\"-nearest neighbors set, $S_{\\mathbf{x}_t}$.\n",
        "     \n",
        "  2. Predict the target value of the test sample, $h(\\mathbf{x}_t)$ by taking the mean of the target values($y''$) present in $S_{\\mathbf{x}_t}$. Mathematically,\n",
        "       \n",
        "     $$\\boxed{h(\\mathbf{x}_t) = \\frac{\\sum_{y'' \\in S_{\\mathbf{x}_t}} y''}{K}}$$\n",
        "     \n",
        "Now, let's build a KNN-regression (brute-force implemenetation) using Numpy module(only Euclidean distance available as distance function)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztNP2iHFem91"
      },
      "source": [
        "## Implementation of K-NN regression\n",
        "\n",
        "Since, most of the functions are similar to K-NN classifier, we are using object-oriented paradigm instead of functional paradigm for shorter implementation of K-NN regression.\n",
        "\n",
        "The main methods of class ```Knnregression``` are:\n",
        "\n",
        "1. ```fit```: Stores the training dataset.\n",
        "\n",
        "Instead of computing euclidean distances between elements using ```for``` loops that is time-consuming, we are defining function,\n",
        "\n",
        "2. ```_calculate_euc_dist_mat```: Computes the Euclidean distance between two feature vectors are return an Euclidean matrix, similar to sklearn's ```metrics.euclidean_distances``` method.\n",
        "\n",
        " Let's briefly discuss about euclidean distance matrix and how to construct it. The Euclidean distance matrix between two feature vectors,\n",
        "\n",
        " $$A = \\begin{pmatrix}\n",
        "a_{11} & a_{12} & \\cdots & a_{1m} \\\\\n",
        "a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "a_{n1} & a_{n2} & \\cdots & a_{nm} \\\\\n",
        "\\end{pmatrix}_{\\text{(nxm)}} \\text{and} \\ B=\\begin{pmatrix}\n",
        "b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
        "b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "b_{z1} & b_{z2} & \\cdots & b_{zm} \\\\\n",
        "\\end{pmatrix}_{\\text{(zxm)}}$$ is given as:\n",
        "\n",
        " $$\n",
        "D = \\begin{pmatrix}\n",
        "d_{11} & d_{12} & \\cdots & d_{1z} \\\\\n",
        "d_{21} & d_{22} & \\cdots & d_{2z} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "d_{n1} & d_{n2} & \\cdots & d_{nz} \\\\\n",
        "\\end{pmatrix}_{\\text{(nxz)}}\n",
        "$$\n",
        "\n",
        " Where,\n",
        " - Feature vector $A$ contains $n$ data samples and $m$ features per sample. Similarly, $B$ contains $z$ data samples with $m$ features per sample.\n",
        "\n",
        " - $a_{ij}, b_{ij} = j^{\\text{th}}$ feature of $i^{\\text{th}}$ sample of feature vectors $A$ and $B$, respectively.\n",
        "\n",
        " - $d_{11}$ represent Euclidean distance between 1st data samples of $A$ and $B$, $d_{12}$ between 1st, 2nd data samples of $A$ and $B$ and so on. Consequently, the Euclidean distance matrix has dimension $n \\times z$.\n",
        "\n",
        " Instead of computing $D$, we will compute  __squared euclidean distance matrix__ ($D^2$) because each element of $D^2$, i.e., $d_{ij}^2$ can be computed as sum of three components that are:\n",
        "\n",
        " 1. Squared L2 norm of $i^{th}$ row of vector $A$, $||a_i||_{2}^2 = \\sum_{k=1}^{m}a_{ik}^2$,\n",
        " 2. Squared L2 norm of $j^{th}$ row of vector $B$$, ||b_j||_2^2=\\sum_{k=1}^{m}b_{jk}^2$, and\n",
        " 3. $-2a_ib_j^T= \\sum_{k=1}^m-2a_{ik}b_{jk}$.\n",
        "\n",
        " The elements of $D^2$:\n",
        "\n",
        " $$\n",
        "D^2 = \\begin{pmatrix}\n",
        "d^2_{11} & d^2_{12} & \\cdots & d^2_{1z} \\\\\n",
        "d^2_{21} & d^2_{22} & \\cdots & d^2_{2z} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "d^2_{n1} & d^2_{n2} & \\cdots & d^2_{nz} \\\\\n",
        "\\end{pmatrix}_{\\text{(nxz)}}\n",
        "$$\n",
        "\n",
        " can be represented as:\n",
        "\n",
        " $$ D^2 = \\begin{pmatrix}\n",
        "||a_1||_{2}^2+||b_1||_2^2 - 2a_1b_1^T& ||a_1||_{2}^2+||b_2||_2^2 - 2a_1b_2^T & \\cdots & ||a_1||_{2}^2+||b_z||_2^2 - 2a_1b_z^T \\\\\n",
        "||a_2||_{2}^2+||b_1||_2^2 - 2a_2b_1^T & ||a_2||_{2}^2+||b_2||_2^2 - 2a_2b_2^T & \\cdots & ||a_2||_{2}^2+||b_z||_2^2 - 2a_2b_z^T \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "||a_n||_{2}^2+||b_1||_2^2 - 2a_nb_1^T & ||a_n||_{2}^2+||b_2||_2^2 - 2a_nb_2^T & \\cdots & ||a_n||_{2}^2+||b_z||_2^2 - 2a_nb_z^T \\\\\n",
        "\\end{pmatrix}_{\\text{(nxz)}}$$\n",
        "\n",
        " In the implementation, we compute $D^2$ by first defining/computing three components and adding them, the components being:\n",
        "\n",
        " 1. Column vector, $||\\mathbf{a}||_2^2 =\\begin{pmatrix}||a_1||_{2}^2 \\\\ ||a_2||_{2}^2 \\\\ \\vdots \\\\ ||a_n||_{2}^2 \\end{pmatrix}_{(n \\times 1)}$\n",
        "\n",
        " 2. Row vector , ${||\\mathbf{b}||_2^2}^T =\\begin{pmatrix}||b_1||_{2}^2 & ||b_2||_{2}^2 & \\cdots & ||b_z||_{2}^2 \\end{pmatrix}_{(1 \\times z)}$\n",
        "\n",
        " 3. Matrix, $W = \\begin{pmatrix}\n",
        "- 2a_1b_1^T&  - 2a_1b_2^T & \\cdots &  - 2a_1b_z^T \\\\\n",
        "- 2a_2b_1^T & - 2a_2b_2^T & \\cdots &  - 2a_2b_z^T \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "- 2a_nb_1^T & - 2a_nb_2^T & \\cdots &  - 2a_nb_z^T \\\\\n",
        "\\end{pmatrix}_{\\text{(nxz)}}$\n",
        "\n",
        " Finally for computing euclidean distance matrix($D$), we take the square root of the sum as:\n",
        "\n",
        " $$\\boxed{D = \\sqrt{D^2} =  \\sqrt{||\\mathbf{a}||_2^2+{||\\mathbf{b}||_2^2}^T+W}}$$\n",
        "\n",
        " To know about the more about the math behind the computing this matrix, follow this blogpost on [Euclidean distance matrix]( https://medium.com/swlh/euclidean-distance-matrix-4c3e1378d87f) by Andrea Grianti.\n",
        "\n",
        "3. ```predict```: computes the predicted target labels for the new unseen samples using aggregation scheme. This method is similar to predict function of K-NN classifier implemented previously.\n",
        "\n",
        "Below is the code for implementation of class ```Knnregression```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QijNO9fMem92"
      },
      "outputs": [],
      "source": [
        "import numpy as np # Importing required module\n",
        "\n",
        "class Knnregression():\n",
        "    \"\"\"\n",
        "\n",
        "    K-NN based regression\n",
        "\n",
        "    This is a K-NN regression built using Numpy module that\n",
        "    only supports numerical data as input and euclidean distance\n",
        "    for computing neighbors.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, k):\n",
        "        '''\n",
        "        Constructs K attribute of K-NN regression\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        k: int\n",
        "          Number of neighbors to take into account in K-NN\n",
        "        '''\n",
        "\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        '''\n",
        "        Trains the K-NN model, i.e. storing the training dataset\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_train: Numpy.array, shape(n,m)\n",
        "                Training feature matrix\n",
        "        y_train: Numpy.array, shape(n,)\n",
        "                Training target values\n",
        "        '''\n",
        "\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def _calculate_euc_dist_mat(self, X_test):\n",
        "        '''\n",
        "        Computes the euclidean distance matrix between two feature vectors\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_test: Numpy.array, shape(z,m)\n",
        "            New feature matrix, this feature vector and stored one's euclidean\n",
        "            distance is computed.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        euc_mat: Numpy.array, shape(n,z)\n",
        "            Euclidean distance matrix, here element (1,1) is euclidean distance\n",
        "            between stored data's 1st sample and new data's 1st sample, (1,2)\n",
        "            between stored data's 1st sample and new data's 2nd sample and so on.\n",
        "        '''\n",
        "        a = np.sum(self.X_train**2, axis=1).reshape(-1,1) # Reshaping for proper euclidean matrix.\n",
        "        b_T = np.sum(X_test**2, axis=1)\n",
        "        W = -2 * np.dot(self.X_train,X_test.T)\n",
        "        euc_mat = np.sqrt(a + b_T + W + 1e-10) # Adding small value to avoid warning.\n",
        "\n",
        "        return euc_mat\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        '''\n",
        "        Predicts the target labels of provided data\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_test: Numpy.array, shape(z,m)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.array(self.predictions): Numpy,array, shape(z,)\n",
        "                Predictions of the target labels\n",
        "        '''\n",
        "\n",
        "        self.predictions = []\n",
        "\n",
        "        dist_mat = self._calculate_euc_dist_mat(X_test)\n",
        "\n",
        "        for i in range(X_test.shape[0]):\n",
        "            distance = dist_mat[:,i] # Taking ith column of distance matrix\n",
        "            near_neigh_index = np.argsort(distance)[:self.k]\n",
        "            near_neigh_labels = self.y_train[near_neigh_index]\n",
        "\n",
        "            self.predictions.append(np.mean(near_neigh_labels)) # Aggregation\n",
        "\n",
        "        return np.array(self.predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqzduEKQem96"
      },
      "source": [
        "Now, let's test the performance of our K-NN regression model on _Boston house price prediction_ dataset available in Scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbIGbTDYem9-"
      },
      "source": [
        "### Importing data and feature scaling\n",
        "\n",
        "First, we need to import the the boston house data. Then, continue with feature scaling on the input feature matrix ```X``` and finally split the dataset into train and test set. Feature scaling is mandatory preprocessing step for K-NN based algorithm because of their dependency on distance function. As the unscaled features degrade the performance of K-NN model, we will be using ```MinMaxScaler``` provided by Scikit-learn to limit the values of features between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6lqwCABem9-"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_boston(return_X_y=True)\n",
        "\n",
        "X_scaled = MinMaxScaler().fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y\n",
        "                                                    , test_size=0.2\n",
        "                                                    , random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SYJCTBNem-C"
      },
      "source": [
        "### Fitting the K-NN regression and prediction\n",
        "\n",
        "Like Scikit-learn's estimator, we will create instance of our class ```Knnregression```, fit the training data and finally predict the output target values. Let's take four neighbors into account while predicting the target value,, i.e. set $K=4$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xitgCGhem-C"
      },
      "outputs": [],
      "source": [
        "knn_reg = Knnregression(k=4)\n",
        "knn_reg.fit(X_train, y_train)\n",
        "\n",
        "knn_y_train_pred = knn_reg.predict(X_train)\n",
        "knn_y_test_pred = knn_reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekl9RxoZem-F"
      },
      "source": [
        "### Performance comparision with linear regression\n",
        "\n",
        "Using MSE as the performance metric, let's compare the training and test MSE for both K-NN regression and linear regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JIEe-rtem-G",
        "outputId": "2b7120ca-f430-4003-f760-8f50651f024b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN, Training MSE: 11.91\n",
            "KNN, Test MSE: 19.80\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "knn_train_error = mean_squared_error(y_train, knn_y_train_pred)\n",
        "knn_test_error = mean_squared_error(y_test, knn_y_test_pred)\n",
        "\n",
        "print(f'KNN, Training MSE: {knn_train_error:.2f}')\n",
        "print(f'KNN, Test MSE: {knn_test_error:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSfUg6Lfem-K"
      },
      "source": [
        "Now, let's fit linear regression model and compute MSE on training and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejqTznFdem-K",
        "outputId": "2687225e-1b5d-4688-dbde-184ef8d21092"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear Regression, Training MSE: 21.64\n",
            "Linear Regression, Test MSE: 24.29\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "\n",
        "lin_reg.fit(X_train, y_train)\n",
        "\n",
        "lin_y_train_pred = lin_reg.predict(X_train)\n",
        "lin_y_test_pred = lin_reg.predict(X_test)\n",
        "\n",
        "lin_train_error = mean_squared_error(y_train, lin_y_train_pred)\n",
        "lin_test_error = mean_squared_error(y_test, lin_y_test_pred)\n",
        "\n",
        "print(f'Linear Regression, Training MSE: {lin_train_error:.2f}')\n",
        "print(f'Linear Regression, Test MSE: {lin_test_error:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoZXSkfTem-O"
      },
      "source": [
        "The MSE errors for K-NN regression and linear regression model are around the same range, with slightly better performance of K-NN. Our implementation of K-NN regression is working as expected. Now, let's check the decision function of K-NN regression on a synthetic dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AW02R-Sem-O"
      },
      "source": [
        "## Decision function of K-NN regression\n",
        "\n",
        "Figure 1 present below illustrates the fit of K-NN regression with $K=4$ and Euclidean distance metric on a linear synthetic dataset with a single feature $\\mathbf{x}$, and continuous target variable $y=2\\mathbf{x}+\\text{noise}$.\n",
        "\n",
        "<center>\n",
        "    <!-- <figure><img src=\"https://doc.google.com/a/fusemachines.com/uc?export=download&id=1QJhwvMW5aA8gWR3WrSiPFQ2Ga3ZUiCbR\" width=600> -->\n",
        "    <img src =\"https://i.postimg.cc/XvJ2tKHr/image.png\" width=600>\n",
        "    </figure>\n",
        "    <figcaption>Figure 1: Decision function of K-NN regression</figcaption>\n",
        "</center>\n",
        "\n",
        "The red points denotes the stored training data points. Now, as seen in the figure, the decision function of K-NN regression takes **non-linear, discontinuous form as combinations of step functions**. The step functions(sudden change in target values) are the result of abrupt shift in nearest neighbors(change in voters) with respect to feature values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0_CkKXTXaat"
      },
      "source": [
        "### Limitation of K-NN Regression's prediction\n",
        "\n",
        "<center>\n",
        "    <figure>\n",
        "    <!-- <img src=\"https://doc.google.com/a/fusemachines.com/uc?export=download&id=1BS_oJy1vooPY2kL3Ow_D7abZ_QX-OT6U\" width=650> -->\n",
        "    <img src=\"https://i.postimg.cc/9fJ67FS7/image.png\" width=650></figure>\n",
        "    <figcaption>Figure 2: Limitation of K-NN Regression</figcaption>\n",
        "</center>\n",
        "\n",
        "Figure 2 shows the prediction from the same K-NN regression as figure 1 for feature values beyond the stored instances. Since, the extreme values of stored sample being between -0.5 and 0.5, new unseen $\\mathbf{x}$ greater or less than 0.5 uses those stored sample resulting in constant prediction. _Incorrect prediction for input samples whose magnitude are beyond the stored instance is a major limitation of K-NN regression model_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzHVmMDaem-P"
      },
      "source": [
        "## Effects of K in K-NN regression\n",
        "\n",
        "As K-NN regression is closely related to K-NN classifier, the effect of K value on the decision function is similar. Small value of $K$ results in high variance, low bias regression model, while large value results in low variance, high bias regression model. Figure 2 shows the decision function for different values of $K$ from 1 to 10 fitted on same dataset as figure 1.\n",
        "\n",
        "<center>\n",
        "    <figure>\n",
        "    <!-- <img src=\"https://doc.google.com/a/fusemachines.com/uc?export=download&id=1zABZSJ7gAaPsLsoaJDyNiJEw4HolCEUp\" width=600> -->\n",
        "    <img src=\"https://i.postimg.cc/NMQRWQfp/image.png\" width=600>\n",
        "    </figure>\n",
        "    <figcaption>Figure 3: Effect of K on decision function of K-NN regression</figcaption>\n",
        "</center>\n",
        "\n",
        "Notice that $K=1$ model fits the dataset perfectly, hitting every training data point with large abrupt changes in target values. This model has large variance because the prediction depends on a single stored training label. Increasing $K$ results in averaging values over neighbor's target value and consequently smoother decision function, as with $K=10$. This model has larger bias than $K=1$. Note: Increasing the value further might mask the original function of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-XhBI4Gem-Q"
      },
      "source": [
        "## Comparision of Linear regression with K-NN regression\n",
        "\n",
        "Now that we delved into the working, decision function of K-NN regression model, let's investigate further when one should use K-NN regression by comparing it with the most popular regression algorithm,  linear regression(Not polynomial regression).\n",
        "\n",
        "Compared to K-NN regression model, linear regression is a parametric model that with linear assumption $f(X)=X\\beta$. Though easy to fit with small number of parameters, the disadvantage of parameteric models is the strict assumption of true function $f(X)$ and consequently constant form of hypothesis function $h(X)$. **Parametric models severely decline in performance if the assumption doesn't match the true mapping function of data**. On the other hand, K-NN regression doesn't assume a strict form of $f(X)$, ergo provides more flexible fit for regression problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "161kYG7Hmw5Q"
      },
      "source": [
        "### Linear dataset\n",
        "\n",
        "Let's compare the two model on same dataset as in figure 1.\n",
        "\n",
        "<center>\n",
        "    <figure>\n",
        "    <!-- <img src=\"https://doc.google.com/a/fusemachines.com/uc?export=download&id=1fOQoaSSdu9BIozKM-GscxWX2B3hVJhqP\"> -->\n",
        "    <img src=\"https://i.postimg.cc/nr3rSQ3G/image.png\">\n",
        "    </figure>\n",
        "    <figcaption>Figure 4: Fit of K-NN and linear regression on dataset having linear relationship between x and y | (a) the decision functions | (b) Mean squared errors</figcaption>\n",
        "</center>\n",
        "\n",
        "Figure 4(a) shows the fit of 1-NN, 10-NN and linear regression model on the synthetic dataset with mapping function $y=2\\mathbf{x}+noise$. Here, since the assumed form of hypothesis function matches the true mapping function, the linear regression performs better than the more flexible K-NN regression (K-NN incurs a cost in variance that is not offset by a reduction in bias(James, G., (2013), pg 106)), as can be seen on figure 4(b). Figure 4(b) shows the MSE score of various K-NN model and linear regression model. None of the K-NN model (value of $K$ranged from 1 to 10) outform the linear regression model in such scenario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--Zqx6lBmx8e"
      },
      "source": [
        "### Non-linear dataset\n",
        "\n",
        "Now, let's try the same experiment on non-linear datasets. Note: In figure 5(a), 6(a), the black colored decision function are the true mapping function not the linear regression's decision function.\n",
        "\n",
        "Figure 5 shows the experiment, when the dataset is slightly complex with mapping function is $y=\\mathbf{x}^3 + \\text{noise}$, the linear regression model still outperforms K-NN regression for lower values of $K$. However, as $K$ increases K-NN regression model performs better than simple linear regression.\n",
        "\n",
        "<center>\n",
        "    <figure>\n",
        "    <!-- <img src=\"https://doc.google.com/a/fusemachines.com/uc?export=download&id=10m9JfKVSllSTQSmAcOrdnj6X38n5f_1Z\"> -->\n",
        "    <img src=\"https://i.postimg.cc/Qd4PNNyY/image.png\">\n",
        "    </figure>\n",
        "    <figcaption>Figure 5: Fit of K-NN and linear regression on dataset with 3rd degree polynomial relation between x and y| (a) the decision function of K-NN and true mapping function | (b) Mean squared errors</figcaption>\n",
        "</center>\n",
        "\n",
        "Finally when the dataset is significantly deviated from linearity, say dataset with mapping $y=\\mathbf{x}^3+\\mathbf{x}^2+noise$, there is substantial change on the model performance. K-NN regression because of its non-parametric nature heavily outperforms linear regression model.\n",
        "\n",
        "<center>\n",
        "    <figure>\n",
        "    <!-- <img src=\"https://doc.google.com/a/fusemachines.com/uc?export=download&id=1AWBfGdalB4SeLC3w3p1cGeiVlPk9f7dz\"> -->\n",
        "    <img src=\"https://i.postimg.cc/qv3nqjbf/image.png\">\n",
        "    </figure>\n",
        "    <figcaption>Figure 6: Fit of K-NN and linear regression on complex dataset | (a) the decision function of K-NN and true mapping function | (b) Mean squared errors</figcaption>\n",
        "</center>\n",
        "\n",
        "The above cases shows that K-NN is worse than linear regression for dataset with linear relationship but better for non-linear datasets. In real-world since most of the dataset have non-linear relationship between features and variables, one might draw conclusion that K-NN is always a better choice than linear regression. However, it not always the truth, due to curse of dimensionality and sensitivity of noise features, even in non-linear cases, K-NN might perform worse than linear regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlklVdzFmyTn"
      },
      "source": [
        "\n",
        "### Effect of large feature dimension and noise variables in K-NN\n",
        "\n",
        "<center>\n",
        "    <figure>\n",
        "    <!-- <img src=\"https://doc.google.com/a/fusemachines.com/uc?export=download&id=1o2EvOReuASTSY6jZw1h6AO-yP5O1vJMV\" height=220> -->\n",
        "    <img src=\"https://i.postimg.cc/pLh7TMM2/image.png\" height=220>\n",
        "    </figure>\n",
        "    <figcaption>Figure 7: Effect of high-dimension and noisy features on K-NN and linear regression</figcaption>\n",
        "</center>\n",
        "\n",
        "Figure 7 shows the performance of K-NN and linear regression model on the same dataset as figure 6 but with additional noise features added that don't affect the target value. As the number of noise features are denoted by $e$ increases the performance of linear regression model is unaffected, but the performance of K-NN regression deteriorates. Increasing the number of samples in dataset might solve this issue, but is generally cumbersome in real-world.\n",
        "\n",
        "The main point to note is, even if K-NN performs better than linear regression for non-linear scenario, we prefer linear regression model for higher dimensional dataset with comparatively lower number of datasample. Remember, __parametric models are generally better for smaller ratio of number of samples per feature__. Even _when the feature dimesion is small if the performance of K-NN is slightly greater than linear regression, prefer linear regresssion due to its simplistic interpretability and small number of parameters_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9FZ9u9aem-Q"
      },
      "source": [
        "## Application of K-NN Regression\n",
        "\n",
        "The decision function of the K-NN regression model is discontinuous, step in nature, therefore, isn't a popular regression algorithm. However, researchers, engineers have used K-NN regression in real-world application due to its simplicity such as:\n",
        "\n",
        "1. __Missing value imputation__ - Scikit-learn provides K-NN imputer, ```impute.KNNImputer``` that uses K-NN regression to impute missing values of features.\n",
        "\n",
        "\n",
        "2. __Image reconstruction__ - Corrupt pixels of image is similar to missing feature values. Using K-NN regression one can reconstruct the corrupt pixel leading to image reconstruction.\n",
        "\n",
        "  - Check out blakefuller3's blog on [Image reconstruction using KNN algorithm]( https://blakefuller3.wordpress.com/2013/05/28/image-reconstruction-using-knn-algorithm/)\n",
        "\n",
        "  - 3D reconstruction paper by Zhu, H., et al. (2019) uses KNN, [Paper here]( https://link.springer.com/article/10.1007%2Fs11042-019-7686-1)\n",
        "  \n",
        "  \n",
        "3. Others (Imandoust, S. B., and Bolandraftar, M. (2013))\n",
        "\n",
        " - Baseline models\n",
        " - Financial sector - Stock market price prediction.\n",
        " - Health sector - Estimation of amount of glucose in blood of diabetic person."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBlJ1ZDWem-R"
      },
      "source": [
        "## Pros/Cons of K-NN\n",
        "\n",
        "At the end of this read, let's discuss the pros and cons of entire K-NN algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UEo56Uem-R"
      },
      "source": [
        "### Pros\n",
        "\n",
        "Following are the pros of using K-NN algorithm:\n",
        "\n",
        "1. Simplicity - K-NN is an instance-based algorithm which easy to understand and implement. (Majority voting or aggregation).\n",
        "\n",
        "2. No strict assumption on true mapping function - Works for linear/non-linear dataset. This property acts double-edged sword(as discussed in comparision subsection.)\n",
        "\n",
        "3. Zero to minute training time\n",
        " - Effective in online-learning scenario, as new data can be seemlessly added(No complete retraining required.)\n",
        "\n",
        " - Acts base model for initial analysis of dataset before running more complex algorithm.\n",
        "\n",
        "\n",
        "4. In comparision to other algorithms, generalizes to multi-class classification problem without additional settings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCV6B7Fhem-S"
      },
      "source": [
        "### Cons\n",
        "\n",
        "Following points represent the cons of using K-NN algorithm:\n",
        "\n",
        "1. Poor performance for large dimensional feature space(due to curse of dimensionality.)\n",
        "\n",
        "2. Requires of domain knowledge for selection of distance metric and optimal value of $K$.\n",
        "\n",
        "3. Needs homogeneous features for optimal performance, i.e. feature magnitude must be scaled.\n",
        "\n",
        "3. Sensitivity to data with noise, missing values and outliers.\n",
        "\n",
        "    - K-NN based models require careful preprocessing steps such as feature scaling, outlier handling, etc. else the performance degrades substantially."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmysE-zOem-S"
      },
      "source": [
        "That's all for the read in K-NN unit. Next, we are discussing programming material of K-NN, where you will learn the things to consider while using K-NN algorithm such as how to select optimal $K$, effect of unscaled, noisy features, importance of preprocessing and learn improvement of K-NN, known as weighted K-NN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s18GxmeHem-T"
      },
      "source": [
        "## Key-Takeaways\n",
        "\n",
        "The main points to remember from this notebooks are:\n",
        "\n",
        "1. K-NN Regressor is similar to K-NN classifier. The difference is the prediction phase, where K-NN regression:\n",
        "\n",
        "   - Predicts new sample's ($\\mathbf{x}_t$) target value ($h(\\mathbf{x}_t)$) by computing the \"$K$\" nearest neighbors ($S_{\\mathbf{x}_t}$) among stored training set then taking average of the neighbors target values(aggregation scheme.)\n",
        "    \n",
        "    $$h(\\mathbf{x}_t) = \\frac{\\sum_{y'' \\in S_{\\mathbf{x}_t}} y''}{K}$$\n",
        "    \n",
        "    \n",
        "2. K-NN Regression's decision function has non-linear and discontinuous form - combination of step functions. Also, K-NN regression gives errorneous predictions for feature values far away from the stored instance's features.\n",
        "\n",
        "\n",
        "3. $K$ values affects K-NN regression similar to K-NN classifier.\n",
        " - Small $K$ - High variance, low bias model.\n",
        " - Large $K$ - Low variance, high bias model.\n",
        "\n",
        "\n",
        "4. When the true mapping function of dataset matches the assumption of parametric models(linear regression), they outperform non-parametric models(K-NN regression). Along with this, parametric models are also better for smaller ratio of number of samples per feature.\n",
        "\n",
        "\n",
        "5. Even for smaller feature dimension if the performance of K-NN regression is slightly greater than linear regression, prefer linear regresssion due to its simplistic interpretability and small number of parameters.\n",
        "\n",
        "\n",
        "6. Though unpopular, K-NN regression are used for missing value imputations and image reconstructions.\n",
        "\n",
        "\n",
        "7. Simplicity, dynamic hypothesis function, faster training, and multi-class generalization are pros of K-NN model, while poor performance in high feature dimension, proper domain knowledge, sensitivity to feature scale, noise and missing values are its cons."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
