{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQgRnx_vrp_e"
      },
      "source": [
        "# Classification using K-NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HKkhkGhrp_g"
      },
      "source": [
        "## Learning objectives\n",
        "\n",
        "After reading this notebook, students must be able to:\n",
        "\n",
        "1. Explain the working of K-NN classifiers.\n",
        "2. Discuss about variation in K values and its effect.\n",
        "3. Implement K-NN algorithm for classification.\n",
        "4. Exemplify the application of K-NN in classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wHM59rVrp_h"
      },
      "source": [
        "As discussed in the previous chapter, K-NN is a lazy learning algorithm used for classification and regression. In this notebook, you are learning about K-NN classifiers, effects of variation of $K$ in classifier along with application areas of K-NN classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNmNdc59rp_h"
      },
      "source": [
        "\n",
        "## K-NN classifier\n",
        "\n",
        "K-NN classifiers are K-NN based classification algorithm with a base of **majority voting** scheme. After setting the hyperparameters of the model, the algorithm of K-NN classifier follows as:\n",
        "\n",
        "\n",
        "1. Training phase\n",
        "\n",
        "  Provided the training dataset, $D = \\{\\{\\mathbf{x}_i \\in \\mathbb{R}^m, y_i\\}, i= 1, 2, \\cdots, n\\}$.\n",
        "\n",
        "  - Store the training dataset in memory, either directly or using advanced data structures(like kd-tree,ball-tree).\n",
        "  \n",
        "\n",
        "2. Prediction phase\n",
        "\n",
        "  Given a test sample $\\mathbf{x}_t$,\n",
        "\n",
        "  1. Compute its \"K\"-nearest neighbors set, $S_{\\mathbf{x}_t}$.\n",
        "    Note: K-nearest neighbors set is defined as, $S_{\\mathbf{x}_t} \\subseteq D$ such that:\n",
        "    1. The cardinality of $S_{\\mathbf{x}_t}$ , $\\vert S_{\\mathbf{x}_t}\\vert = K$.\n",
        "    2. All points of training set $D$ that is not in $S_{\\mathbf{x}_t}$ is atleast as far away from $\\mathbf{x}_t$ as the furthest point in $S_{\\mathbf{x}_t}$. Mathematically, $\\forall(\\mathbf{x}',y') \\in D \\backslash S_{\\mathbf{x}_t}$\n",
        "    \n",
        "    $$\\text{dist}(\\mathbf{x}_t, \\mathbf{x}') \\ge\\max_{(\\mathbf{x}'',y'')\\in S_\\mathbf{x}} \\text{dist}(\\mathbf{x}_t,\\mathbf{x}'')$$\n",
        "    \n",
        "    <center>\n",
        "    <!-- <figure><img src=\"https://doc.google.com/a/fusemachines.com/uc?export=download&id=1o8XnxCsP3p4a0yQnLw6Iy6XhVo5dskuI\" width=600/> -->\n",
        "    <img src=\"https://i.postimg.cc/fyhvK1Vh/image.png\" width=600/></figure>\n",
        "    <figcaption>Figure 1: Nearest neighbor set visualization</figcaption>\n",
        "</center>\n",
        "    Figure 1 shows the above mentioned properties of $S_{\\mathbf{x}_t}$ on two-dimensional data set. Here, unseen data point($\\mathbf{x}_t$) denoted by red circle. Applying 3-NN, the $S_{\\mathbf{x}_t}$ computed is shown by black circle(with its element as blue dots) that has cardinality($\\vert S_{\\mathbf{x}_t}\\vert$) equal to three($K$). Also, the maximum distance between query point and element of $S_{\\mathbf{x}_t}$($d_1$) is less than distance between query point and next closest data point $\\mathbf{x}'_1$($d_2$) which is not in $S_{\\mathbf{x}_t}$.\n",
        "\n",
        "  2. Predict the target value of the test sample, $h(\\mathbf{x}_t)$ as\n",
        "highest count/frequency of class label in $S_{\\mathbf{x}_t}$. Mathematically,  \n",
        "    \n",
        "    $$\\boxed{h(\\mathbf{x}_t) = \\text{mode}(\\{y'': (\\mathbf{x}'',y'')\\in S_{\\mathbf{x}_t}\\})}$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNZjI8vPrp_i"
      },
      "source": [
        "<center>\n",
        "    <figure>\n",
        "    <!-- <img src=\"https://doc.google.com/a/fusemachines.com/uc?export=download&id=1CkGeCXpvcq-49yA1SmqWlSj9VWwCO1hh\" width=800/> -->\n",
        "    <img src=\"https://i.postimg.cc/XYnPfHh5/k-nearest-neighbor-classifier.gif\" width=800/>\n",
        "    \n",
        "    </figure>\n",
        "    <figcaption>Figure 2: Working of K-NN classifier in two dimensional multiclass classification scenario </figcaption>\n",
        "</center>\n",
        "\n",
        "Figure 1 shows an example of a K-NN classifier for two-dimensional feature space with three classes, where $K$ is set to 5. For the new test sample denoted by the blue cross, the 5-nearest neighbors are shown by arrows. Finally, using the majority vote scheme, the predicted label of the test sample is red-triangle class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxozrp-Nrp_j"
      },
      "source": [
        "## Implementation of K-NN classifier\n",
        "\n",
        "Now that you grasped the K-NN classifier's underlying concepts let's implement a K-NN classifier using the Numpy module. Using a functional paradigm for easier understanding, we must create functions required for implementing K-NN classifier, starting with the training function, ```train```.\n",
        "\n",
        "Note: We will be using brute force and euclidean distance for computing nearest neighbors. Also, this implementation of the K-NN classifier only supports numerical data.\n",
        "\n",
        "Numpy being the required module for implementation, should be imported at first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y8V52be0rp_k"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEyZRCv2rp_o"
      },
      "source": [
        "### Training function\n",
        "\n",
        "The training portion of the K-NN classifier simply requires the storage of training data or advanced data structure. As the brute force is the computation method, this implementation doesn't require computing any data structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eyCHWIL8rp_p"
      },
      "outputs": [],
      "source": [
        "def train(X_train, y_train):\n",
        "    '''trains the K-NN classifier; i.e. storage of training data'''\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytb9dEisrp_s"
      },
      "source": [
        "As you can see, the ```train``` function doesn't require any statement and returns nothing. However, we can compute advanced data structures such as kd-tree in this step and store it. Next, let's develop functions for computing neighbors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHVqcrdZrp_t"
      },
      "source": [
        "### Compute the distance between the new sample and stored samples\n",
        "\n",
        "First, let's define function ```euclidean``` to compute the Euclidean distance between two data samples of shape ```(1,m)```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "J4PYGYdNrp_u"
      },
      "outputs": [],
      "source": [
        "# Takes two samples x and y and computes the euclidean distance\n",
        "euclidean = lambda x,y : np.sqrt(np.sum((x-y)**2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OUKKzuxrp_x"
      },
      "source": [
        "Computing distance between the new data sample and all the stored training samples requires the application of loop. The ```calculate_distance``` function defined below computes the euclidean distance between a new sample and every stored training sample and returns the distance in list ```distances```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FGspSKx1rp_z"
      },
      "outputs": [],
      "source": [
        "def calculate_distance(X_new, X_train):\n",
        "    ''' Calculates distance between new sample and stored training samples'''\n",
        "\n",
        "    distances = []\n",
        "\n",
        "    for i in range(len(X_train)):\n",
        "        distances.append(euclidean(X_new,X_train[i,:]))\n",
        "    return distances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiPz3DQarp_3"
      },
      "source": [
        "### Obtain labels of k-nearest neighbors\n",
        "\n",
        "After the computation of distance metrics, the next step is finding the \"K\"-nearest neighbors and then obtain their respective target labels. Let's define a function named ```predict_labels``` that:\n",
        "\n",
        "1. First, finds the index of \"k\"-nearest neighbors from the ```distances``` list computed from ```calculate_distance``` function.\n",
        "    - Here, we use ```np.argsort``` to find the indices that would sort ```distances``` in ascending order, then take first k elements and store the index of ```near_neigh_index```.\n",
        "    \n",
        "    \n",
        "2. Then, we compute the corresponding target values of the k-nearest neighbors of new sample, store in ```neigh_labels``` and return it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hdGjVsPprp_3"
      },
      "outputs": [],
      "source": [
        "def predict_labels(y_train, distances, k):\n",
        "    '''\n",
        "    Return array of target values of k-nearest neighbors of a new sample\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_train: Numpy.array, shape(n,)\n",
        "        training target vector\n",
        "    distances: list,\n",
        "            distance between new sample and stored samples\n",
        "    k: int,\n",
        "        number of neighbors to take into account\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    neigh_labels: Numpy.array, shape(k,)\n",
        "                target labels of k-nearest neighbors\n",
        "\n",
        "    '''\n",
        "\n",
        "    near_neigh_index = np.argsort(distances)[:k] # Find indices of k nearest neighbors\n",
        "    neigh_labels = y_train[near_neigh_index]  # labels of k nearest neighbors\n",
        "\n",
        "    return neigh_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDXRtHKBrp_7"
      },
      "source": [
        "### Prediction from K-NN classifier\n",
        "\n",
        "Finally, let's combine all the above-defined functions and implement K-NN classifier, ```knn_classifier```, which operates as:\n",
        "\n",
        "1. Use ```train``` function to store the dataset. (Here, do nothing as we aren't computing any data structure.)\n",
        "2. Initialize list ```predictions``` to store the predicted labels.\n",
        "3. For each element of the set of new samples:\n",
        "    1. Compute the distance between each stored sample using ```calculate_distance``` and store in ```neigh_dist```.\n",
        "    2. Compute the labels of its k-nearest neighbors using ```predict_labels``` and store in ```k_labels```\n",
        "    3. Use the voting scheme to compute its label and append to ```predictions``` list.\n",
        "        Here, we are using ```np.bincount``` to count each label's occurrence and then find the maximum occurrence one using ```np.argmax```.\n",
        "        \n",
        "4. Return the ```predictions``` that are the prediction of K-NN classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bWVFo0MArp_8"
      },
      "outputs": [],
      "source": [
        "def knn_classifier(X_train, y_train, X_test, k):\n",
        "    '''K-NN classifier : Trains and then predicts label of test set\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train: Numpy.array, shape(n,m)\n",
        "        training feature vector\n",
        "    y_train: Numpy.array, shape(n,)\n",
        "        training target vector\n",
        "    X_test: Numpy.array, shape(z,m)\n",
        "        Data to be classified\n",
        "    k: int\n",
        "        Number of neighbors to take into account\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    predictions: Numpy.array, shape(z,)\n",
        "        predicted labels of X_test\n",
        "    '''\n",
        "\n",
        "    train(X_train, y_train) # Training phase\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(X_test)): # Prediction phase\n",
        "        neigh_dist = calculate_distance(X_test[i,:], X_train)\n",
        "        k_labels = predict_labels(y_train, neigh_dist, k)\n",
        "        predictions.append(np.argmax(np.bincount(k_labels)))\n",
        "\n",
        "    return np.array(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKZagwnzrp__"
      },
      "source": [
        "That's all for the implementation of the K-NN classifier. Note: Brute-force implementation of K-NN has time complexity $\\approx \\text{O(knm)}$, where n - number of training samples, m - dimension of features, k - number of voters.\n",
        "\n",
        "Now, let's test the performance of this classifier in the ```iris``` dataset.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy790SGkrqAA"
      },
      "source": [
        "### Loading Iris dataset with train-test split\n",
        "\n",
        "We are using Scikit-learn for loading the dataset and train-test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QRO0KuTUrqAA"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_iris, y_iris = load_iris(return_X_y = True) # Loading the iris dataset\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_iris,  # Train-test split\n",
        "                                                    y_iris,\n",
        "                                                    test_size=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COdSpnKtrqAD"
      },
      "source": [
        "### Performance of K-NN classifier\n",
        "\n",
        "Let's set the value of \"K\" at random to be 3, and then test the performance of the ```knn_classifier``` on both train and test set using Scikit-learn's ```classification_report```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "afP-kI6GrqAE"
      },
      "outputs": [],
      "source": [
        "# Predictions of training set\n",
        "y_train_pred = knn_classifier(X_train, y_train, X_train, 3)\n",
        "\n",
        "# Predictions of test set\n",
        "y_test_pred = knn_classifier(X_train, y_train, X_test, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Wli8m8phrqAH",
        "outputId": "3f7db40d-b8fb-458b-a08e-6678f762810a",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t------------------training set------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        36\n",
            "           1       1.00      0.90      0.95        30\n",
            "           2       0.93      1.00      0.96        39\n",
            "\n",
            "    accuracy                           0.97       105\n",
            "   macro avg       0.98      0.97      0.97       105\n",
            "weighted avg       0.97      0.97      0.97       105\n",
            "\n",
            "\t--------------------test set--------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        14\n",
            "           1       0.90      0.95      0.93        20\n",
            "           2       0.90      0.82      0.86        11\n",
            "\n",
            "    accuracy                           0.93        45\n",
            "   macro avg       0.93      0.92      0.93        45\n",
            "weighted avg       0.93      0.93      0.93        45\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Performance evaluation\n",
        "print('\\t------------------training set------------------')\n",
        "print(classification_report(y_train, y_train_pred))\n",
        "print('\\t--------------------test set--------------------')\n",
        "print(classification_report(y_test, y_test_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7wy5GoLrqAL"
      },
      "source": [
        "As seen, our implementation of the K-NN classifier performs exceptionally with 97% accuracy in the training set and 96% in the test set. That ends the portion on the implementation of the K-NN classifier. Now, let's visualization the decision boundary of the K-NN classifier for varying values of $K$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-2MsaWCrqAM"
      },
      "source": [
        "## Decision-boundary of 1-NN classifier\n",
        "\n",
        "Starting with smallest value of $K$, i.e. $K$=1, the prediction of new sample only depends upon its closest neighbor. Using this knowledge, the potential decision boundaries of the 1-NN classifier can be visualized using [**Voronoi diagram**]( https://en.wikipedia.org/wiki/Voronoi_diagram). Voronoi diagram shows the region of points that are closest to a given set of points. For an arbitary synthetic data with two classes(red and blue), the decision boundary using 1-NN classifier is:\n",
        "\n",
        "<center>\n",
        "    <figure>\n",
        "    <!-- <img src=\"https://doc.google.com/a/fusemachines.com/uc?export=download&id=1yaeHcN_XAQHN37_RzZkJljIQa3f0V-Ip\" width=800> -->\n",
        "    <img src=\"https://i.postimg.cc/6QPc7YnC/image.png\" width=800>\n",
        "    </figure>\n",
        "    <figcaption>Figure 3: Voronoi diagram showing potential decision boundary of 1-NN classifier using Euclidean distance metric</figcaption>\n",
        "</center>\n",
        "\n",
        "The region belonging to each point is colored according to their respective class. A new sample situated top-left would be classified as a red class in the above figure. Else if situated at bottom-right, would be classified as a blue class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qQ7icpErqAM"
      },
      "source": [
        "## Effect of K in K-NN classifier\n",
        "\n",
        "Let's understand the *effect of the number of voters* from a simple example. During an election for president of a club(name not important) with 1000 people, we are assigned to figure out the percentage of people who will vote for representative A out of two representatives(A and B).\n",
        "\n",
        "A simple solution is to randomly choose 40 club members from a particular age group, say 20-30, query each one on who they are planning to vote. Say the result comes out as:\n",
        "\n",
        "\n",
        "<center>Table 1: Sample survey result on voting</center>\n",
        "\n",
        "| Vote for A | Vote for B | Unsure(No response) | Total sample |\n",
        "|-|-|-|-|\n",
        "| 16 | 8 | 16 | 40 |\n",
        "\n",
        "From this result, we can estimate the percentage of voting for representative A would be nearly equal to $$\\frac{16}{16+8}*100= 66.67\\%$$\n",
        "\n",
        "However, the actual results came to be representative B, winning by 70% majority. How is this possible? There are many faults in the survey we conducted that introduced bias and variance:\n",
        "\n",
        "- _Source of bias_\n",
        "\n",
        " - Using club members of a particular age group.\n",
        " - Not following up on the unsure members.\n",
        "\n",
        "- _Source of variance_\n",
        " - Small sample size from the population."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KZI2gLJrqAN"
      },
      "source": [
        "Similarly, this idea extends to the number of voters in the K-NN classifier. Let's take a two-dimensional synthetic dataset with three classes, created using Scikit learn's ```make_classification``` method for visualization of decision boundaries.\n",
        "\n",
        "<center>\n",
        "    <figure>\n",
        "    <!-- <img src=\"https://doc.google.com/a/fusemachines.com/uc?export=download&id=19DRooRGbIUkm8sK450pM1Aajnt4huL4z\" width=800/> -->\n",
        "    <img src=\"https://i.postimg.cc/W1v1Vvyk/image.png\" width=800/>\n",
        "    </figure>\n",
        "    <figcaption>Figure 4: Effect of K value in decision boundary of K-NN classifier</figcaption>\n",
        "</center>\n",
        "\n",
        "As seen in figure 3, the dataset is noisy. Applying the K-NN algorithm starting with $K=1$, the decision boundary is flexible/jagged, capturing almost every training datapoint's true label.  Due to a small number of voters, **small values of $K$ provide a more flexible fit, i.e., creates high variance and low bias classifier**. In contrast, a Higher $K$ value results in less flexible the decision boundary, producing an almost linear decision boundary at $K=100$. So,  **large values of $K$ creates low variance and high bias classifier**, the reason being a large number of voters tends to mask the actual structure of original mapping function $f(\\mathbf{x})$.\n",
        "\n",
        "In machine learning, finding the balance between bias and variance results in failure or success of the model's generalization power. Ergo, tuning the hyperparameter $K$ of K-NN classifier and setting the best value is a must. We will learn how to find the best value of $K$ in this unit's programming material.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4ky8Fp0rqAN"
      },
      "source": [
        "## Application of K-NN classifier\n",
        "\n",
        "Though generally slower and less accurate than Neural networks for larger data size, K-NN performs well with non-linear data and is desirable when having less knowledge regarding the dataset. let's discuss a few application areas of K-NN classifiers:\n",
        "\n",
        "1.  **Image/ Video/ Text classification** . However, this requires preprocessing, such as dimensionality reduction and feature extraction.\n",
        "\n",
        "   - For example, [HertaSecurity]( https://www.hertasecurity.com/en) uses K-NN to identify a person's face. Firstly, this system uses the DL algorithm to extract feature vectors from faces and later uses K-NN on those feature vectors for classification.\n",
        "    \n",
        "    \n",
        "2. **K-NN search** - task related to finding similar items. A few examples of K-NN search are:\n",
        "\n",
        "   - **Concept search**, finding semantically similar documents. Examples are finding novels with similar emotions.\n",
        "   - **Recommender systems**, recommending users similar items that they prefer.\n",
        "   \n",
        "   \n",
        "3. **Outlier detection**, using the notion of distance/neighborhoods to detect outliers.\n",
        "    \n",
        "    - Dang, T. T., et al. (2015) demonstrate the use of distance-based K-NN for outlier detection in large-scale traffic data in their [paper]( https://ieeexplore.ieee.org/document/7251924)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDD2MlUcrqAO"
      },
      "source": [
        "That's all for this read on K-NN classifiers. Next, we are discussing the K-NN model for the regression task, its comparison with linear regression models, the pros/cons of K-NN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XdSQg5trqAP"
      },
      "source": [
        "\n",
        "## Key-Takeaways\n",
        "\n",
        "The main points to remember from this notebook are:\n",
        "\n",
        "1. KNN-classifier, a lazy algorithm used for classification that works by:\n",
        "    - Storing the training set ($D$) or data structure representing the training set.\n",
        "    - Classifying new sample ( $ x_t $ )  by, computing the $K$ nearest neighbors ($S_{\\mathbf{x}_t}$) among stored training set then, predicting the class of new sample ($h(\\mathbf{x}_t)$) by majority vote.\n",
        "\n",
        "$$ h(\\mathbf{x}_t) =  \\text{mode} ( \\{ y'' : (\\mathbf{x}'', y'' ) \\in S_{\\mathbf{x}_t} \\} )$$\n",
        "\n",
        "    \n",
        "2.  K-NN classifier slower than other model-based algorithms for a dataset containing large amounts of samples/features. It requires finding similarity of a new sample with all stored training samples, computational complexity $\\approx \\text{O(knm)}$.\n",
        "\n",
        "\n",
        "3. Hyperparameter $K$ affects the K-NN model's decision boundaries as:\n",
        "    - **Jagged and non-linear for lower values** of $K$, i.e., high variance and low bias.\n",
        "    - **Smooth and linear for higher values** of $K$, i.e., high bias and low variance.\n",
        "\n",
        "\n",
        "4. K-NN models have industrial applications such as image/video classification, K-NN search(recommender systems), and outlier detection."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai_ml_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
