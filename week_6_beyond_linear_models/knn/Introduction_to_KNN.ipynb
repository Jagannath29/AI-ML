{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TLuKJZ3GQmr"
   },
   "source": [
    "# k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXbVn6_AtMc_"
   },
   "source": [
    "## ML models based on generalization\n",
    "\n",
    "For us, humans, generalizing/learning new ideas mainly breaks down into two concepts:\n",
    "\n",
    "1. *Memorization*: where you learn by heart, memorize/store the ideas without understanding the new topic.\n",
    "\n",
    "2. *Understanding*: where you gain insight, have the general knowledge of all the new topic's underlying concepts.\n",
    "\n",
    "Similarly, supervised machine learning systems can be categorized into _two categories_ based on how they generalize to unseen data samples:\n",
    "\n",
    "- Model-based learning\n",
    "- Instance-based learning\n",
    "\n",
    "To understand both of them, let's take an example. Say, you are given a corpus and assigned to build an emotion detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptacGerPuVcD"
   },
   "source": [
    "### Model-based learning\n",
    "\n",
    "One way of creating the emotion detection system is by feeding the labeled corpus, denoted as $D = \\{\\{\\mathbf{x}_i, y_i\\}, i= 1, 2, \\cdots, n\\}$, to the system that then learns some parameterized hypothesis function, $h: \\mathbf{x} \\to y$, which approximates the original mapping function, $f: \\mathbf{x} \\to y$ of the dataset.\n",
    "\n",
    "Given a new unseen document, this ML system predicts the emotion using the learned hypothesis function. Such a system categorize as **model-based learning**. Machine learning models you learned until now - linear/logistic regression, tree-based methods, neural networks fall under model-based learning.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "<img src=\"https://i.postimg.cc/6phWNfKW/image.gif\" width=700/>\n",
    "</figure>\n",
    "<figcaption> Figure 1: Model-based learning(will be changed, needs labeling and change new data as x)</figcaption>\n",
    "</center>\n",
    "\n",
    "Figure 1 demonstrates model-based learning for binary classification. Here, the model learns the decision boundary denoted by the red line using a training sample. Prediction of new unseen data (X) is based on which side of the boundary it falls on, right side - green, left side - blue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2s46teVcXFL"
   },
   "source": [
    "### Instance-based learning\n",
    "\n",
    "Another approach for creating the emotion detection system is using the essence of similarity. i.e. **Similar input data have similar target values/labels**. The system first memorizes/stores(rote) all the training corpus in memory in the approach. It then predicts the unseen document's emotion label as the emotion of most closely resembling(similar) stored document. A simple similarity measure between the unseen and stored document would be common word counts. Such a system classifies as **instance-based learning** because it's based on comparing unseen instances with stored training instances.\n",
    "\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "<img src=\"https://i.postimg.cc/gkXs5gvh/Instance-based-learning.gif\" width=700  />\n",
    "</figure>\n",
    "<figcaption> Figure 2: Instance-based learning</figcaption>\n",
    "</center>\n",
    "\n",
    "Figure 2 demonstrates an instance-based learning system for a similar binary classification problem on two-dimensional space. After storing training instances, a new sample(x) is compared with stored instances using distance metrics (similarly measure). The new sample's label is a triangle as the most similar training instance belonged to the triangle class.\n",
    "\n",
    "Note: Instance-based learning models are heavily dependent on similarity measures whose output is affected by feature magnitudes. **Feature scaling is mandatory preprocessing in these models** for proper functioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvOlj6URuVcF"
   },
   "source": [
    "#### Working of instance-based learning\n",
    "\n",
    "Instance-based learning, aka lazy learners/memory-based learning, is a non-parametric *supervised* learning method that, instead of learning the underlying structure(explicit generalization), works by:\n",
    "\n",
    "1. First, storing all the training examples in the memory - training phase.\n",
    "2. Then, for new unseen sample, predicts target value using its relationship(similarly measure) with the stored instances - prediction phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkSNwB1guVcF"
   },
   "source": [
    "#### Characteristic of instance-based learning\n",
    "\n",
    "Unlike model-based learning models, instance-based learning models have different characteristics that are:\n",
    "\n",
    "- The hypothesis function isn't constant; the estimation of hypothesis function is proportional to the number of training samples. There are *no assumptions regarding the number of parameters of the model* that construct the hypothesis function. Consequently the name, *non-parametric models*.\n",
    "\n",
    "\n",
    "- These systems *processes training data only when unseen data is to be predicted*. So, they are referred to as *lazy learners*.\n",
    "\n",
    "Few instance-based learning algorithms are **K-nearest neighbors**(K-NN), [locally weighted regression]( https://en.wikipedia.org/wiki/Local_regression), [radial basis function]( https://en.wikipedia.org/wiki/Radial_basis_function), and [kernel machines]( https://en.wikipedia.org/wiki/Kernel_method). Among these, K-NN is the most popular algorithm that you are learning in this unit. Discussing other algorithms is beyond the scope of this course. Before learning KNN, let's discuss the advantages and disadvantages of the instance-based learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8T8ujLQuVcG"
   },
   "source": [
    "### Advantage/Disadvantage of instance-based learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqD4KlzuuVcH"
   },
   "source": [
    "#### Advantages\n",
    "\n",
    "Using similarity measure for prediction of new samples instead of approximating function, the advantages of instance-based learning are as follows:\n",
    "\n",
    "* Simple to understand, prediction related to finding similar data points along with instant training.\n",
    "\n",
    "* Works exceptionally in an \"online\" learning scenario, where the system trains incrementally using batches of continuous data instances. In this case, adding new data observed requires only an update to the database.\n",
    "\n",
    "* Acquiring new data of some class does not degrade the model performance of older instances. i.e., Due to local approximation of hypothesis, instance-based learning models have tolerance for data interference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWHqdzatuVcH"
   },
   "source": [
    "#### Disadvantages\n",
    "\n",
    "The instance-based learning model can't discard the training data. For finding a similarity measure of a new sample, these models query all the attributes/features of _all training samples_. As a result, the disadvantages are:\n",
    "\n",
    "* Requirement of a large amount of memory to store the data.\n",
    "\n",
    "* Sluggish during the query time for a large number of training data. Linear increment in computational complexity.\n",
    "\n",
    "* Due to curse of dimensionality, performance degradation for high dimensional data samples. i.e., Worse performance on a dataset with a huge number of features.\n",
    "\n",
    "That's all for the discussion on instance-based learning, now let's discuss its most popular algorithm, KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDcqcfiBPf5L"
   },
   "source": [
    "## Introduction to K-NN\n",
    "\n",
    "To better understand KNN, let's start with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7DKHmp8Q172"
   },
   "source": [
    "### Binary classification example\n",
    "\n",
    "Having the synthetic restaurant dataset present below with categorical features such as food quality, ambience, hygiene, and service, you are building an instance-based machine learning system for a binary classification problem, predicting whether the restaurant will rise to popularity or fail miserably(target variable).\n",
    "\n",
    "<center>\n",
    "Table 1: Synthetic restaurant dataset\n",
    "\n",
    "\n",
    "| Id | Food quality | Ambience | Hygiene | Service | Will fail? |\n",
    "|-|-|-|-|-|-|\n",
    "| 1. | High | Low | Medium | Low | Yes |\n",
    "| 2. | Medium | Medium | Medium | High | No |\n",
    "| 3. | Low | Low | Low | Low | Yes |\n",
    "| 4. | Medium | High | High | Medium | No |\n",
    "| 5. | Low | High | Low | High | Yes |\n",
    "| 6. | High | Medium | Low | Low | Yes |\n",
    "\n",
    "</center>\n",
    "\n",
    "Let's try to think of methods to build an ML system for the problem mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nosps_lESGFj"
   },
   "source": [
    "### Rote learning\n",
    "\n",
    "We can start by finding exact samples in the stored set. That's called **rote learning**, where the new sample requires an exact match of features with one of the stored training data. Say a new sample is as follows:\n",
    "\n",
    "<center>Table 2: Synthetic restaurant data's test sample 1\n",
    "\n",
    "| Id | Food quality | Ambience | Hygiene | Service | Will fail? |\n",
    "|-|-|-|-|-|-|\n",
    "|-| Low | High | Low | High | - |\n",
    "\n",
    "</center>\n",
    "\n",
    "This new sample exactly matches with the Id-5 of the training set. So the prediction of the new sample is the target value of Id-5, i.e., \"Yes.\" However, there is a flaw. If there isn't a matching sample in the training set, then this method fails. How can we improve this method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nV8x5-QV_QC"
   },
   "source": [
    "### Nearest neighbor algorithm\n",
    "\n",
    "**Nearest neighbor search** that you studied in the previous unit improves the rote learning model. The main assumption being **similar samples have similar targets**. After storing the training data, the nearest neighbor predictor works in the following way:\n",
    "\n",
    "1. First, find the nearest neighbor of the new sample(here, using hamming distance).\n",
    "\n",
    "2. Then, predicts the new sample's target value as that of the nearest neighbor training sample.\n",
    "\n",
    "This method is referred to as the **1-nearest neighbor**(1-NN) classifier.\n",
    "\n",
    "<center>Table 3: Synthetic restaurant data's test sample 2\n",
    "\n",
    "| Id | Food quality | Ambience | Hygiene | Service | Will fail? |\n",
    "|-|-|-|-|-|-|\n",
    "|-| Medium| Medium | Low | High | - |\n",
    "\n",
    "</center>\n",
    "\n",
    "Applying 1-NN, the closest training sample for the above new sample is Id-2, matching three features. So, the target prediction of the new sample is \"No.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "266UJcY2aHcL"
   },
   "source": [
    "### K-NN algorithm\n",
    "\n",
    "We can still improve 1-NN algorithm, let's use an analogy to understand how. When we are buying a house, we often compare the prices among multiple neighboring house(neighborhood, we will discuss more about neighbor/neighborhood in clustering unit) for predicting price, don't we? So, instead of one nearest neighbor, how about taking *arbitary(K) number of nearest neighbors* of the new sample into account for prediction? This is the **K-Nearest Neighbor(K-NN)** algorithm that generalizes nearest neighbor algorithm over multiple($K$) neighbors.\n",
    "\n",
    "K-NN is a simple, instance-based learning algorithm, used for *classification and regression problems*. The main assumption of K-NN being similar input features have similar target values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYvCXzYPuVcL"
   },
   "source": [
    "#### Working of K-NN\n",
    "\n",
    "The K-NN algorithm consists of two steps, training phase, and prediction phase, but before that user must set the hyperparameters of the K-NN model as:\n",
    "\n",
    "- Setting hyperparameters\n",
    "\n",
    "  - Choose the number of neighbors($K$) to examine and a distance function($d$) for similarity measure.\n",
    "\n",
    "Now, the training and prediction phase conducts as follows:\n",
    "\n",
    "1. Training phase\n",
    "\n",
    "  - Store the training dataset in memory. Note: Using spatial-access methods such as kd-tree, ball-tree, and storing data structure allows faster neighbor search/query.\n",
    "\n",
    "\n",
    "2. Prediction phase\n",
    "\n",
    "  1. Given a test sample, compute its \"$K$\"-nearest neighbors.\n",
    "  2. Predict the target value of the test sample:\n",
    "    - For classification\n",
    "       - **Assign the target label by majority voting**. The highest count/frequency of class labels among the K-nearest neighbors is the test sample's target label.\n",
    "       \n",
    "     - For regression\n",
    "        - **Assign the target value by aggregate scheme**. The mean of the target value of K-nearest neighbors is the target value for the test sample.\n",
    "      \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmsqoSh0uVcM"
   },
   "source": [
    "Continuing the above restaurant binary classification task, let's use the K-NN algorithm with the value of K set to 3:\n",
    "\n",
    "1. First, 3-NN computes the three nearest neighbors for the new sample 2(present in table 3): Id-2, Id-5, and Id-6(with Id-5,6 having two matching features).\n",
    "\n",
    "2. Then, assigns the target variable of the new sample as the highest count target label among the nearest neighbors,\n",
    "\n",
    "$$\\text{mode(\"No\", \"Yes\", \"Yes\")= \"Yes\"}$$\n",
    "\n",
    "**Note**: For predicting the target of a new sample without using advanced data structures, the new sample is compared with $n$ stored training samples. So, the worst computational complexity for predicting a new sample's target is $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hqa67ZwHuVcM"
   },
   "source": [
    "That's all for this notebook. In the next chapters, we will dive in-depth into K-NN classifier and regressor along with visulization, discussion on effect of K and pros/cons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yiniI1uuVcN"
   },
   "source": [
    "## Key takeaways\n",
    "\n",
    "Key-points to note from this notebook are:\n",
    "\n",
    "1. Model-based learning system learns parameters/hypothesis function from training samples. The hypothesis function predicts the target value of unseen samples.\n",
    "\n",
    "\n",
    "2. Instance-based learning, lazy-learning model is a non-parametric system that learns by memorizing training samples. Prediction of an unseen sample's target is inferred from a similar stored training sample.\n",
    "\n",
    "\n",
    "3. Instance-based learning system works for a large number of samples and is exceptional in online learning. However, these systems require large memory space, are sluggish for large feature dimensions, and suffer from the curse of dimensionality.\n",
    "\n",
    "\n",
    "4. Conducting feature scaling is important being training/prediction in instance-based learning models due to their dependency on similarity measures(distance metrics).\n",
    "\n",
    "\n",
    "5. Most popular instance-based learning algorithm is K-NN, the nearest neighbor search, simple supervised classification/regression algorithm.\n",
    "\n",
    "\n",
    "6. First, K-NN stores the training sample, then predicting the new sample's target queries the $K$-nearest neighbors. The new sample's target value is either average of its neighbors(for regression) or its neighbors' highest count label (for classification)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
