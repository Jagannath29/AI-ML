{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GJXD_qXpk9d"
      },
      "source": [
        "# UnderSampling\n",
        "\n",
        "In this reading assignment, you will learn about undersampling as one of the resampling techniques to remove the issues of imbalanced learning.\n",
        "\n",
        "\n",
        "Undersampling is a technique where we reduce the number of samples for the majority class so that they match up to the length of our minority class.It is also called downsampling.\n",
        "\n",
        "Out of various methods to undersmaple the majority class, we will talk about random undersampling throughout this course. So, undersampling here means random undersampling.\n",
        "\n",
        "Let's look at the toy example of a binary classifier with two classes 'Yes' and 'No.'\n",
        "\n",
        "We have a dataset of 1000 rows. The number of rows under the 'Yes' class is 900, and the number of rows under 'No' class is 100.\n",
        "\n",
        "\n",
        "<!-- ![alt text](https://drive.google.com/uc?export=view&id=1i71lxcIcsRv_wMzJmHttvnz6R-5Z5aFB) -->\n",
        "![alt text](https://i.postimg.cc/y8TWpbj3/image.png)\n",
        "\n",
        "\n",
        "We can see there is a clear imbalance between the classes. Whatever model you will build will be biased towards the class 'Yes' as you are feeding the data from the category 'Yes' more times than that of the 'No' class. To create the unbiased classifier, we will undersample from the 900 rows and try to reduce the rows from the majority class so that they match the minority class. The first step is always to keep reducing to keep the proportion somehow the same.\n",
        "\n",
        "The main logic behind undersampling is to shrink down the 900 rows of class Yes (Majority class)  to 100 (same as the rows of the minority class).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb2oUh3m3_61"
      },
      "source": [
        "Let's implement this in code now.\n",
        "We will take the [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud/home) dataset from Kaggle, published on 2018 for the demonstration.\n",
        "\n",
        "This dataset contains anonymized credit card transactions labeled as fradulent or genuine.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGRqZCXPpnS5",
        "outputId": "79dd3553-2505-4f13-a4c5-0e2645d837a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQQH0khQ7Agy"
      },
      "source": [
        "df = pd.read_csv('creditcard.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2sKHPDq64VT",
        "outputId": "af922da0-5b04-43d3-db87-0d0450eccd1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0     0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62    0.0\n",
              "1     0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69    0.0\n",
              "2     1 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66    0.0\n",
              "3     1 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50    0.0\n",
              "4     2 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99    0.0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoKiceCA7LYX",
        "outputId": "7269fff8-ab44-406e-8a01-1b40273ee1db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "df.Class.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    9926\n",
              "1.0      38\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjYKBmjIAPtH"
      },
      "source": [
        "Here, we can see the difference between the counts of majority class (Class 0, Non Fraud) and minority class (Class 1, Fraud) which is why this is imbalanced dataset.\n",
        "\n",
        "Let's visualise this in the bar diagram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBTE4qzH7Pet",
        "outputId": "0a8f720a-d25e-4176-c26e-c7f81084224b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "source": [
        "df.Class.value_counts().plot(kind='bar', title = 'Data Distribution')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fcaa5e84550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEOCAYAAABrSnsUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAASqUlEQVR4nO3df5Bd5V3H8fdHItRKS5Kyk0KSNlhiFepQMYU4HbUjHX62hj9sS0WJNRq1qNVxbEE7DeWHUscpFW2pmQYaWiQwWCVtUYy0qFX5EUoLAmJWWkxSfiwkhFIKkvL1j/tsuWx3SXbvZu+y+37N3Nlznuc5z/luJrOfe5577r2pKiRJs9v39bsASVL/GQaSJMNAkmQYSJIwDCRJGAaSJAwDaVyS/H2SlZM0108lubdr/+tJ3jwZc7f57krypsmaTzObYaBpof0h/HaSbyZ5LMm/J/mNJHv1fzTJkiSVZE4PNVSSbyV5IsmjSW5I8o7uMVV1UlWt38u5Dn+hMVX1r1X12onWO+J8n0xy/oj5j6yqGydjfs18hoGmk7dW1cuAVwMXAu8D1k1xDUdV1YHAa4FPAn+ZZM1kn6SX0JL2BcNA005V7aqqjcA7gJVJXgeQ5JQktyd5PMnWJOd0HfYv7edj7Zn9TyZ5TZIvtGf5jyS5Isncvazhkar6FPCbwNlJXtFquDHJr7btw5P8c5Jdbf6rWvtwLV9ttbwjyZuSbEvyviQPApcNt4049RuS3J1kZ5LLkrykzfnLSb7UPXD46iPJauB04L3tfJ9t/d9ddkpyQJKPJPlGe3wkyQGtb7i230/ycJIHkrxrb/6dNHMYBpq2quoWYBvwU63pW8AZwFzgFOA3k5za+n66/ZxbVQdW1X8AAf4EOBT4UWAxcM44y7gWmAMcM0rfecA/AvOARcBftLqHazmq1XJV238lMJ/Olc/qMc53OnAC8Brgh4H376nAqloLXAH8aTvfW0cZ9kfAcuD1wFHt9+me+5XAQcBCYBXw0STz9nRuzRyGgaa7b9D5A0pV3VhVd1bVs1V1B3Al8DNjHVhVg1W1qaqerqoh4MMvNH6MOZ4BHhmuYYRn6PxhP7SqnqqqL40yptuzwJpWz7fHGPOXVbW1qnYAFwDvHE+9L+B04Nyqerj9W3wQ+KWu/mda/zNVdR3wBJ2lMs0ShoGmu4XADoAkxyb5YpKhJLuA3wAOHuvAJAuSbEiyPcnjwKdfaPwYc3w/MDBcwwjvpXP1cUu7c+dX9jDdUFU9tYcxW7u276dzVTMZDm3zjTX3o1W1u2v/SeDASTq3XgQMA01bSd5AJwyGn3H/NbARWFxVBwEfp/PHGGC0j9/949b+Y1X1cuAXu8bvrRXAbuCWkR1V9WBV/VpVHQr8OvCxPdxBtDcfEby4a/tVdK6MoLNE9tLhjiSvHOfc36BzFTPa3JJhoOknycuTvAXYAHy6qu5sXS8DdlTVU0mOAX6h67AhOsswP9TV9jI6yx27kiwE/mAcNcxPcjrwUeBDVfXoKGPelmRR291J5w/ys23/oRG17K0zkyxKMp/OOv/w6w1fBY5M8vr2ovI5I47b0/muBN6fZCDJwcAH6FwpSYBhoOnls0m+SWep5I/orPF339XybuDcNuYDwNXDHVX1JJ019n9r71NYTmdd/GhgF/B54DN7UcNXkzwBDAK/CvxeVX1gjLFvAG5u4zcC76mq+1rfOcD6Vsvb9+K8w/6azovS9wH/A5zffr//Bs4F/gnYwnNXS8PWAUe08/3dKPOeD2wG7gDuBL48PLcEEL/cRpLklYEkyTCQJBkGkiQMA0kSnbfZvygdfPDBtWTJkn6XIUkvGrfddtsjVTUwWt+LNgyWLFnC5s2b+12GJL1oJLl/rD6XiSRJhoEkyTCQJGEYSJLYizBIcmn79qP/7Gqbn2RTki3t57zWniQXJxlMckeSo7uOWdnGb0nXF4on+Ykkd7ZjLk4y3k+VlCT1aG+uDD4JnDii7SzghqpaCtzQ9gFOApa2x2rgEuiEB7AGOJbONyyt6foWpUuAX+s6buS5JEn72B7DoKr+he/9Yo8VwPq2vR44tav98uq4CZib5BA6X+O3qap2VNVOYBNwYut7eVXdVJ1PzLu8ay5J0hSZ6GsGC6rqgbb9ILCgbS/k+d/UtK21vVD7tlHaR5VkdZLNSTYPDQ1NsHRJ0kg9v4DcntFPyedgV9XaqlpWVcsGBkZ9E50kaQIm+g7kh5IcUlUPtKWeh1v7dp7/tX2LWtt24E0j2m9s7YtGGT8jLDnr8/0uYUb5+oWn9LsEacaa6JXBRmD4jqCVwLVd7We0u4qWA7vactL1wPFJ5rUXjo8Hrm99jydZ3u4iOqNrLknSFNnjlUGSK+k8qz84yTY6dwVdCFydZBVwPzD8tX7XASfT+crAJ2lfWVhVO5KcB9zaxp1bVcMvSr+bzh1LPwD8fXtIkqbQHsOgqt45Rtdxo4wt4Mwx5rkUuHSU9s3A6/ZUhyRp3/EdyJIkw0CSZBhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJoscwSPJ7Se5K8p9JrkzykiSHJbk5yWCSq5Ls38Ye0PYHW/+SrnnObu33Jjmht19JkjReEw6DJAuB3wGWVdXrgP2A04APARdV1eHATmBVO2QVsLO1X9TGkeSIdtyRwInAx5LsN9G6JEnj1+sy0RzgB5LMAV4KPAD8LHBN618PnNq2V7R9Wv9xSdLaN1TV01X1NWAQOKbHuiRJ4zDhMKiq7cCfAf9LJwR2AbcBj1XV7jZsG7CwbS8EtrZjd7fxr+huH+WY50myOsnmJJuHhoYmWrokaYRelonm0XlWfxhwKPCDdJZ59pmqWltVy6pq2cDAwL48lSTNKr0sE70Z+FpVDVXVM8BngDcCc9uyEcAiYHvb3g4sBmj9BwGPdrePcowkaQr0Egb/CyxP8tK29n8ccDfwReDn25iVwLVte2Pbp/V/oaqqtZ/W7jY6DFgK3NJDXZKkcZqz5yGjq6qbk1wDfBnYDdwOrAU+D2xIcn5rW9cOWQd8KskgsIPOHURU1V1JrqYTJLuBM6vqOxOtS5I0fhMOA4CqWgOsGdF8H6PcDVRVTwFvG2OeC4ALeqlFkjRxvgNZkmQYSJIMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkugxDJLMTXJNkv9Kck+Sn0wyP8mmJFvaz3ltbJJcnGQwyR1Jju6aZ2UbvyXJyl5/KUnS+PR6ZfDnwD9U1Y8ARwH3AGcBN1TVUuCGtg9wErC0PVYDlwAkmQ+sAY4FjgHWDAeIJGlqTDgMkhwE/DSwDqCq/q+qHgNWAOvbsPXAqW17BXB5ddwEzE1yCHACsKmqdlTVTmATcOJE65IkjV8vVwaHAUPAZUluT/KJJD8ILKiqB9qYB4EFbXshsLXr+G2tbax2SdIU6SUM5gBHA5dU1Y8D3+K5JSEAqqqA6uEcz5NkdZLNSTYPDQ1N1rSSNOv1EgbbgG1VdXPbv4ZOODzUln9oPx9u/duBxV3HL2ptY7V/j6paW1XLqmrZwMBAD6VLkrpNOAyq6kFga5LXtqbjgLuBjcDwHUErgWvb9kbgjHZX0XJgV1tOuh44Psm89sLx8a1NkjRF5vR4/G8DVyTZH7gPeBedgLk6ySrgfuDtbex1wMnAIPBkG0tV7UhyHnBrG3duVe3osS5J0jj0FAZV9RVg2Shdx40ytoAzx5jnUuDSXmqRJE2c70CWJBkGkiTDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKTEAZJ9ktye5LPtf3DktycZDDJVUn2b+0HtP3B1r+ka46zW/u9SU7otSZJ0vhMxpXBe4B7uvY/BFxUVYcDO4FVrX0VsLO1X9TGkeQI4DTgSOBE4GNJ9puEuiRJe6mnMEiyCDgF+ETbD/CzwDVtyHrg1La9ou3T+o9r41cAG6rq6ar6GjAIHNNLXZKk8en1yuAjwHuBZ9v+K4DHqmp3298GLGzbC4GtAK1/Vxv/3fZRjnmeJKuTbE6yeWhoqMfSJUnDJhwGSd4CPFxVt01iPS+oqtZW1bKqWjYwMDBVp5WkGW9OD8e+Efi5JCcDLwFeDvw5MDfJnPbsfxGwvY3fDiwGtiWZAxwEPNrVPqz7GEnSFJjwlUFVnV1Vi6pqCZ0XgL9QVacDXwR+vg1bCVzbtje2fVr/F6qqWvtp7W6jw4ClwC0TrUuSNH69XBmM5X3AhiTnA7cD61r7OuBTSQaBHXQChKq6K8nVwN3AbuDMqvrOPqhLkjSGSQmDqroRuLFt38codwNV1VPA28Y4/gLggsmoRZI0fr4DWZJkGEiSDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJ9BAGSRYn+WKSu5PcleQ9rX1+kk1JtrSf81p7klycZDDJHUmO7pprZRu/JcnK3n8tSdJ49HJlsBv4/ao6AlgOnJnkCOAs4IaqWgrc0PYBTgKWtsdq4BLohAewBjgWOAZYMxwgkqSpMeEwqKoHqurLbfubwD3AQmAFsL4NWw+c2rZXAJdXx03A3CSHACcAm6pqR1XtBDYBJ060LknS+E3KawZJlgA/DtwMLKiqB1rXg8CCtr0Q2Np12LbWNlb7aOdZnWRzks1DQ0OTUbokiUkIgyQHAn8D/G5VPd7dV1UFVK/n6JpvbVUtq6plAwMDkzWtJM16PYVBku+nEwRXVNVnWvNDbfmH9vPh1r4dWNx1+KLWNla7JGmK9HI3UYB1wD1V9eGuro3A8B1BK4Fru9rPaHcVLQd2teWk64Hjk8xrLxwf39okSVNkTg/HvhH4JeDOJF9pbX8IXAhcnWQVcD/w9tZ3HXAyMAg8CbwLoKp2JDkPuLWNO7eqdvRQlyRpnCYcBlX1JSBjdB83yvgCzhxjrkuBSydaiySpN74DWZJkGEiSDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJTKMwSHJiknuTDCY5q9/1SNJsMi3CIMl+wEeBk4AjgHcmOaK/VUnS7DGn3wU0xwCDVXUfQJINwArg7r5WJc1gS876fL9LmFG+fuEp/S6hJ9MlDBYCW7v2twHHjhyUZDWwuu0+keTeKahtNjgYeKTfRexJPtTvCtQn/v+cPK8eq2O6hMFeqaq1wNp+1zHTJNlcVcv6XYc0Gv9/To1p8ZoBsB1Y3LW/qLVJkqbAdAmDW4GlSQ5Lsj9wGrCxzzVJ0qwxLZaJqmp3kt8Crgf2Ay6tqrv6XNZs4tKbpjP/f06BVFW/a5Ak9dl0WSaSJPWRYSBJMgwkSYbBrJZkfpL5/a5DUv8ZBrNMklcl2ZBkCLgZuCXJw61tSX+rk9QvhsHscxXwt8Arq2ppVR0OHAL8HbChr5VJTZIFSY5ujwX9rmc28NbSWSbJlqpaOt4+aSokeT3wceAgnvsUgkXAY8C7q+rL/aptpjMMZpn2ibA7gPU89+GAi4GVwMFV9fZ+1SYl+Qrw61V184j25cBfVdVR/als5jMMZpn2cR+r6HxE+MLWvA34LLCuqp7uV23SHq5cB9uypvYBw0DStJHkYuA1wOU8/8r1DOBrVfVb/aptpjMM9F1J3lJVn+t3HZrdkpzE869ctwMbq+q6/lU18xkG+q4kH6yqNf2uQ9LUMwxmoSQ/wujPvO7pX1XSC0uyun3BlfYB32cwyyR5H533EwS4pT0CXJnkrH7WJu1B+l3ATOaVwSyT5L+BI6vqmRHt+wN3+T4DTVdJ3lVVl/W7jpnKK4PZ51ng0FHaD2l90nT1wX4XMJNNi28605T6XeCGJFt47ta9VwGHA962p75KcsdYXYAfS7EPuUw0CyX5PuAYnv8C8q1V9Z3+VSVBkoeAE4CdI7uAf6+q0a5qNQm8MpiFqupZ4KZ+1yGN4nPAgVX1lZEdSW6c+nJmD68MJEm+gCxJMgwkSRgGkiQMA0kS8P9ZHFJKA2Km6wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y447Jioq9rbx"
      },
      "source": [
        "From this plot, we can see that we have a very imbalanced dataset, i.e., just about 99 % of the dataset belongs to the Non-Fraud class.\n",
        "\n",
        "Many machine learning models are designed to maximize overall accuracy or minimize overall error rate, while in the case of Imbalalanced learning, things are not the same. Suppose if in this example, even if we classify every entry to the non-fraud class, we will get an accuracy of 99 %, but this is not what we want"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RITzgB54IKJ5"
      },
      "source": [
        "### Undersampling Majority Samples\n",
        "\n",
        "Undersampling can be the right choice when we have a ton of data - think million of rows. But we have a drawback, i.e., removal of information which may be valuable.\n",
        "\n",
        "We will use the resampling module from Scikit Learn to randomly remove samples from the majority class, i.e., class 0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUHjh0kdJEuA"
      },
      "source": [
        "from sklearn.utils import resample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqq9Z_xdFDpn"
      },
      "source": [
        "# Separate input features and target\n",
        "y = df.Class\n",
        "X = df.drop('Class', axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHh5iWKjDq6C",
        "outputId": "7e535b0d-f6b7-43cc-c91a-88ff3d029842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "X = pd.concat([X, y], axis=1)\n",
        "X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0     0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62    0.0\n",
              "1     0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69    0.0\n",
              "2     1 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66    0.0\n",
              "3     1 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50    0.0\n",
              "4     2 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99    0.0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_rV-tNt-pWt"
      },
      "source": [
        "not_fraud = X[X.Class==0]\n",
        "fraud = X[X.Class==1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtVf45dd-0kH",
        "outputId": "072ca073-4adf-449c-e005-e19075f136d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# upsample minority\n",
        "not_fraud_downsampled = resample(not_fraud,\n",
        "                          replace=True, # sample with replacement\n",
        "                          n_samples=len(fraud), # match number in majority class\n",
        "                          random_state=27) # reproducible results\n",
        "\n",
        "# combine majority and upsampled minority\n",
        "downsampled = pd.concat([fraud, not_fraud_downsampled])\n",
        "\n",
        "# check new class counts\n",
        "downsampled.Class.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    38\n",
              "1.0    38\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUueP4q0At6c"
      },
      "source": [
        "Here, we have reduced the number of samples from the majority class(Class 0, Non-Fraud) to the number same as that of minority class (Class 1, Fraud) and use this downsampled data to train the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAdXHhRAvJeZ"
      },
      "source": [
        "__Merits of undersampling__\n",
        "\n",
        "- It improves the run time and storage problems since the number of samples are reduced.\n",
        "\n",
        "__Demerits of undersampling__\n",
        "\n",
        "- It discards the potentially useful information which could be important for building rule classifiers.\n",
        "\n",
        "- Randomly selecting the samples to undersample may be the biased one which can result in inaccurate results with the actual test data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHFJqnH_xd24"
      },
      "source": [
        "You will see the use of undersampling technique in a ML model and how it actually alters the performance with different evaluation metrics in **Chapter 6:Imbalance-Learn**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bb1qsYlA8sh"
      },
      "source": [
        "## UnderSampling Vs OverSampling\n",
        "\n",
        "From an example demonstrated above, we are now familiar with Undersampling.\n",
        "\n",
        "As stated earlier in the *Introduction* chapter, Undersampling and oversampling are some of the significant approaches to random resampling for imbalanced classification. Random Undersampling randomly delete examples in the majority class while Random Oversampling randomly duplicates example in the minority class. Random oversampling involves randomly selecting samples from the minority class, with replacement, and adding them to the training dataset while on the other hand, Undersampling involves randomly selecting examples from the majority class and deleting them from the training dataset.\n",
        "\n",
        "Here, both resampling techniques can be repeated until the desired class distribution is achieved in the training dataset, such as equal split across the classes.\n",
        "\n",
        "Both these techniques fall under 'Naive Resampling' since they assume nothing about the data, and no heuristics are used. That's why they are fast and straightforward to execute, which is suitable for huge and complex datasets.\n",
        "\n",
        "\n",
        "While random Undersampling suffered from the loss of potentially useful information, random oversampling suffers from the problem of overfitting.\n",
        "\n",
        "We can get interesting results by combining both oversampling and Undersampling. Common examples include SMOTE or SMOTE with ENN. We will learn these hybrid algorithms in the upcoming chapters.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5ptSZeRO_oA"
      },
      "source": [
        "## Key Takeaways\n",
        "Here, in this chapter, we visited one of the approaches to resolve the problems created due to the Imbalanced Dataset. Random Undersampling falls under Resampling techniques, which deals with the concept of randomly removing samples from the majority class, with or without replacement. This is one of the earliest methods used to alleviate the imbalance in the dataset; it may decrease the variance of the classifier and may potentially remove the useful samples and information embedded within them.\n",
        "\n",
        "Then after, we compared Undersampling and Oversampling in terms of how they remove the imbalance within the majority and minority classes. Contrary to Undersampling, Random Oversampling involves supplementing the training data with multiple copies of the minority classes."
      ]
    }
  ]
}