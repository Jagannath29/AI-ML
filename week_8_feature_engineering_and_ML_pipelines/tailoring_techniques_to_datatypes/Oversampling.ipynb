{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTU42RP3ZQ5O"
      },
      "source": [
        "# Oversampling\n",
        "\n",
        "By the end of this chapter you will be able to learn about another resampling technique, oversampling with some of it's extensions.\n",
        "\n",
        "One of the ways to fight with the issues of imbalanced learning is to generate new samples in the classes, which are underrepresented. The most naive strategy is to create new samples by randomly sampling with replacement of the currently available samples.\n",
        "\n",
        "\n",
        "Let's use the same credit card dataset for the illustration of oversampling (Random oversampling).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "PLTmeKIff3oh",
        "outputId": "b256a247-6359-4ee6-ef56-ceed80cebaa2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "WziDklswf814",
        "outputId": "57ffb545-e3af-4c1c-eaed-39d263278bfc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0    98898\n",
              "1.0      222\n",
              "Name: Class, dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('creditcard.csv')\n",
        "df.Class.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "LorTYWkMf84V",
        "outputId": "b5224b6a-f677-4ecf-8526-6e98b9e3dd9d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, ax = plt.subplots()\n",
        "g = sns.countplot(df.Class, palette='viridis')\n",
        "g.set_xticklabels(['Not Fraud', 'Fraud'])\n",
        "\n",
        "\n",
        "# function to show values on bars\n",
        "def show_values_on_bars(axs):\n",
        "    def _show_on_single_plot(ax):\n",
        "        for p in ax.patches:\n",
        "            _x = p.get_x() + p.get_width() / 2\n",
        "            _y = p.get_y() + p.get_height()\n",
        "            value = '{:.0f}'.format(p.get_height())\n",
        "            ax.text(_x, _y, value, ha=\"center\")\n",
        "\n",
        "    if isinstance(axs, np.ndarray):\n",
        "        for idx, ax in np.ndenumerate(axs):\n",
        "            _show_on_single_plot(ax)\n",
        "    else:\n",
        "        _show_on_single_plot(axs)\n",
        "\n",
        "show_values_on_bars(ax)\n",
        "\n",
        "sns.despine(left=True, bottom=True)\n",
        "plt.xlabel('')\n",
        "plt.ylabel('')\n",
        "plt.title('Distribution of Transactions', fontsize=19)\n",
        "plt.tick_params(axis='x', which='major', labelsize=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBWV5Bd3f88x"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24fMvx38iYN0"
      },
      "outputs": [],
      "source": [
        "# Separate input features and target\n",
        "y = df.Class\n",
        "X = df.drop('Class', axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "0UqfmyNniYS0",
        "outputId": "4a3bdce7-427b-4194-826c-42cc26e8ab70"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>38632</th>\n",
              "      <td>39473</td>\n",
              "      <td>-0.367265</td>\n",
              "      <td>1.003264</td>\n",
              "      <td>1.058041</td>\n",
              "      <td>0.634395</td>\n",
              "      <td>0.478383</td>\n",
              "      <td>0.133001</td>\n",
              "      <td>0.542855</td>\n",
              "      <td>0.162472</td>\n",
              "      <td>-0.278744</td>\n",
              "      <td>-0.057422</td>\n",
              "      <td>-1.140486</td>\n",
              "      <td>-0.898883</td>\n",
              "      <td>-0.999888</td>\n",
              "      <td>0.466827</td>\n",
              "      <td>1.528735</td>\n",
              "      <td>-0.304064</td>\n",
              "      <td>-0.130449</td>\n",
              "      <td>0.122280</td>\n",
              "      <td>0.286039</td>\n",
              "      <td>0.067017</td>\n",
              "      <td>0.109312</td>\n",
              "      <td>0.471196</td>\n",
              "      <td>-0.252427</td>\n",
              "      <td>-0.664045</td>\n",
              "      <td>-0.097668</td>\n",
              "      <td>-0.185811</td>\n",
              "      <td>0.415347</td>\n",
              "      <td>0.200582</td>\n",
              "      <td>5.90</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47905</th>\n",
              "      <td>43409</td>\n",
              "      <td>-1.691728</td>\n",
              "      <td>-1.621100</td>\n",
              "      <td>0.641290</td>\n",
              "      <td>-2.690706</td>\n",
              "      <td>-1.849708</td>\n",
              "      <td>-0.974309</td>\n",
              "      <td>0.594209</td>\n",
              "      <td>0.271848</td>\n",
              "      <td>-0.027033</td>\n",
              "      <td>-1.518221</td>\n",
              "      <td>-1.153751</td>\n",
              "      <td>-0.092356</td>\n",
              "      <td>-0.458316</td>\n",
              "      <td>0.210699</td>\n",
              "      <td>0.244318</td>\n",
              "      <td>-1.708103</td>\n",
              "      <td>0.154639</td>\n",
              "      <td>1.572906</td>\n",
              "      <td>-0.747027</td>\n",
              "      <td>0.245026</td>\n",
              "      <td>0.022969</td>\n",
              "      <td>-0.312588</td>\n",
              "      <td>0.767806</td>\n",
              "      <td>0.343325</td>\n",
              "      <td>-0.113248</td>\n",
              "      <td>-0.231492</td>\n",
              "      <td>-0.086751</td>\n",
              "      <td>0.002726</td>\n",
              "      <td>358.30</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70751</th>\n",
              "      <td>54001</td>\n",
              "      <td>-1.869899</td>\n",
              "      <td>-0.401754</td>\n",
              "      <td>1.506120</td>\n",
              "      <td>-0.121822</td>\n",
              "      <td>1.205160</td>\n",
              "      <td>-1.740027</td>\n",
              "      <td>0.376592</td>\n",
              "      <td>-0.097288</td>\n",
              "      <td>-0.325536</td>\n",
              "      <td>-0.824835</td>\n",
              "      <td>-0.365298</td>\n",
              "      <td>-0.195884</td>\n",
              "      <td>-0.175036</td>\n",
              "      <td>-0.415398</td>\n",
              "      <td>0.513039</td>\n",
              "      <td>0.717365</td>\n",
              "      <td>-0.304062</td>\n",
              "      <td>-0.146657</td>\n",
              "      <td>-0.759373</td>\n",
              "      <td>-0.190122</td>\n",
              "      <td>-0.236899</td>\n",
              "      <td>-0.680860</td>\n",
              "      <td>0.743500</td>\n",
              "      <td>0.303036</td>\n",
              "      <td>0.120165</td>\n",
              "      <td>-0.045695</td>\n",
              "      <td>-0.041088</td>\n",
              "      <td>0.124162</td>\n",
              "      <td>1.29</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86662</th>\n",
              "      <td>61342</td>\n",
              "      <td>-0.659883</td>\n",
              "      <td>0.958209</td>\n",
              "      <td>0.924674</td>\n",
              "      <td>0.956823</td>\n",
              "      <td>-0.153232</td>\n",
              "      <td>0.664856</td>\n",
              "      <td>-0.446185</td>\n",
              "      <td>0.733530</td>\n",
              "      <td>-0.456112</td>\n",
              "      <td>0.069876</td>\n",
              "      <td>1.423374</td>\n",
              "      <td>0.961256</td>\n",
              "      <td>0.159049</td>\n",
              "      <td>0.610445</td>\n",
              "      <td>1.169320</td>\n",
              "      <td>-0.388365</td>\n",
              "      <td>0.163111</td>\n",
              "      <td>0.141162</td>\n",
              "      <td>0.000807</td>\n",
              "      <td>-0.220236</td>\n",
              "      <td>0.536499</td>\n",
              "      <td>1.482456</td>\n",
              "      <td>0.083840</td>\n",
              "      <td>-0.254299</td>\n",
              "      <td>-1.356347</td>\n",
              "      <td>-0.342630</td>\n",
              "      <td>-0.033175</td>\n",
              "      <td>0.148644</td>\n",
              "      <td>2.37</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48754</th>\n",
              "      <td>43773</td>\n",
              "      <td>0.780559</td>\n",
              "      <td>-0.610152</td>\n",
              "      <td>1.104996</td>\n",
              "      <td>1.712299</td>\n",
              "      <td>-0.755186</td>\n",
              "      <td>0.973725</td>\n",
              "      <td>-0.437642</td>\n",
              "      <td>0.377361</td>\n",
              "      <td>0.927653</td>\n",
              "      <td>-0.318997</td>\n",
              "      <td>0.647050</td>\n",
              "      <td>1.847792</td>\n",
              "      <td>-0.006091</td>\n",
              "      <td>-0.579812</td>\n",
              "      <td>-2.423487</td>\n",
              "      <td>-0.947803</td>\n",
              "      <td>0.509033</td>\n",
              "      <td>-0.870863</td>\n",
              "      <td>0.419865</td>\n",
              "      <td>0.092578</td>\n",
              "      <td>-0.279710</td>\n",
              "      <td>-0.649971</td>\n",
              "      <td>-0.049601</td>\n",
              "      <td>0.028305</td>\n",
              "      <td>0.357764</td>\n",
              "      <td>-0.541674</td>\n",
              "      <td>0.064602</td>\n",
              "      <td>0.042238</td>\n",
              "      <td>143.39</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "38632  39473 -0.367265  1.003264  1.058041  ...  0.415347  0.200582    5.90    0.0\n",
              "47905  43409 -1.691728 -1.621100  0.641290  ... -0.086751  0.002726  358.30    0.0\n",
              "70751  54001 -1.869899 -0.401754  1.506120  ... -0.041088  0.124162    1.29    0.0\n",
              "86662  61342 -0.659883  0.958209  0.924674  ... -0.033175  0.148644    2.37    0.0\n",
              "48754  43773  0.780559 -0.610152  1.104996  ...  0.064602  0.042238  143.39    0.0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = pd.concat([X, y], axis=1)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGf875whih97"
      },
      "outputs": [],
      "source": [
        "not_fraud = X[X.Class==0]\n",
        "fraud = X[X.Class==1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "H_DZ5PhEiiCr",
        "outputId": "2e2cface-1f41-4354-d5e5-45635e35a661"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0    74166\n",
              "0.0    74166\n",
              "Name: Class, dtype: int64"
            ]
          },
          "execution_count": 12,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# upsample minority\n",
        "fraud_upsampled = resample(fraud,\n",
        "                          replace=True, # sample with replacement\n",
        "                          n_samples=len(not_fraud), # match number in majority class\n",
        "                          random_state=27) # reproducible results\n",
        "\n",
        "# combine majority and upsampled minority\n",
        "upsampled = pd.concat([not_fraud, fraud_upsampled])\n",
        "\n",
        "# check new class counts\n",
        "upsampled.Class.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-6gN2BfN-1K"
      },
      "source": [
        "When we randomly replicate/duplicate instances in the dataset, the learned model might fit the training data too closely and as a result not generalise well to the unseen data.In order to get rid of this issue, a method of generating synthetic instances instead of merely copying existing instances in the dataset. This is *Synthetic Minority Over-Sampling Technique*, SMOTE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF73Xwq6Qk7_"
      },
      "source": [
        "__Note__ : You should always split into “test” and “train” sets before trying oversampling techniques. Oversampling before splitting the data can allow the exact same observations to be present in both the “test” and “train” sets. In addition, as the positive labels are duplicated many times, it is very common to have the model overfit the training data when using random oversampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2uvhjIwwMRe"
      },
      "source": [
        "__Merits of Oversampling__\n",
        "\n",
        "- Unlike downsampling, oversampling methods leads to no information loss since it doesn't discard any samples.\n",
        "\n",
        "- It performs relatively better than undersampling.\n",
        "\n",
        "__Demerits of Overrsampling__\n",
        "- Likelihood of overfitting is increased since it replicates/duplicates the minority class samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4xIIDDOZekE"
      },
      "source": [
        "## SMOTE\n",
        "\n",
        "\n",
        "Synthetic Minority Oversampling Technique, SMOTE is one of the most widely used approaches to synthesize new examples. It works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space, and drawing a new sample at a point along that line.\n",
        "\n",
        "SMOTE first selects a minority class example, `a` at random and finds its k nearest minority class neighbors. The synthetic instance is then created by choosing one of the k nearest neighbors `b` at random and connecting `a` and `b` to form a line segment in the feature space. The synthetic instances are generated as a convex combination of both instances `a` and `b`.\n",
        "We can refer to this as a type of data augmentation, which has proved to be a very effective one for tabular data.\n",
        "\n",
        "\n",
        "<center>\n",
        "<!-- <img src = 'https://drive.google.com/uc?export=view&id=10hCt5ipM8zMeo94OjraoXqkegIai-2xU'height =\"300\" width=\"650\"/> -->\n",
        "\n",
        "<img src = 'https://i.postimg.cc/GtZ7yDBr/image.png' height=\"300\" width=\"650\"/>\n",
        "</center>\n",
        "\n",
        "However, the algorithm has some weaknesses dealing with imbalance and noise.\n",
        "One such drawback stems from the fact that SMOTE randomly chooses a minority instance to oversample\n",
        "with uniform probability. While this allows the method to effectively combat between-class imbalance,\n",
        "the issues of within-class imbalance and small disjuncts are ignored. Input areas counting many minority\n",
        "samples have a high probability of being inflated further, while sparsely populated minority areas are\n",
        "likely to remain sparse.\n",
        "\n",
        "\n",
        "Another major concern is that SMOTE may further amplify noise present in the data. This is likely\n",
        "to happen when linearly interpolating a noisy minority sample, which is located among majority class\n",
        "instances, and its nearest minority neighbor. The method is susceptible to noise generation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctE3mePUxr75"
      },
      "source": [
        "__Merits of SMOTE__\n",
        "\n",
        "- It reduces the problem of overfitting as synthetic samples are generate rather than duplication.\n",
        "\n",
        "- No loss of important information.\n",
        "\n",
        "__Demerits of SMOTE__\n",
        "\n",
        "- Overlapping of classes increases which leads to additional noise since SMOTE doesn't consider neighbouring examples from other classes.\n",
        "\n",
        "- It does't work well with high dimensional data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZkPC5W2PJS3"
      },
      "source": [
        "There are several hybrid techniques that have been developed by combining oversampling and undersampling such as:\n",
        "\n",
        "> SMOTE + Tomek\n",
        "\n",
        "\n",
        ">SMOTE + ENN\n",
        "\n",
        "Although over sampling class examples balance class distributions, some other problems usually present in data sets with skewed class distributions are not solved. Class clusters are not well defined since some majority class examples might be invading the minority class space. The opposite can also be true, since interpolating minority class examples can expand the minority class clusters. If we induce a classifier under such a situation can lead to overfitting. So we propose applying Tomek links to the over sampled training set as a data cleaning method.\n",
        "\n",
        "Tomek Links refers to a method for identifying pairs of nearest neighbors in a dataset that have different classes. Removing one or both of the examples in these pairs (such as the examples in the majority class) has the effect of making the decision boundary in the training dataset less noisy or ambiguous.\n",
        "\n",
        "Instead of avoiding only the majority class examples or minority class examples from Tomek links, examples from both classes can also be removed removed.\n",
        "Here, minority class examples that participate of a Tomek link are removed.\n",
        "We can see the application in the figure below. *Fig a* is the original dataset, *Fig b* is oversampling with SMOTE. *Fig c* depicts the identification of Tomek links whereas *Fig d* is producing a balanced dataset with well defined class clusters. So,  this is SMOTE +Tomek.\n",
        "\n",
        "\n",
        "\n",
        "Next we have SMOTE +ENN which is similar to SMOTE+Tomek links. ENN tends to remove more examples that it will provide a more in depth data cleaning.\n",
        "\n",
        "\n",
        "<!-- ![alt text](https://drive.google.com/uc?export=view&id=1YxRByMVomWCPmfPftd7_bZKnIzNuSPXy) -->\n",
        "![alt text](https://i.postimg.cc/K8JFXSML/image.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuB4b1Prwylq"
      },
      "source": [
        "You will see the use of these techniques in a ML model and how it actually alters the performance with different evaluation metrics in **Chapter 6:Imbalance-Learn**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBCn-kYKZnU4"
      },
      "source": [
        "## Key Takeaways\n",
        "Here in this chapter, we came across the oversampling technique to resolve the challenges that come with imbalanced data.\n",
        "\n",
        "Also, there is a synthetic Minority oversampling technique that avoids the overfitting problem created by naive oversampling. By SMOTE, redundant instances are reduced, and better performance is achieved.  There are two improvements in SMOTE, i.e., SMOTE + Tomek and SMOTE + ENN.\n",
        "\n",
        "You can also define a sequence of oversampling and undersampling methods to be applied to imbalanced dataset or when evaluating a classifier model. You can manually combine different oversampling and undersampling techniques to get better performance of the model.\n",
        "\n",
        "Learn more about this combination of the resampling techniques [here.](https://machinelearningmastery.com/combine-oversampling-and-undersampling-for-imbalanced-classification/)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
