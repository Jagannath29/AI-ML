{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1DAfIzR1Vpp"
   },
   "source": [
    "# Data Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S747ROhf1vdH"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DtALIY9D1wEk"
   },
   "source": [
    "Real-life data are enormous, even colossal. Never think of using them directly in your project, unless you have some algorithms that can crunch the data as a whole. Practical applications use several techniques to reduce the feature dimension of the given data. Once reduced, you can analyze and visualize data to extract information and discover knowledge. \n",
    "\n",
    "There are several data reduction techniques. You have already discussed sampling and how it is useful for understanding data and its statistics. In this section, we have strategies like:\n",
    "* Dimensionality Reduction,\n",
    "* Attribute Subset Selection,\n",
    "* Numerosity Reduction,\n",
    "* and Data cube Aggregation\n",
    "\n",
    "**Note**: _Features and attributes are interchangeable throughout this document_\n",
    "\n",
    "\n",
    "#### Learning Outcomes\n",
    "1. Recall different dimensionality reduction technique\n",
    "2. Understand different attribute subset selection techniques\n",
    "3. Understand to apply histogram as well as clustering as the data reduction technique\n",
    "4. Learn about data cube and its function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEw9j_Ki1YNI"
   },
   "source": [
    "## Dimensionality Reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1MtT4IuS1tDl"
   },
   "source": [
    "As you know there are several techniques that are associated with dimensionality reduction. In the previous section (Module 3, Unit 2), you familiarized yourself with concepts like Principal Component Analysis (PCA), and Singular Value Decomposition (SVD). \n",
    "\n",
    " You can find the techniques used for reducing the dimension of data in the list below:\n",
    "\n",
    "* Principal Component Analysis (PCA)  \n",
    "PCA reduces dimension of you data retaining the variation present in the data, up to the maximum extent.\n",
    "\n",
    "* Singular Value Decomposition (SVD)  \n",
    "SVD allows you to dissect a matrix into smaller matrices that extracts the features into a new axes. These new axes has different variances present in the data.\n",
    "  \n",
    "* Linear Discriminant Analysis (LDA)  \n",
    "Unlike PCA, LDA maximizes the seperatibility between all the known catagories. \n",
    "\n",
    "* Autoencoders  \n",
    "Unsupervised method for learning data encoding. Unlike PCA can model non-linearity present within the data.\n",
    "\n",
    "* Non-negative Matrix Factorization  \n",
    "Similar to SVD but find only positive values. Mostly used in recommendation systems\n",
    "\n",
    "* Independent Component Analysis (ICA)  \n",
    "ICA can separate independent non-guassian signals from a multivariate signals. As a result, it can filter the information from cluttered data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cc7UrNgJ1cJ3"
   },
   "source": [
    "## Attribute Subset Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wLH8yWpS8BYh"
   },
   "source": [
    "You can call this Feature Subset Selection or Attribute Subset Selection. The idea behind subset selection is to remove the redundant feature and select only those features that represent the original distribution of your data. \n",
    "\n",
    "You have encountered different feature selection techniques in the previous unit. Here you learn about techniques like,\n",
    "* Best Subset Selection\n",
    "* Forward Stepwise Selection\n",
    "* Backward Stepwise Selection\n",
    "* Hybrid Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YmFhIwkxB_Wj"
   },
   "source": [
    "### Best Subset Selection\n",
    "For your simplicity, consider there are four features present within your data set. For this dataset, there are $2^4$ subsets of the features. \n",
    "\n",
    "Now, best subset selection fits all combinations models starting with only one feature. It stores the score. Then it selects another combination that includes two features, then three features and finally, four features. Once it gets the score for all the features, it picks the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "coq-RrjwD4So"
   },
   "source": [
    "### Forward Stepwise Selection\n",
    "\n",
    "You cannot use the best subset selection if your dataset has large features. Large features will only work on training data even if they don't apply to real use case scenario.\n",
    "\n",
    "Forward stepwise selection is an alternative to best subset selection. Unlike the best selection, forward stepwise selection begins with the model having no feature. It then adds other features one by one until it explores all the features. Finally, it selects the single best model among these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ARlPZKxDPuvB"
   },
   "source": [
    "### Backward Stepwise Selection\n",
    "Unlike forward stepwise selection, backward stepwise selection begins with the model, including all features. It then removes one feature at a time until only one feature remains. Among these features, it selects the best model.\n",
    "\n",
    "To better understand these algorithms, check the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hS1mSHf-Py7A"
   },
   "source": [
    "### Hybrid Selection\n",
    "Hybrid selection approach uses a combination of forward stepwise selection and backward stepwise selection. As it goes on adding relevant features, it also removes those features that don't contribute to model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M9GOjz7C1nNL"
   },
   "source": [
    "## Numerosity Reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BD5rABnZ1ueC"
   },
   "source": [
    "### Introduction\n",
    "Numerosity Reduction deals with techniques to reduce original data into a smaller form of data representations. These techniques are parametric as well as non-parametric.\n",
    "\n",
    "Parametric models consist of regression and log-linear model while non-parametric methods include histograms, clustering, sampling and data cube aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jFgDmmvIHnIC"
   },
   "source": [
    "\n",
    "### Parametric Numerosity Reduction\n",
    "Sometimes you can create new attributes from your features to better understand the data. This process is called feature construction. For example, you can estimate area from width and height.\n",
    "\n",
    "#### Regression\n",
    "Using regression, you can approximate the data for a given parameter $x$. For any random variable $y$, you can parameterize a linear model based on $x$ to output y as \n",
    "$$\n",
    "y = \\beta_0 + \\beta_1x,\n",
    "$$\n",
    "where w and b are regression coefficient.\n",
    "\n",
    "You can even extend the idea of linear regression into multiple linear regression, which results in y based on two or more parameters.\n",
    "\n",
    "#### Log-Linear Models\n",
    "Say you have $n$ attributes in your data. You consider each value in this set as a point in $n$-dimensional space. By using a log-linear model, you can estimate probability at each point using a smaller set of attributes. As a result, you can get a representation of higher dimensional space from lower dimensional space.\n",
    "\n",
    "#### Example Case\n",
    "One of the particular case can be using regression to fill in the sparse data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XpsJIuJhMLtH"
   },
   "source": [
    "### Non-parametric Numerosity Reduction\n",
    "\n",
    "#### Histograms\n",
    "Histograms bin the data to approximate the distribution. For any attribute A, histogram partitions the data distribution of A into buckets. Each bucket represents a continuous range for a given attribute.\n",
    "\n",
    "To partition a dataset into equal-width histogram or equal-frequency histogram.\n",
    "\n",
    "**Equal-width histogram** has a uniform width of each interval.\n",
    "\n",
    "**Equal-frequency** has intervals with constant frequency each.\n",
    "\n",
    "Histogram can represent single-attributes as well as multiple attributes.\n",
    "\n",
    "#### Clustering\n",
    "Clustering techniques partitions data into groups or cluster. Each data within a cluster is similar to another within the same cluster and dissimilar from a different cluster. \n",
    "\n",
    "You can determine quality from diameter of cluster and centroid distace. Diameter of cluster is the maximum distance between two cluster points. Centroid distance is the average distance between centroid of two different distance. You can learn more about clustering in Module 3, Unit 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aDHTmAuU19Gq"
   },
   "source": [
    "## Data Cube Aggregation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DN9gFYcS19kq"
   },
   "source": [
    "You can perform a wide range of operations on data present with the dataset. These can be operations like,\n",
    "* Selection,\n",
    "* Join,\n",
    "* Group,\n",
    "* View,\n",
    "* Update,\n",
    "* Delete\n",
    "* Aggregate, and more\n",
    "\n",
    "Aggregation using group by is difficult for creating histograms, finding cross-table data, and roll up totals. You can look up the details on this paper [3].\n",
    "\n",
    "Datacube stores multidimensional aggregated information. A multidimensional data space has different points representing different attributes. A cell in the cube corresponds to this data point that represent the given data attribute. \n",
    "\n",
    "At the base level, a cube corresponds to the entity of interest. These low-level cubes are used for the analysis. At higher abstraction, there are apex cuboids which carry information from base cuboids. The apex cuboids are accountable to reduce the size of data.\n",
    "\n",
    "A datacube has two operations that to perform on the data. \n",
    "\n",
    "**Roll-up** \\\n",
    "Roll-up operation allows you to travel up a concept hierarchy and create a summarization of your data.\n",
    "\n",
    "**Drill-Down**\\\n",
    "Drill-down is similar to roll-up but in reverse. Instead of travelling up a concept hierarchy, you travel down, unfolding the details.\n",
    "\n",
    "Suppose you have the following data of different products and you want to find which quarter had most sales.\n",
    "\n",
    "| Id |  Product |  Sales (in Thousands) | Location  | Time |\n",
    "|---|---|---|---|---|\n",
    "|  1 |  Laptop | 30  |  Kathmandu | Q1 |\n",
    "|  1 |  Laptop | 21  |  Lalitpur | Q2 |\n",
    "|  1 |  Laptop | 19  |  Pokhara | Q3 |\n",
    "|  1 |  Laptop | 30  |  Gorkha | Q4 |\n",
    "|  2 |  TV |  48 | Kathmandu  | Q1|\n",
    "|  2 |  TV |  18 | Lalitpur  | Q2|\n",
    "|  2 |  TV |  12 | Pokhara  | Q3|\n",
    "|  2 |  TV |  22 | Gorkha  | Q4|\n",
    "|  3 |  Mobile |  60 |  Kathmandu | Q1 | \n",
    "|  3 |  Mobile |  48 |  Lalitpur | Q2 | \n",
    "|  3 |  Mobile |  38 | Pokhara | Q3 | \n",
    "|  3 |  Mobile |  54 |Gorkha | Q4 | \n",
    "|  4 | Monitors| 17| Kathmandu| Q1|\n",
    "|  4 | Monitors| 19| Lalitpur| Q2|\n",
    "|  4 | Monitors| 11| Pokhara| Q3|\n",
    "|  4 | Monitors| 13| Gorkha| Q4|\n",
    "\n",
    "\n",
    "\n",
    "A simple way would be to group these data on time and aggregate based on sales. \n",
    "\n",
    "With data cube, you can represent this data in terms of product, time and location. Once represented, you can roll up for time and find how much times were sold for given quarter. \n",
    "\n",
    "With drill-down operation you can look further locations in each major city and identify which blocks or area had highest sales.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QTUHwb1cPg7o"
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. There are several techniques to perform dimensionality reduction. All these are highly dependent on the type of data and problems.\n",
    "\n",
    "2. Best subset selection explores all possible combinations of the data set; however, it is computationally expensive. There is it more practical to use techniques like forwarding subset selection or backward subset selection\n",
    "\n",
    "3. In the case of a large amount of data that hinders the generalization of the model, it is better to use techniques like histogram analysis, regression analysis, or even clustering to reduce the data.\n",
    "\n",
    "4. Data cube allows a different way to view the data and perform analysis on it. The primary feature of the data cube is roll-up and drill-down operations.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4.4.2 Data Reduction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
