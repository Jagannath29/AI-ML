{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6fFDKh0Ghzp"
      },
      "source": [
        "# Introduction to Hyperparameter Tuning\n",
        "\n",
        "In this unit we will learn about hyperparameter tuning. Before getting into the details of hyperparameter tuning, let us learn about hyperparameters.\n",
        "\n",
        "## Hyperparameter\n",
        "\n",
        "The changeable settings in which parameters of a machine learning algorithm are trained are called hyperparameters.\n",
        "\n",
        "The difference between parameter and hyperparameter is that the parameters are learned by machine learning model to fulfill the learning objective, and hyperparameters are the setting in which the parameters learn. Hence, hyperparameters are used to train machine learning model parameters during training phase.\n",
        "\n",
        "Some hyperparameters used in neural networks are:\n",
        "- learning rate,\n",
        "- batch size,\n",
        "- the optimizer,\n",
        "- number of layers,\n",
        "- types of layers,\n",
        "- number of hidden units in a layer,\n",
        "- regularization,\n",
        "- choice of activation function, etc.\n",
        "\n",
        "## Hyperparameter Tuning\n",
        "\n",
        "Hyperparameter tuning refers to choosing the optimization settings (hyperparameters) that are selected manually or programmatically to make the machine learning model perform its best, i.e. find the model hyperparameters that yield the best score on the validation metrics. Hyperparameters define the model behaviour and parameters that can be learned.\n",
        "\n",
        "In general, hyperparameters are prespecified before training the model. Hyperparameters can be tuned by trying different possible hyperparameters and training models with these as their settings. Although most of the hyperparameters are prespecified before training in general, they can also be changed during the training process. Tuning the hyperparameters during the training phase makes use of changeable hyperparameters for more effective optimization process. Some of the hyperparameters that can be changed during the training process include learning rate tuning, batch size tuning, loss function tuning, etc.\n",
        "\n",
        "We need to specify such settings for machine learning models to learn. If we use only one set of hyperparameters, we may get a good model but we probably would not get the best model. When a model architecture is reused for different tasks or dataset, the same set of hyperparameters can be tried without tuning them. This still does not guarantee to find the best model.\n",
        "\n",
        "In order to reach the minimum error with the highest accuracy, we need  to carefully choose these hyperparameters. The details about various hyperparameter tuning methods will be covered in the upcoming chapters.\n",
        "\n",
        "For now, let us get into one of the hyperparameter tuning, tuning the learning rate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxEQYsyfRSl6"
      },
      "source": [
        "### Hyperparameter Tuning: Tuning the Learning Rate\n",
        "\n",
        "Learning Rate is one of the Hyperparameters of a Neural Network. It determines the step size of gradient descent update in each iteration. We usually specify the learning rate at first and train our model with it. There are many possible outcomes with varying learning rates. We can see the various paths the error takes while using varying learning rates in the figure below.\n",
        "\n",
        "<!-- <img src=\"https://drive.google.com/uc?export=view&id=1ceC6QvOkNzfm5kSeoV9TlZyoB6UHS5AZ\" alt=\"Fig: Effect of Various Learning Rates on Error value\" width=\"300\"/> -->\n",
        "\n",
        "<img src =\"https://i.postimg.cc/QdsSmh4Y/image.png\" alt=\"Fig: Effect of Various Learning Rates on Error value\" width=\"300\"/>\n",
        "\n",
        "There are mainly four different distinct possible categories of learning rates. Following is the explanation of it.\n",
        "\n",
        "- Very high learning rate: When the learning rate is very high, the model can not learn, it will increase the error after taking gradient descent steps.\n",
        "- High learning rate: When the learning rate is high, the model learns but it cannot reach the minimum value. This is because the parameters jump around the minimum due to large learning rate.\n",
        "- Good learning rate: When the learning rate is good, the model improves on each iteration till it reaches minimum. It avoids the problem of slow convergence and oscillating loss around minima.\n",
        "- Low learning rate: When the learning rate is low, the decrease in error on each iteration is small. It takes a long time to reach minimum error value. It might also get stuck at local minima.\n",
        "\n",
        "Usually, we choose learning rates such as $0.001$, $3\\times10^{-4}$ and $5\\times10^{-5}$ for Deep Neural Networks. We may choose any values in between these values.\n",
        "\n",
        "In practice, models are trained on various learning rates. The value that yields the best score on the validation metrics is chosen. There is no hard and fast rule to selecting the learning rate. Although various learning rates can be used to train the model, we can also change the learning rate during training.\n",
        "\n",
        "#### Changing Learning Rate during Training\n",
        "\n",
        "Learning rates can be changed during the training phase to improve the learning process. This can be done manually according to heuristics by visualizing the error plot of the training. It can also be automated. Generally, we decrease the learning rate after each epoch of training. We can use decaying learning rate such as stepwise decay, exponential decay, etc. to decrease the learning rate.\n",
        "\n",
        "These learning rates are a function of the number of training epochs. Decaying Learning Rate helps to preserve the learned parameter space and optimizes within that space to reach the minimum. It can be considered as moving to the minima by avoiding the large jumps in gradient descent step. We can train neural networks to reduce error in small amounts, by using decaying learning rate.\n",
        "\n",
        "Following is the visualization of exponential learning rate decay.\n",
        "\n",
        "<!-- <img src=\"https://drive.google.com/uc?export=view&id=1Nzuu5YmR1Ka9zlcmcz9DGcNpu9w2bOHc\" alt=\"Fig: Effect of Various Learning Rates on Error value\" width=\"400\"/> -->\n",
        "<img src = \"https://i.postimg.cc/brkH0BQj/image.png\" alt=\"Fig: Effect of Various Learning Rates on Error value\" width=\"400\"/>\n",
        "\n",
        "Details on learning rate tuning and other hyperparameter tuning methods will be covered in later chapters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcamydkqAkNA"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}