{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCYQZZbmb6E0"
   },
   "source": [
    "# Boosting\n",
    "\n",
    "\n",
    "\n",
    "## Learning objective\n",
    "\n",
    "After reading this notebook, students will be able:\n",
    "- Distinguish boosting with bagging.\n",
    "- Explain boosting as a sequential, weighted averaging technique.\n",
    "- Explain the working of boosting with an algorithm.\n",
    "- Understand the significance of the contribution factor, $\\alpha$.\n",
    "- Express generic algorithm of boosting mathematically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zD2e3ZXn9u0Q"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "__Boosting__, like bagging, is an ensemble technique that forms a robust model by combining multiple models. But unlike bagging, boosting combines many weak learners (models with high bias and low variance), each model added sequentially. Here the term \"sequentially\" refers to the fact that models are added one at a time. This sequential nature comes into existence because each model is dependent on the previously added models. Boosting sequentially adds high bias and low variance models to create a low bias and low variance model.\n",
    "\n",
    "\n",
    "The goal of boosting is to additively combine multiple weak learners to form a powerful \"committee\" .  Each sequentially added model corrects the errors or shortcomings of a previous/existing model. This technique of adding a new model based on the shortcoming of existing models creates a robust model that is much better than the individual models. Based on how an existing model's error is identified, there are two popular boosting techniques: AdaBoost and Gradient Boosting. We will talk about these techniques in the upcoming chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPqQtrAwI4gh"
   },
   "source": [
    "#### Model Averaging\n",
    "The concept of __model averaging__ is also prevalent with boosting. In bagging, we trained multiple models on a bootstrap-resampled version of training data and averaged their predictions. For classification, we performed aggregation through majority voting.\n",
    "\n",
    "In boosting, we train multiple models on the modified(reweighted/updated) version of training data and perform weighted averaging of multiple models. For classification, we perform weighted majority voting. We will understand the reason behind weighted voting/averaging later by introducing the term $\\alpha$, the contributing factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88viSojzA2z1"
   },
   "source": [
    "## Working of Boosting\n",
    "Until now, we saw what makes boosting different from bagging. We also discussed the working of boosting briefly. Let us now see in detail how boosting works with the help of the following figure.\n",
    "\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<figure>\n",
    "<!-- <img src=\"https://doc.google.com/a/fusemachines.com/uc?export=download&id=1V2sdgm9TY7eURV7uHX24NsKlacWPKvON\" > -->\n",
    "<img src = \"https://i.postimg.cc/HLytJ71L/image.png\">\n",
    "<figcaption>Figure 1. Generic Boosting Framework\n",
    "</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwuh_QKQBLi0"
   },
   "source": [
    "The figure above shows a binary dataset consisting of white and black circles. The white circle represents instances belonging to the positive class and the black circle represents instances belonging to the negative class. Our goal is to build a model that can correctly classify these data points.\n",
    "\n",
    "First, we fit a model, __Model 1__, a weak learner on the training dataset. Since __Model 1__ is a weak learner, it is not surprising to say it makes mistakes. Suppose the model fails to classify some points correctly. Those points are highlighted by orange color in the above figure.\n",
    "\n",
    "One way to reduce(or compensate) the error/shortcomings of the model __Model 1__ is to make it robust/strong or increasing its capability. We can make it robust by tuning its hyperparameters to obtain an optimal model. We already learned how to perform hyperparameter tuning to obtain the best appropriate model. But here, we are interested in using the power of committee power, specifically boosting to improve an existing model __Model 1__.\n",
    "\n",
    "Another way to reduce(or compensate) the error/shortcomings of an existing model is to add another model, that mainly focus on the shortcoming of an existing model.  Here, we add model __Model 2__, a new weak learner that mainly focuses on the shortcomings of model __Model 1__.\n",
    "\n",
    "There are two consequences of adding __Model 2__:\n",
    "- It might fail to correct all the mistakes of the previous model.\n",
    "While focusing heavily on the previous model's erroneous samples, it may make misclassify other samples.\n",
    "\n",
    "From these two arguments, we may conclude that the combination of both models __Model 1__ and __Model 2__ still has some errors, highlighted by the orange color. To compensate for this error, we add another model, __Model 3__, again a new weak learner. In this way, we iteratively add multiple weak learners. The addition of an unnecessarily large number of models might overfit the training data. The number of models to be added is problem specific and is one of the hyperparameters in boosting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpcOpPGiXZoU"
   },
   "source": [
    "\n",
    "Until now, we introduced boosting as a sequential additive model and understood its working with the help of the figure. Let us now delineate all the steps involved in boosting.\n",
    "\n",
    "> 1. Train a weak learner $h_1$ on the original training dataset.\n",
    "\n",
    " > 2. Identify the errors or shortcomings of an existing model.\n",
    "\n",
    "> 3. Modify the training data based on the errors or shortcomings of an existing model.\n",
    "\n",
    "> 4. Train another model $h_2$ on the modified training data and add it to an existing model, $h_1$.\n",
    " - The modified training data has already incorporated the errors or shortcoming of an existing model(i.e, model $h_1$).\n",
    " - After addition of newly created model $h_1$, the resulting ensemble model $F$ is\n",
    " $$F = h_1+ h_2$$\n",
    "\n",
    "In this way, we can repeat steps 2, 3, and 4 for several iterations. That means in each iteration($t$), we find the errors/shortcomings of an existing model. We modify the training data to incorporate the error or shortcomings of an existing model, and train a new model $h_t$ on the modified dataset.\n",
    "\n",
    "Suppose we form an ensemble combining $M$ number of weak learners, $h_t$. The prediction made by the ensemble on a given data point $\\mathbf{x}$ is:\n",
    "\n",
    "$$F(\\mathbf{x}) = \\sum_{t=1}^M \\alpha_t h_t(\\mathbf{x})$$\n",
    "\n",
    "Where,\n",
    "\n",
    "$F(\\mathbf{x})$ is a prediction made by the ensemble of $M$ weak learners $h_t$.\n",
    "\n",
    "$\\alpha_t$ is a contribution factor of each learner $h_t$.\n",
    "\n",
    "This equation shows the iterative addition of multiple models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gFOQlC-XmPe"
   },
   "source": [
    "### What is $\\alpha$?\n",
    "In the above section, we have introduced a term $\\alpha$. Let us understand the significance of $\\alpha$ with the help of an example. Suppose you faced some problems. Since you are already familiar with the power of ensemble, you decide to refer multiple people to solve your problem. You decide to perform majority voting on the candidate solutions provided by a different person to find the best solution. Does this technique of finding the solution is optimal?\n",
    "\n",
    "Of course not. This is not an optimal way of finding a solution. What about the expertise of people? Some people are expert(more accurate) on the type of problem you have faced. Their suggestion is more appropriate and more accurate than general people who may perhaps guess the solution. While combining the solution of experts and non-experts, do they deserve an equal vote?\n",
    "\n",
    "No, they don't. We must give more emphasis to the decision of an expert and less to the non-expert. It means we take more contribution from the decision of expert and less from a non-expert. A similar technique applies to boosting. Among those $M$ iteratively added models $h_t$ some are more accurate, and some are less accurate. We need to give more emphasis to the more accurate models and less on less accurate models. We consider this factor in our ensemble by including the term $\\alpha_t$, which denotes the importance of the model $h_t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgSzGlTeG3zv"
   },
   "source": [
    "## Mathematical Formulation of Boosting\n",
    "\n",
    "Until now, we intuitively discussed boosting with the help of an example and listed out all the steps involved in boosting. Generally, boosting is just an additive expansion of multiple base functions. By base function, we mean regressor and/or classifier.\n",
    "The additive modeling of boosting is expressed as:  \n",
    "$$F(\\mathbf{x}) = \\sum_{t=1}^M \\alpha_t h_t(\\mathbf{x})$$\n",
    "\n",
    "Intuitively, we have discussed how to obtain the above additive model. Generally, the above model is obtained by optimization/minimization of a function called a loss function. We have already discussed function optimization in the linear regression module. We can obtain the above expression by optimizing a loss function as:\n",
    "\n",
    "$$\n",
    "F(\\mathbf{x}) = \\underset{\\{\\alpha_t, h_t\\}_{1}^M}{\\arg\\min}\\sum_{i=1}^N L\\left(y_i, \\sum_{t=1}^M \\alpha_t h_t(\\mathbf{x}_i)\\right)\n",
    "$$\n",
    "\n",
    "This represents the simultaneous optimization of $M$ different learners such that their combination minimizes the overall loss function.\n",
    "\n",
    "Here, we are optimizing all weak learners $h_t({\\mathbf{x}})$ and their contribution factor $\\alpha_t$simultaneously, which is a highly computationally intensive task.\n",
    "\n",
    "The viable alternative to this optimization problem in terms of computational requirement is __stagewise forward additive modeling__. In stagewise forward additive modeling, we obtain one optimal function at a time without changing the parameters of an already added models. For example, we obtain model $h_1$ and $\\alpha_1$ that minimize a loss function at the first iteration. In the second iteration, we add another model $h_2$ and $\\alpha_2$ so that the combination of $h_1$ and $h_2$ minimizes the loss function. While obtaining $h_2$ and $\\alpha_2$, we don't adjust $\\alpha_1$ and the parameters of $h_1$. The algorithm for stagewise additive modeling is given as:\n",
    "\n",
    "\n",
    "\n",
    "1. Initialize $F_0(\\mathbf{x}) = 0$  \n",
    "2. For $t=1$ to $M$  \n",
    "  Compute : $\\alpha_t, h_t = \\arg\\min_{\\alpha, h} \\sum_{i=1}^N L\\left(y_i, F_{t-1}(\\mathbf{x}_i) + \\alpha h(\\mathbf{x}_i)\\right)$\n",
    "\n",
    "  Update : $F_t({\\mathbf{x}})= F_{t-1} + \\alpha_th_t(\\mathbf{x})$\n",
    "\n",
    "\n",
    "\n",
    " Initially, we set our model such that it returns zero for any given input. Then for each iteration($t$), we compute the model $h_t$ and it's corresponding weight or contribution $\\alpha_t$ such that when added on the existing  model $F_{t-1}(\\mathbf{x})$ reduces the overall loss/error. Note that during each optimization stage, we don't adjust the parameters of an existing model $F_{t-1}(\\mathbf{x})$.\n",
    "     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkkwQErL3yyC"
   },
   "source": [
    "In this way, we talked about how boosting differs from bagging and discussed the working of boosting. We also discussed on how to add the models and established the need for weighted averaging/voting. There are two popular boosting techniques: AdaBoost and Gradient Boosting. We will study these techniques in the upcoming chapters.  The idea of boosting becomes even more apparent once we study these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbfkRrUdZPGn"
   },
   "source": [
    "### Key Takeaways\n",
    "\n",
    "- Boosting is a sequential ensemble method that combines multiple weak learners.\n",
    "\n",
    "- Iterative addition in boosting is based on the errors/shortcomings of an existing model.\n",
    "\n",
    "- In boosting, we performed weighted averaging of multiple models.\n",
    "\n",
    "- $\\alpha$ in boosting determines the contribution/importance of individual weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
