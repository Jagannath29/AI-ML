{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls-RR4HhM4Oz"
      },
      "source": [
        "# Error Decomposition\n",
        "\n",
        "\n",
        "\n",
        "## Learning Objectives\n",
        "After going through this notebook, students should be able to:\n",
        "- Decompose error into bias, variance, and noise.\n",
        "- Define bias, variance, and noise.\n",
        "- Identify the cause behind the model's poor performance, i.e, bias, variance, or noise.\n",
        "- Propose a solution to improve the performance of the machine learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "767Dgf6WjlLb"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Na-49cvM4O1"
      },
      "source": [
        "A machine learning model trained on a dataset may not be 100% accurate. It has some errors. The errors may be introduced due to:  \n",
        "\n",
        "* The model being too simple or very complex.\n",
        "* Ambiguity in the data.  \n",
        "\n",
        "In this chapter, we will break the errors of a machine learning model into its components, examine sources of these errors and methods that can help us eliminate these errors.\n",
        "\n",
        "\n",
        "## Error Decomposition For Regression\n",
        "\n",
        "Let us discuss some assumptions and notations that will be used in this notebook.\n",
        "\n",
        "* $\\textit{A}$ represents a machine learning algorithm.\n",
        "* $D = \\{(\\mathbf{x_1}, y_1),\\dots, (\\mathbf{x_n}, y_n)\\}$ represents a dataset drawn i.i.d from some distribution $P(X, Y)$.\n",
        "* $y$ is the target of a regression problem.\n",
        "* $h_D$ represents a machine learning model.\n",
        "* The dataset is noisy. For the same value of features, $\\textbf{x}$, the output/label $y$ may be different.\n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        " <figure>\n",
        " <img src=\"https://doc.google.com/a/fusemachines.com/uc?export=download&id=1lIzk034tgOWRx0lFj1_ImtKBno-79rmt\" width=\"600\" >\n",
        " <figcaption>Figure 1: Machine learning algorithm when applied to a dataset results in a machine learning model</figcaption>\n",
        " </figure>\n",
        "</div>\n",
        "\n",
        "<!-- https://drive.google.com/file/d/1lIzk034tgOWRx0lFj1_ImtKBno-79rmt/view?usp=sharing -->\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKR3nLbVM4O1"
      },
      "source": [
        "Let us define a few more terms.\n",
        "\n",
        "\n",
        "An **expected label** is the label we expect to get from the trained machine learning model for an input $\\bf x$. It is denoted by $\\bar y(\\bf x)$. It is computed as:\n",
        "$\\bar y(\\textbf{x}) = E_{y|\\textbf{x}}[Y] = \\int_y y Pr(y|\\textbf{x}) \\partial y$\n",
        "\n",
        "**Note**   \n",
        "Since the random variable involved, in this case the output/label, is continuous, we are using integration to compute the expectation. If the output/label was discrete, we would have used summation as follows:\n",
        "$\\bar y(\\textbf{x}) = E_{y|\\textbf{x}}[Y] = \\sum_y y Pr(y|\\textbf{x})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPy9wna8M4O2"
      },
      "source": [
        "Our model, $h_D$, is not 100% accurate. To compute its error, we sample some data points $(\\textbf{x}, y)$ from the original distribution $P$ and compute the squared loss. Mathematically, **expected test error** of our model $h_D$ is given as:\n",
        "$E_{(\\textbf{x}, y)\\sim P}[(h_D(\\textbf{x})- y)^2]=\\int_x \\int_y (h_D(\\textbf{x})- y)^2 Pr(\\textbf{x}, y)\\partial y \\partial \\textbf{x}$\n",
        "\n",
        "\n",
        "Our model, $h_D$, depends on the dataset $D$ sampled from a distribution $P$. There are different sets of data that can be sampled from $P$. Different datasets will result in different models, even when the same algorithm, $ A $, trains the model. This makes $h_D$ a random variable.\n",
        "<!-- https://drive.google.com/file/d/12ZjCrQaj96xS8Dpc5bLgicMlUZKOP60C/view?usp=sharing -->\n",
        "\n",
        "<div align=\"center\">\n",
        " <figure>\n",
        " <!-- <img src=\"https://doc.google.com/a/fusemachines.com/uc?export=download&id=12ZjCrQaj96xS8Dpc5bLgicMlUZKOP60C\" width=\"300\" > -->\n",
        " <img src=\"https://i.postimg.cc/8CvG0cGf/image.png\" width=\"300\">\n",
        " <figcaption>Figure 2: Multiple datasets results in multiple models. Small green circles represent the universe/distribution of data points from where $D_i$'s are sampled.</figcaption>\n",
        " </figure>\n",
        "</div>\n",
        "\n",
        "An **expected model** can be obtained using the formula,\n",
        "$\\bar h = E_{D \\sim P^n}h_D = \\int_D h_D Pr(D) \\partial D$\n",
        "Here,\n",
        "$\\bar h$ represents the expected model.\n",
        "$D \\sim P^n$ represents a dataset with $n$ instances drawn from a distribution $P$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPZHG28WM4O3"
      },
      "source": [
        "The expression for **expected test error** provided earlier computes the expected error of a model $h$ trained on data $D$ sampled from the distribution $P$ using the algorithm $A$. However, we are interested in the expected test error made by a model ($h$) trained using algorithm $A$ no matter what dataset is used, as long as it is sampled from the distribution $P$. Expected test error, given an algorithm $A$ and distribution $P$, can be computed as:\n",
        "\n",
        "$$\n",
        "E_{D \\sim P^n, (\\mathbf{x}, y)\\sim P} \\left[(h_D(\\mathbf{x})- y)^2\\right]= \\int_D\\int_\\mathbf{x}\\int_y (h_D(\\mathbf{x})- y)^2\\, Pr(\\mathbf{x},y)\\,Pr(D)\\, d\\mathbf{x}\\, dy\\, dD\n",
        "$$\n",
        "\n",
        "Now, let us decompose this expected test error into its components.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "E_{\\mathbf{x}, y, D}[(h_D(\\mathbf{x})- y)^2] &= E_{\\mathbf{x}, y, D}\\left[\\left(h_D(\\mathbf{x})- \\bar h(\\mathbf{x}) + \\bar h(\\mathbf{x}) - y\\right)^2\\right] \\\\\n",
        "&= E_{\\mathbf{x}, D}[(h_D(\\mathbf{x})- \\bar h(\\mathbf{x}))^2] \\\\\n",
        "&\\quad + 2 E_{\\mathbf{x}, y, D} \\left[(h_D(\\mathbf{x})- \\bar h(\\mathbf{x})) (\\bar h(\\mathbf{x}) - y)\\right] \\\\\n",
        "&\\quad + E_{\\mathbf{x}, y}[(\\bar h(\\mathbf{x}) - y)^2] \\tag{1}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "The middle term in equation (1) becomes 0 as shown below:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "2 E_{\\mathbf{x}, y, D} \\left[(h_D(\\mathbf{x})- \\bar h(\\mathbf{x})) (\\bar h(\\mathbf{x}) - y)\\right] \n",
        "&= 2 E_{\\mathbf{x}, y}\\left[E_D[h_D(\\mathbf{x}) - \\bar h(\\mathbf{x})] (\\bar h(\\mathbf{x})-y)\\right] \\\\\n",
        "&= 2 E_{\\mathbf{x}, y}\\left[(E_D[h_D(\\mathbf{x})] - \\bar h(\\mathbf{x})) (\\bar h(\\mathbf{x})-y)\\right] \\\\\n",
        "&= 2 E_{\\mathbf{x}, y}\\left[(\\bar h(\\mathbf{x}) - \\bar h(\\mathbf{x})) (\\bar h(\\mathbf{x})-y)\\right] \\\\\n",
        "&= 0\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Equation (1) can now be written as:\n",
        "\n",
        "$$\n",
        "E_{\\mathbf{x}, y, D}[(h_D(\\mathbf{x})- y)^2] = E_{\\mathbf{x}, D}[(h_D(\\mathbf{x})- \\bar h(\\mathbf{x}))^2] + E_{\\mathbf{x}, y}[(\\bar h(\\mathbf{x}) - y)^2] \\tag{2}\n",
        "$$\n",
        "\n",
        "We can further break down the right-most expression of equation (2) as:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "E_{\\mathbf{x}, y}[(\\bar h(\\mathbf{x}) - y)^2] &= E_{\\mathbf{x}, y}\\left[\\left((\\bar h(\\mathbf{x}) - \\bar y(\\mathbf{x})) + (\\bar y(\\mathbf{x}) - y)\\right)^2\\right] \\\\\n",
        "&= E_{\\mathbf{x}, y}[(\\bar h(\\mathbf{x}) - \\bar y(\\mathbf{x}))^2] \\\\\n",
        "&\\quad + E_{\\mathbf{x}, y}[(\\bar y(\\mathbf{x}) - y)^2] \\\\\n",
        "&\\quad + 2 E_{\\mathbf{x}, y} [(\\bar h(\\mathbf{x}) - \\bar y(\\mathbf{x}))(\\bar y(\\mathbf{x}) - y)] \\tag{3}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Just like the earlier expression, the third term in equation (3) also reduces to 0:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "E_{\\mathbf{x}, y} [(\\bar h(\\mathbf{x}) - \\bar y(\\mathbf{x}))(\\bar y(\\mathbf{x}) - y)] \n",
        "&= E_{\\mathbf{x}} \\left[(\\bar h(\\mathbf{x}) - \\bar y(\\mathbf{x})) E_{y|\\mathbf{x}}[\\bar y(\\mathbf{x}) - y]\\right] \\\\\n",
        "&= E_{\\mathbf{x}} \\left[(\\bar h(\\mathbf{x}) - \\bar y(\\mathbf{x})) (\\bar y(\\mathbf{x}) - E_{y|\\mathbf{x}}[y])\\right] \\\\\n",
        "&= E_{\\mathbf{x}} \\left[(\\bar h(\\mathbf{x}) - \\bar y(\\mathbf{x})) (\\bar y(\\mathbf{x}) - \\bar y(\\mathbf{x}))\\right] \\\\\n",
        "&= 0 \\tag{4}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Finally, using (3) and (4), we can write equation (2) as:\n",
        "\n",
        "$$\n",
        "E_{\\mathbf{x}, y, D}[(h_D(\\mathbf{x})- y)^2] = E_{\\mathbf{x}, D}[(h_D(\\mathbf{x})- \\bar h(\\mathbf{x}))^2] + E_{\\mathbf{x}, y}[(\\bar h(\\mathbf{x}) - \\bar y(\\mathbf{x}))^2] + E_{\\mathbf{x}, y}[(\\bar y(\\mathbf{x}) - y)^2]\n",
        "$$\n",
        "\n",
        "Or, in terms of error components:\n",
        "\n",
        "$$\n",
        "\\underbrace{E_{\\mathbf{x}, y, D}[(h_D(\\mathbf{x})- y)^2]}_{\\text{Expected test error}} =\n",
        "\\underbrace{E_{\\mathbf{x}, D}[(h_D(\\mathbf{x})- \\bar h(\\mathbf{x}))^2]}_{\\text{Variance}} +\n",
        "\\underbrace{E_{\\mathbf{x}, y}[(\\bar h(\\mathbf{x}) - \\bar y(\\mathbf{x}))^2]}_{\\text{Bias}^2} +\n",
        "\\underbrace{E_{\\mathbf{x}, y}[(\\bar y(\\mathbf{x}) - y)^2]}_{\\text{Noise}}\n",
        "$$\n",
        "\n",
        "The above expression shows the error of machine learning models along with its components: **Variance**, **Bias$^2$**, and **Noise**. For simplicity, we will refer to **Bias$^2$** as **Bias**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG7qM3oiNjRk"
      },
      "source": [
        "## Error Decomposition for Classification\n",
        "\n",
        "Now, let us see how we can decompose the error for a classification problem. The assumptions and terminologies we talked about earlier still hold. But, the loss function changes to $L(h_D(\\textbf{x}), y)$. Here, the output $y$ is a discrete label.  \n",
        "\n",
        "\n",
        "$\n",
        "L(h_D(\\textbf{x}), y) = \\begin{cases}\n",
        "1 \\text{ if }  h_D(\\textbf{x}) \\neq y, \\\\\n",
        "0 \\text{ otherwise}.\n",
        "\\end{cases}\n",
        "$\n",
        "\n",
        "\n",
        "For classification, our goal is to decompose $E_P[L(h_D(\\textbf{x}), y)]$ into bias, variance and noise. To do so, we define a main prediction as,\n",
        "$y^m(\\textbf{x}) = argmin_{h_D(\\textbf{x})}E_P[L(h_D(\\textbf{x}), y)]$.   \n",
        "Hence, main prediction is the one that gives us minimum expected loss. The loss is minimum if the predictions made by majority of all possible models is equal to $y$ i.e the mode of predictions is equal to $y$.    \n",
        "\n",
        "\n",
        "Now, expression for bias, variance, and noise can be written as:\n",
        "* **Bias** $= L(y^m, \\bar y)$\n",
        "\n",
        "  * $\n",
        "\\text{Bias} = \\begin{cases}\n",
        "1 \\text{ if }  y^m \\neq \\bar{y}, \\\\\n",
        "0 \\text{ otherwise}.\n",
        "\\end{cases}\n",
        "$\n",
        "* **Variance** $ = E[L(h_D(\\textbf{x}), y^m)]$\n",
        "   * $\n",
        "\\text{Variance} = \\begin{cases}\n",
        "1 \\text{ if }  h_D(\\textbf{x}) \\neq y^m, \\\\\n",
        "0 \\text{ otherwise}.\n",
        "\\end{cases}\n",
        "$\n",
        "* **Noise** $ = E[L(y, \\bar y)]$\n",
        " * $\n",
        "\\text{Noise} = \\begin{cases}\n",
        "1 \\text{ if }  y \\neq \\bar y, \\\\\n",
        "0 \\text{ otherwise}.\n",
        "\\end{cases}\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5cICn9P4pqT"
      },
      "source": [
        "## Bias, Variance, and Noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM6V5FDZM4O3"
      },
      "source": [
        "From the above relation, we can define bias, variance, and noise as:\n",
        "* **Variance**:\n",
        " Variance tells us how different is our model $h_D$ trained on data $D$ from the expected model or the best model that can be obtained using the algorithm $A$. Variance captures how much our model changes if we choose a different training set, $D$.\n",
        "\n",
        "* **Bias**:\n",
        " Bias tells us how bad is the best model, $\\bar h(\\textbf{x})$, that can be trained using algorithm $A$ in capturing the information/pattern in our data.\n",
        "\n",
        "* **Noise**:\n",
        " Noise gives the measure of ambiguity in our data. It is the property of the dataset.\n",
        "\n",
        "Now we know that our test error is made up of three quantities. Let us see how we can detect and deal with each of these quantities.\n",
        "* **Variance**\n",
        " **Detection:** A model is said to have high variance if it performs well on the train set but lags in the test set. For a high variance model, the train error is much less than the threshold, $\\varepsilon$, but the test error is greater than the threshold.   \n",
        " **Solution:** High variance can be removed by:\n",
        " * Adding more training data\n",
        " * Using a simpler model\n",
        " * Bagging\n",
        "\n",
        "* **Bias**\n",
        " **Detection:** If the model's train error is higher than some threshold $\\varepsilon$, the model is said to be highly biased. A model with high bias can never achieve 100% accuracy on the training set whatsoever.   \n",
        " **Solution:** High bias can be removed by:\n",
        " * Using a complex model\n",
        " * Adding more features\n",
        " * Boosting\n",
        "\n",
        "\n",
        "* **Noise**\n",
        " **Detection**: If two(or more) instances have the same feature values $\\textbf{x}$ but have different labels $y$, then the dataset is noisy.   \n",
        " **Solution:** The noise in the dataset should be removed, or if possible, the dataset should be changed: use a new one.\n",
        "\n",
        "\n",
        "Thus, to improve our model's performance, we need to find out the source of the error $-$bias, variance, or noise. Depending on this source, we apply suitable techniques to improve the performance of the model. In the next few chapters, we will study about bagging, random forest, and boosting that can help us to reduce bias and variance systematically.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNv3Lq1GjNei"
      },
      "source": [
        "## Key Takeaways\n",
        "\n",
        "\n",
        "* From a given distribution($P$), we can sample multiple dataset $D_1, D_2, … $ and train multiple models.\n",
        "\n",
        "\n",
        "* Expected Test error can be decomposed into three components: bias, variance, and noise.  \n",
        "\n",
        "\n",
        "* Bias tells us how bad an algorithm is in capturing the underlying pattern/information. It can be reduced by adding more features, using a complex model or boosting.\n",
        "\n",
        "\n",
        "* Variance captures how much our model changes when we change the dataset. It can be reduced by  adding more training data, using simpler model or bagging.\n",
        "\n",
        "\n",
        "* Noise gives the measurement of ambiguity in our data. To reduce noise, we identify noisy instances and remove them from the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAZdkK-FmkII"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xxa0H_rM4O4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
