{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZDND2QoaoMv"
   },
   "source": [
    "# Bagging\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "After going through this notebook, students should be able to:\n",
    "- Explain why averaging reduces variance.\n",
    "- Define bagging and bootstrapping.\n",
    "- List steps involved in creating a bagged model.\n",
    "- Justify that bagging reduces overall error even when it increases bias.\n",
    "- List the advantages of bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbz_VownaoMw"
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRgk2hRkaoMw"
   },
   "source": [
    "From our previous discussion we know that the test error of a machine learning model can be broken down into three components: Bias, Variance and noise as follows:  \n",
    "$\\underbrace{E[(h_D(\\textbf{x})- y)^2]}_\\text{Expected test error} =\\underbrace{E[(h_D(\\textbf{x})- \\bar h(\\textbf{x}))^2]}_\\text{Variance} + \\underbrace{E[(\\bar h(\\textbf{x}) - \\bar y(\\textbf{x}))^2]}_{\\text{Bias}^2} + \\underbrace{E[(\\bar y(\\textbf{x}) -  y)^2]}_\\text{Noise}$  \n",
    "\n",
    "In this we chapter will see how we can produce a low variance model by combining several strong learners$-$models with high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aea4293VaoMx"
   },
   "source": [
    "  \n",
    "Let us focus on the variance part of the equation:  \n",
    "\n",
    "$\\textbf{Variance}= E[(h_D(\\textbf{x})- \\bar h(\\textbf{x}))^2]$  \n",
    "\n",
    "Variance decreases as our model $h_D\\textbf(x)$ approaches the expected model, $\\bar h(\\textbf{x})$, and becomes 0 when $ h_D\\textbf(x) = \\bar h(\\textbf{x}) $. To reduce the variance, our goal should be to make our model as close to the expected model, $\\bar h(\\textbf{x}$), as possible. According to the law of large numbers, for any independently and identically distributed random variable $x$ with mean $\\bar x$ if our sample size is large then the average of the sample is very close to the actual mean $\\bar x$.  \n",
    "$\n",
    "\\frac{1}{m} \\sum_{i=1}^{m} x_i \\rightarrow \\bar x \\text{ as }m \\rightarrow \\infty.\n",
    "$  \n",
    "\n",
    "For a machine learning model, the above equation can be written as:   \n",
    "$\n",
    "\\hat h = \\frac{1}{m} \\sum_{i=1}^{m} h_{i}\\rightarrow \\bar h \\text{ as }m \\rightarrow \\infty.\n",
    "$    \n",
    "\n",
    "Here, $h_i$ represents a machine learning model trained on dataset $D_i$.    \n",
    "\n",
    "\n",
    "From the above equation we can see that if we have $m$ different training data, we can train $m$ models to get $\\hat h$ which will be very close to our average model $\\bar h$ and thus reduce the variance. But, the problem is we only have one dataset $D$. This is where bagging or bootstrap aggregating comes into play.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_IOXV4KaoMx"
   },
   "source": [
    "## Bagging and Bootstraping\n",
    "**Bagging** is an ensemble method which trains $m$ different high variance models/classifiers$-$strong learners$-$ on bootstrap samples of the same training data $D$ and finally combines them to get a single model with reduced variance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnDm-gvvaoMy"
   },
   "source": [
    " <div align=\"center\">\n",
    "    <figure>\n",
    "     <!-- <img src=\"https://doc.google.com/a/fusemachines.com/uc?export=download&id=1MmRME8EIsb2nib-RDYKrUBf5RdAcagd1\" width=\"600\"> -->\n",
    "     <img src = \"https://i.postimg.cc/Rq5sFQN4/image.png\">\n",
    "     <figcaption>Figure 1: a) Bootstrapped Samples b) Bagging</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "<!-- https://drive.google.com/file/d/1MmRME8EIsb2nib-RDYKrUBf5RdAcagd1/view?usp=sharing -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0skSZx-aoMz"
   },
   "source": [
    "Bagging employs **bootstrap sampling**. Bootstrap Sampling is a method that involves drawing data samples repeatedly with replacement from a data source. Using bootstrap sampling we create $m$ different datasets by taking multiple samples $-D_1, D_2, \\dots D_m-$ from the original dataset $D$ with replacement. It is important to sample with replacement else the samples wont be independent. If the samples aren't independent then the law of large numbers won't hold true. Avereraging the models trained on these samples won't get us closer to the average model.  \n",
    "\n",
    "Each of these samples obtained after bootstrapping is called a **bootstrap sample**.  These samples are used to train multiple models $h_1, h_2, \\dots, h_m$. These models are finally averaged to get the final model.  Bagging can be summarized as below:  \n",
    "1. Sample $m$ datasets $D_1, D_2, \\dots, D_m$ from $D$ with replacement.  \n",
    "2. For each $D_i$ train a model $h_i$.  \n",
    "3. Combine all models to get the final model:   \n",
    "Regression: $\\hat h = \\frac{1}{m}\\sum_{i=1}^{m}h_i$   \n",
    "Classification: $\\hat h = \\text{Mode}(h_i, i\\varepsilon 1, \\dots, m)$  \n",
    "\n",
    "An important thing to note about bootstrap samples is that when creating bootstrap samples, some data points of $D$ are repeated and some don't even appear in the bootstrap sample. It is found that only 63% of the data in these bootstrap samples are unique and the rest 37% are duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjaCFAfHbHNZ"
   },
   "source": [
    "## Effect of Bagging on Bias\n",
    "\n",
    "Since, the bootstrap sample contains only 63% of the unique data from the original dataset, it is difficult for the individual model to learn the underlying distribution/pattern. This causes the bias of the model and hence the ensemble to increase slightly. However, the decrease in variance is more significant than the increase in bias causing the overall error to decrease.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HD1zj8epaoMz"
   },
   "source": [
    "## Advantages of Bagging  \n",
    "\n",
    "Some of the advantages of bagging are:  \n",
    "* **Easy to implement**  \n",
    "  To implement bagging we don't need to make any changes to the inner working of algorithms(decisoin trees, SVM, etc.). All we need is bootstrap samples which are easy to obtain once we have a dataset.\n",
    "* **Bagging effectively reduces the variance**  \n",
    "* **Can be implemented for any algorithm**\n",
    "* **Parallel Execution**  \n",
    "  Bagging combines multiple models to form an ensemble. Each of these models can be trained independently from others. This allows us to train multiple models at once and decreases the training time significantly.\n",
    "* **Gives an unbiased estimate of the test error**  \n",
    "  Bootstrap samples contain only 63% of the original data $D$. Samples present in $D$ but absent in a bootstrap samples are called **out of bag samples**. For each out of bag sample, we feed it to all models that didn't have the sample in their training set and combine their output to get the predicted value. We compare the true value with the predicted value to find the error. We repeat this process for all instances in the dataset to get an unbiased estimate of test error also called **out of bag error**. Hence, bagging doesn't need a separate test set to evaluate its performance on unseen data.\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dF6BP4d9Nbw"
   },
   "source": [
    "In this chapter we learned about bagging and its significance in reducing the error by reducing variance. We also learned about its advantages. In the next chapter we will study about a similar method, **random forest**, which is even more effective in reducing variance but is limited to decision trees.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WClYgRc5BuO"
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "* Averaging can be used to systematically reduce variance.\n",
    "* Bagging trains multiple independent models using bootstrap samples and combines their output to give the final prediction.\n",
    "* Bagging brings a significant decrease in variance along with a slight increase in bias. However, the decrease in variance is very large in comparison to increase in bias causing the error to decrease.  \n",
    "* Bagging doesn't need a separate test set for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVU-LY6vmOaC"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
