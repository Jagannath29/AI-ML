{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENh2WSO60oZN"
   },
   "source": [
    "# Elastic Net Regression\n",
    "\n",
    "## Limitations of Ridge and Lasso Regression\n",
    "\n",
    "Lasso Regression addresses the issue of Ridge Regression, that is, not punishing the coefficients to zero. Still, Lasso has some limitations which are listed below:\n",
    "1. It tends to fail in group selection (i.e. it chooses one variable from a group of highly correlated variables and ignores the rest). Some of the ignored variables might be important in predicting the target.\n",
    "2. In a dataset containing more number of variables(d) than the number of data samples(n), ${d>n}$, Lasso Regression selects n variables only. That means it doesn't incorporate the characters of all features in the generated model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpNVgvE_zL-l"
   },
   "source": [
    "These shortcomings were addressed by Elastic Net Regression. It is a regularized linear model that combines the lasso and ridge regression techniques to improve the model. It penalizes the coefficients based on their $L_1$ norm and $L_2$ norm.\n",
    "\n",
    "$$Loss_{elastic}= RSS + \\lambda ((1-\\alpha)\\cdot L_2 norm + \\alpha \\cdot L_1 norm) $$\n",
    "\n",
    "$${{\\sum_{i=1}^{n} (y_i-\\hat{y_i})^2}+ \\lambda\n",
    "( {(1-\\alpha)}\\cdot\\sum_{j=1}^{d} \\boldsymbol{\\beta_j^2}}+\\alpha \\cdot \\sum_{j=1}^{d}\\mid\\boldsymbol{\\beta_j}\\mid)$$\n",
    "The lasso penalty creates a sparse model. Likewise, the ridge penalty restricts the model from ignoring the highly correlated variables and stabilizes the $L_1$ regularization path.\n",
    "\n",
    "Notice that the equation can be tuned to form Lasso as well as Ridge Regression.\n",
    "1. If $\\alpha=0$, the $L_2$ penalty remains and the equation reduces to Ridge Regression.\n",
    "2. If $\\alpha=1$, the $L_1$ penalty remains and the equation reduces to Lasso Regression.\n",
    "3. If ${0<\\alpha<1}$, we get the combination of Ridge and Lasso Regression.\n",
    "\n",
    "Therefore, we should choose $\\alpha$ between 0 and 1 to optimize the elastic net.\n",
    "\n",
    "The elastic net cost in *argmin* or argument minimum representation is\n",
    "\n",
    "$$\\boldsymbol{\\beta_{lasso}} = \\underset{\\boldsymbol{\\beta\\in\\mathbb{R}}}{\\arg\\min}||y-X\\beta||_2^2+\\lambda(\\alpha||\\beta||_1^1+(1-\\alpha)||\\beta||_2^2)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5KNJgadzKZq"
   },
   "source": [
    "## Geometrical Interpretation of Elastic Net\n",
    "\n",
    "We came to know that the elastic net regression is combination of Ridge and Lasso Regression. Similarly, the geometric shape of elastic net penalty can also be visualized with the help of Ridge and Lasso penalty. The elastic net penalty lies in between the $L_2$ norm circle and $L_1$ norm diamond shape with the shape of a square with rounded sides as shown in the figure below.\n",
    "\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"https://i.postimg.cc/Qx9PMX1Z/shape-of-regularization-penalty.png\" height=\"300\" width=\"380\">\n",
    "<figcaption>Figure 1: Shape of Regularisation Penalty </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Now, lets interpret the geometry of elastic net penalty along with the OLS contours with the help of the figure below.\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"https://i.postimg.cc/1zJMHTwK/Geometry-of-Elastic-Net-Regression.png\" height=\"599\" width=\"688\">\n",
    "<figcaption>Figure 2: Geometry of Elastic Net Regression</figcaption>\n",
    "</figure>\n",
    "\n",
    "As discussed before, the Elastic Net penalty is the convex combination of both Lasso and Ridge penalty (dashed lines) so its shape is a geometry that is neither a complete circle nor a complete diamond. Let's call this an elastic ball. If m=2 and $0$<$\\alpha$<$1$, then the equation\n",
    "$$(1-\\alpha)\\cdot\\sum_{j=1}^{d}{\\beta_j^2}+\\alpha\\cdot\\sum_{j=1}^{d}|\\beta_j|\\leq c$$ forms the elastic ball as shown in the figure. Similarly, the term $(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$ is the OLS solution that generates an ellipse. The centre of the ellipse denotes the point in which the least square error(RSS) is minimum. The ellipse contour plot represents the residual sum of squares (RSS) increasing from inner ellipse to the outer ellipse quadratically. Likewise, the regularization curve is an elastic ball which increases from origin towards the circumference.\n",
    "\n",
    "\n",
    "The goal of this optimization problem is to find the point where the cost function is minimum. We need to find that combination of $\\beta_1$ and $\\beta_2$ where the penalized loss function is minimum. In the figure, the ridge estimate lies at the point of intersection of the ellipse and the circle(red point), the lasso estimate lies at the point of intersection of the ellipse and the diamond(purple point) and the elastic net estimate lies at the point of intersection of the ellipse and the elastic ball (green point).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jp5hClcgKCYo"
   },
   "source": [
    "## Implementation on Real World Dataset\n",
    "\n",
    "For implementation of closed form Ridge Regression Equation we will use the [Boston House Prices Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html).\n",
    "\n",
    "It is one of the datasets provided by sklearn. It has 506 instances with 13 numericals/categorical features of the Boston city. The *medv* variable is the target variable. It is the median value of owner-occupied homes per $1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngPb8ScOb73M"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gt6UhSDAb5za"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "executionInfo": {
     "elapsed": 3386,
     "status": "ok",
     "timestamp": 1596868602468,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "zTdE5gyHcNOa",
    "outputId": "e76c2476-84c0-42f0-aaa7-0ec84c4f2b4a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX  ...    TAX  PTRATIO       B  LSTAT  MEDV\n",
       "0  0.00632  18.0   2.31   0.0  0.538  ...  296.0     15.3  396.90   4.98  24.0\n",
       "1  0.02731   0.0   7.07   0.0  0.469  ...  242.0     17.8  396.90   9.14  21.6\n",
       "2  0.02729   0.0   7.07   0.0  0.469  ...  242.0     17.8  392.83   4.03  34.7\n",
       "3  0.03237   0.0   2.18   0.0  0.458  ...  222.0     18.7  394.63   2.94  33.4\n",
       "4  0.06905   0.0   2.18   0.0  0.458  ...  222.0     18.7  396.90   5.33  36.2\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset.\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "#Load the dataset\n",
    "boston_df=load_boston()\n",
    "\n",
    "#Create dataframe of dataset\n",
    "boston=pd.DataFrame(boston_df.data, columns= boston_df.feature_names)\n",
    "boston['MEDV']=boston_df.target\n",
    "\n",
    "#Print the first five samples\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYBDS2nbcWUI"
   },
   "outputs": [],
   "source": [
    "# We train the model with features other than MEDV as it is the target variable\n",
    "X=boston.drop(columns=['MEDV'])\n",
    "y=boston['MEDV'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMoDJJJrb9_e"
   },
   "source": [
    "We will be first scaling the data to mean value of zero(0) and standard deviation of 1. Then we implement sklearn's [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) and calculate the coefficient values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_B2dzr7gmVH"
   },
   "outputs": [],
   "source": [
    "# We scale the data to mean value of zero and standard deviation of 1.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X=StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9gTP1wjcYSr"
   },
   "outputs": [],
   "source": [
    "# Split the dataset in train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2sXYWPFdHbB"
   },
   "outputs": [],
   "source": [
    "#Implementation of Elastic Net Regression\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#ElasticNet with lambda value of 0.1, Lasso and Ridge ratio of 0.5 each.\n",
    "elastic=ElasticNet(alpha=0.1,l1_ratio=0.5,fit_intercept=True, normalize=False)\n",
    "elastic.fit(X_train,y_train)\n",
    "\n",
    "#Predicting the fitted model in test set.\n",
    "y_pred=elastic.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLqdNkdlx9uz"
   },
   "outputs": [],
   "source": [
    "# Formatting to display 2 decimal places only\n",
    "pd.options.display.float_format = \"{:,.2f}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "executionInfo": {
     "elapsed": 870,
     "status": "ok",
     "timestamp": 1596868695162,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "x_kGa0zlgtNx",
    "outputId": "a134d141-1bcb-4368-9cf8-ac21bd2daf2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  35.68988731868601\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beta value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CRIM</th>\n",
       "      <td>-0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN</th>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS</th>\n",
       "      <td>-0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS</th>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>-1.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM</th>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>-0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS</th>\n",
       "      <td>-2.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD</th>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX</th>\n",
       "      <td>-0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO</th>\n",
       "      <td>-2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT</th>\n",
       "      <td>-3.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Beta value\n",
       "CRIM          -0.77\n",
       "ZN             0.71\n",
       "INDUS         -0.26\n",
       "CHAS           0.63\n",
       "NOX           -1.19\n",
       "RM             2.75\n",
       "AGE           -0.09\n",
       "DIS           -2.06\n",
       "RAD            0.75\n",
       "TAX           -0.78\n",
       "PTRATIO       -2.08\n",
       "B              0.68\n",
       "LSTAT         -3.30"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the mean squared error and coefficient values\n",
    "mean_sq_error = mean_squared_error(y_test, y_pred)\n",
    "print('MSE: ',mean_sq_error)\n",
    "\n",
    "coefficients=elastic.coef_\n",
    "\n",
    "# The beta values corresponding to the column\n",
    "index=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
    "       'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "pd.DataFrame(coefficients,columns=['Beta value'],index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJX54Ob7g3Iw"
   },
   "source": [
    "We obtained the coefficients using Elastic net with equal share of lasso and ridge penalty (i.e we set l1_ratio=0.5). We used the learning rate $\\lambda$=0.1 in our implementation and obtained the mean squared error of 35.68. Elastic Net Regression usually performs better than the Lasso and Ridge Regression because it shrinks coefficients and selects entire groups of highly correlated variables, addressing the limitations of both the regression methods. The coefficient values from Ridge, LASSO and Elastic Net has been visualized in the bar plot below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qykk_1rpS8bW"
   },
   "source": [
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"https://i.postimg.cc/RZ0T0dj6/Comparison-of-coefficients-values-of-Ridge-LASSO-and-Elastic-Net-Regression.png\" height=\"300\" width=\"450\">\n",
    "<figcaption>Figure 3: Comparison of coefficients values of Ridge, LASSO and ElasticNet Regression </figcaption>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVzeusK8zUM2"
   },
   "source": [
    "## Bias Variance TradeOff\n",
    "\n",
    "Remember the concept of bias and variance discussed in the previous course. Let's go through this concept mathematically.\n",
    "\n",
    "Let a linear model $y_i= f(x_i)+ \\epsilon_i$ where $\\mathbb{E($\\epsilon$)}$=0 and ${Var[\\epsilon]}=\\sigma^2$. We will be estimating f by minimizing the loss function $\\hat f$. We will apply $\\hat f$ to new $y$ at $x_0$ to obtain expected MSE equals to\n",
    "\n",
    "$$\\text{MSE}=\\mathbb{E}\\mathbb{[(y-\\hat{f}(x_0))^2]} = [Bias(\\hat{f}(x_0))]^2+Var(\\hat{f}(x_0))+\\sigma^2$$\n",
    "\n",
    "where,\n",
    "$$Bias(\\hat{f}(x_0))=\\mathbb{E}\\mathbb{[\\hat{f}(x_0)]}-f(x_0)$$\n",
    "$$\\qquad Var(\\hat{f}(x_0))=\\mathbb{E}[(\\hat{f}(x_0)-\\mathbb{E}\\mathbb{[\\hat{f}(x_0)])^2]}$$\n",
    "$$\\sigma^2=\\text{Noise}$$\n",
    "\n",
    "Here,\n",
    "$\\mathbb{E[\\hat{f}(x_0)]}$ is the expected value of predicted output for $x_0$.\n",
    "$\\hat{f}(x_0)$ is the predicted output for $x_0$ and $f(x_0)$ is the true output for $x_0$.\n",
    "\n",
    "Bias is the measure of how much a model fails in fitting training data and variance is the measure of how much a model fails in predicting testing data. And Bias Variance Tradeoff is related to finding the right balance of bias and variance which minimizes the error of the model.\n",
    "\n",
    "Remember that OLS is an unbiased estimator, which imples that the OLS estimators are linear, unbiased(low bias) and has least variance in compared to other linear models. But if there is multicollinearity on the dataset(two or more variables are highly correlated) or there are a large number of variable in the dataset, the variance of the OLS estimators is very high as shown in the figure below. So, the model results in poor generalization of testing data.  \n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"https://i.postimg.cc/s2d1LD9j/Bias-Variance-Trade-Off.png\" height=\"356\" width=\"481\">\n",
    "<figcaption>Figure 3: Bias Variance TradeOff</figcaption>\n",
    "</figure>\n",
    "\n",
    "The unbiased OLS lies at the rightmost side of the figure as shown above. With the help of regularisation we reduce the variance with the cost of increasing the bias of the model we the target of moving left towards the optimal model in the figure.\n",
    "\n",
    "From the figure, we have come to know  that the model complexity can be decreased by decreasing the number of features. This can be obtained by reducing the coefficients, $\\beta$'s, towards zero. Since, regularised regression performs the task of penalizing the model with the help of penalty factor $\\lambda$. The greater the value of $\\lambda$, the more are the coefficient values penalized. But, the value of $\\lambda$ should be increased till an extent; it is because the model will lose its important characteristics if $\\lambda$ is too high. Also, high value of $\\lambda$ will increase the bias of the model and it may result in an underfit model. Since the lasso regression penalized coefficients exactly equal to zero, it creates a sparse model, and thus results in a model with lower variance than OLS with the cost of increase in bias.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZO7NGZ0TXEaz"
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Elastic Net Regression is the combination of Lasso and Ridge Regression.\n",
    "- It selects all features of the group containing highly correlated variables unlike Lasso.\n",
    "- It works well with dataset where the number of features is greater than the number of samples.\n",
    "- It usually performs better than Lasso and Ridge regression..\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LZUpeYwzUF1"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
