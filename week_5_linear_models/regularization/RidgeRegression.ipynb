{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLD5dcRTiNwb"
   },
   "source": [
    "# Ridge Regularisation\n",
    "Regularization in machine learning is a technique used to prevent overfitting, where a model learns the training data too well, including its noise and outliers, and performs poorly on new, unseen data. It helps models generalize better by adding a penalty to the loss function, which encourages simpler models with smaller coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhXiCfNzic2a"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "You have already been introduced to the concept of regularisation. Let's recall a bit.\n",
    "$$\n",
    "\\text{New optimization function} = \\text{Error} + \\underbrace{\\lambda*\\text{Model complexity}}_{\\text{Regularization/Penalty}}\n",
    " $$\n",
    "\n",
    "It is a technique that penalizes the polynomial model that uses a higher degree of *n* and prevents the risk of overfitting. There are three methods to perform the regularisation task. They are as follows:\n",
    "1. Ridge Regression\n",
    "2. Lasso Regression\n",
    "3. Elastic Net Regression\n",
    "\n",
    "We will go through each of them one by one. But first, lets understand the need of regularisation.\n",
    "\n",
    "Sometimes your dataset consists of noise (outliers, irrelevant data points) and your model tries to capture it, making the model overfit. The more your model is overfit, the more difficult it is to generalize over unseen data. So, it is important to fit a model that minimizes your loss function, neither too close to zero (the case of overfitting) nor too far from zero (the case of underfitting). Thats where regularisation comes in action.\n",
    "\n",
    "We know OLS method finds the unbiased parameter estimates that best fits the data. It means that OLS treats every independent variables equally i.e it doesn't consider which independent variable is more important than other variables and it just finds the best fit parameters of these variables. So, there is always one set of ${\\boldsymbol\\beta}$ estimates which results in lowest Residual Sum of Squares (RSS). But the question arises, will those estimates always result in the best model? The answer is No, because there may be a possibility of the model overfitting the data or generalizing poorly to unseen data. To solve this problem, we apply Ridge Regression.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kjEb2G-ikHR"
   },
   "source": [
    "## Mathematical Formulation of Ridge Regression\n",
    "\n",
    "As we recalled from OLS, the objective of OLS is to find a column matrix or a column vector, $\\boldsymbol{\\beta}$, such that _Sum of Squared Errors_, $\\text{SSE}$ is minimum. Likewise, the objective of Ridge Regression is to find a column matrix or column vector, $\\boldsymbol{\\beta}$, that minimized the sum of _Residual Sum of Squares_ ($\\text{RSS}$) and penalty equivalent to sum of squared coefficients, which is written as:\n",
    "\n",
    "$$\\text{Cost}_{Ridge}=\\text{RSS} + \\lambda \\sum_{j=1}^{d} \\boldsymbol{\\beta_j^2}= \\sum_{i=1}^{n} (y_i-\\hat{y_i})^2 + \\lambda \\sum_{j=1}^{d} \\boldsymbol{\\beta_j^2}= \\sum_{i=1}^{n}\\epsilon_i^2 + \\lambda \\sum_{j=1}^{d}\\boldsymbol{\\beta_j^2}$$\n",
    "\n",
    "where ${\\lambda}$(lambda) is a tuning parameter that along with the summation of the square of coefficient, ${\\boldsymbol\\beta}$, form the second term of the equation called shrinkage penalty. Remember that we will not be including bias or $\\beta_0$ in penalty term because the model might assign high value to bias in order to compensate the cost. When the parameters ${\\boldsymbol\\beta_1},...,{\\boldsymbol\\beta_d}$ are close to zero, the shrinkage penalty is small. The tuning parameter ${\\lambda}$ controls the contribution of RSS and summation of square of coefficent on the parameter estimates. It can be any value from 0 to $\\infty$.  \n",
    "\n",
    "When ${\\lambda}$ = 0, the penalty term has is removed from the equation (i.e., the model is not penalized at all)  and the ridge regression acts like OLS by generating the least square estimates.\n",
    "\n",
    "When ${\\lambda \\rightarrow \\infty}$, the shrinkage penalty grows and the parameter estimates will approach zero. The model is highly penalized. It results in a simple model which poses the risk of underfitting your data i.e., the model won't learn the data enough to make good prediction.\n",
    "\n",
    "So, finding the optimal value of ${\\lambda}$ is the goal which is discussed at the end. The ideal value of lambda produces a model that generalizes well to the unseen data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Since the parameters are estimates, we usually put _hats_ on them so, the normal equation or the closed form equation to determine estimated parameters is:\n",
    "$$\\boldsymbol{\\beta_{Ridge}}=\\hat{\\boldsymbol{\\beta}} =(\\mathbf{X}^T\\mathbf{X}+\\lambda I)^{-1} \\mathbf{X}^T\\mathbf{y} ......(2)$$\n",
    "\n",
    "We can recall, OLS method assumed that ${\\mathbf{X}^T\\mathbf{X}}$ always exists i.e it is invertible matrix . Here, in equation (1), we can see that a small constant value ${\\lambda}$ is added to the diagonal entities of matrix ${\\mathbf{X}^T\\mathbf{X}}$ before taking its inverse to form equation (2). This $\\lambda$I term is also called ridge. We are adding ridge to the diagonal and basically producing an invertible matrix, ${\\mathbf{X}^T\\mathbf{X}+\\lambda I}$ even if the ${\\mathbf{X}^T\\mathbf{X}}$ is singular. This is also one of the reasons Ridge Regression is preferred over OLS. Due to the addition of ridge in the diagonal term, ridge regression is called 'ridge'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWXSCBOMhISn"
   },
   "source": [
    "## Implementation on Read World Dataset\n",
    "\n",
    "For implementation of closed form Ridge Regression Equation we will use the [Boston House Prices Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html).\n",
    "\n",
    "It is one of the datasets provided by sklearn. It has 506 instances with 13 numericals/categorical features of the Boston city. The *medv* variable is the target variable. It is the median value of owner-occupied homes per $1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZ_ASBHCILqw"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5u9yWe7zgU0l"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "executionInfo": {
     "elapsed": 1955,
     "status": "ok",
     "timestamp": 1603361498376,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "DYfOiIzJIahO",
    "outputId": "5ac24f9d-c47a-4f16-f21a-ac916c10a359"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>18.00</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.54</td>\n",
       "      <td>6.58</td>\n",
       "      <td>65.20</td>\n",
       "      <td>4.09</td>\n",
       "      <td>1.00</td>\n",
       "      <td>296.00</td>\n",
       "      <td>15.30</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>6.42</td>\n",
       "      <td>78.90</td>\n",
       "      <td>4.97</td>\n",
       "      <td>2.00</td>\n",
       "      <td>242.00</td>\n",
       "      <td>17.80</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>7.18</td>\n",
       "      <td>61.10</td>\n",
       "      <td>4.97</td>\n",
       "      <td>2.00</td>\n",
       "      <td>242.00</td>\n",
       "      <td>17.80</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>7.00</td>\n",
       "      <td>45.80</td>\n",
       "      <td>6.06</td>\n",
       "      <td>3.00</td>\n",
       "      <td>222.00</td>\n",
       "      <td>18.70</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>7.15</td>\n",
       "      <td>54.20</td>\n",
       "      <td>6.06</td>\n",
       "      <td>3.00</td>\n",
       "      <td>222.00</td>\n",
       "      <td>18.70</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CRIM    ZN  INDUS  CHAS  NOX   RM  ...  RAD    TAX  PTRATIO      B  LSTAT  MEDV\n",
       "0  0.01 18.00   2.31  0.00 0.54 6.58  ... 1.00 296.00    15.30 396.90   4.98 24.00\n",
       "1  0.03  0.00   7.07  0.00 0.47 6.42  ... 2.00 242.00    17.80 396.90   9.14 21.60\n",
       "2  0.03  0.00   7.07  0.00 0.47 7.18  ... 2.00 242.00    17.80 392.83   4.03 34.70\n",
       "3  0.03  0.00   2.18  0.00 0.46 7.00  ... 3.00 222.00    18.70 394.63   2.94 33.40\n",
       "4  0.07  0.00   2.18  0.00 0.46 7.15  ... 3.00 222.00    18.70 396.90   5.33 36.20\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset.\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "#Load the dataset\n",
    "boston_df=load_boston()\n",
    "\n",
    "#Create dataframe of dataset\n",
    "boston=pd.DataFrame(boston_df.data, columns= boston_df.feature_names)\n",
    "boston['MEDV']=boston_df.target\n",
    "\n",
    "#Print the first five samples\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ux4SxOHHJwUp"
   },
   "source": [
    "From the correlation matrix of the dataset, the features that are more significant in calculating the target variable can be found to be *RM*, *PTRATIO* and *LSTAT*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF82ex1DKh_a"
   },
   "source": [
    "## Implementation of Ridge Regression Closed Form Equation\n",
    "\n",
    "From above mathematical formulation, we derived the closed form equation of Ridge Regression. Here, we will scale the dataset because the model might focus more on feature containing higher values. Then, we will implement the equation to obtain $\\beta$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "offh9MVfJvve"
   },
   "outputs": [],
   "source": [
    "# We train the model with features other than MEDV as it is the target variable\n",
    "X=boston.drop(columns=['MEDV'])\n",
    "y=boston['MEDV'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qPHPhgRNep3"
   },
   "outputs": [],
   "source": [
    "# Feature Scaling should be done before regularization, Otherwise some features will be given more weight than other\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "X=scaler.fit_transform(X)\n",
    "\n",
    "# Bias/Intercept term to 1 for each samples\n",
    "X=np.c_[np.ones((len(X),1)),X]\n",
    "\n",
    "# Implementing closed form equation of Ridge Regression with lambda=0.01\n",
    "lm=0.01\n",
    "betas = np.linalg.inv(X.T.dot(X)+lm*np.identity(len(X.T.dot(X)))).dot(X.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mMqjbgAtgL6"
   },
   "outputs": [],
   "source": [
    "# Formatting to display 2 decimal places only\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "executionInfo": {
     "elapsed": 2667,
     "status": "ok",
     "timestamp": 1603361499189,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "H5866FrtaaLG",
    "outputId": "80a9f18f-ff61-4fe6-91bf-e52355099bff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BIAS</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Beta value</th>\n",
       "      <td>26.54</td>\n",
       "      <td>-9.57</td>\n",
       "      <td>4.63</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.69</td>\n",
       "      <td>-8.60</td>\n",
       "      <td>19.91</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-16.16</td>\n",
       "      <td>7.02</td>\n",
       "      <td>-6.45</td>\n",
       "      <td>-8.94</td>\n",
       "      <td>3.71</td>\n",
       "      <td>-19.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            BIAS  CRIM   ZN  INDUS  CHAS  ...  RAD   TAX  PTRATIO    B  LSTAT\n",
       "Beta value 26.54 -9.57 4.63   0.56  2.69  ... 7.02 -6.45    -8.94 3.71 -19.00\n",
       "\n",
       "[1 rows x 14 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The beta values corresponding to the column\n",
    "index=['BIAS','CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
    "       'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "pd.DataFrame(betas,columns=['Beta value'],index=index).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxO4FOEyikQN"
   },
   "source": [
    "## Geometrical Intrepretation of Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTk-DslHikW0"
   },
   "source": [
    "We learnt that Ridge Regression places some constraint on the parameters, ${\\boldsymbol{\\beta}'s}$. Thus formed ${\\boldsymbol{\\beta_{Ridge}}}$ minimizes the penalized residual sum of squares. With '$n$' number of samples and 'd' number of features. The ridge regression loss function is as follows.\n",
    "\n",
    "$${\\sum_{i=1}^{n} (y_i-\\hat{y_i})^2 + \\lambda \\sum_{j=1}^{d} \\boldsymbol{\\beta_j^2}} = \\sum_{i=1}^{n}(y_i-\\sum_{j=0}^{d}x_{ij}\\boldsymbol{\\beta_j})^2+\\lambda \\sum_{j=1}^{d}\\boldsymbol{\\beta_j^2}$$\n",
    "\n",
    "We are constraining the parameters such that the optimization function gets penalized if $\\boldsymbol{\\beta}$'s take large value. This is equivalent to minimizing the $\\sum_{i=1}^{n}(y_i-\\sum_{j=0}^{d}x_{ij}\\boldsymbol{\\beta_j})^2$ term subject to $\\sum_{j=1}^{d}\\boldsymbol{\\beta_j^2 \\leq c}$ for some constant '$c$' where $c>0$. It can also be viewed as a minimization problem with *argmin* or \"Argument of Minimum\".\n",
    "\n",
    "\n",
    "$$\\underset{\\boldsymbol{\\beta\\in\\mathbb{R}}}{\\arg\\min}\\sum[y_i-\\hat{y_i}]=\\underset{\\boldsymbol{\\beta\\in\\mathbb{R}}}{\\arg\\min}\\sum[y_i-(\\beta_{i0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} + \\cdots +\\beta_{d}x_{id})]$$\n",
    "\n",
    "*argmin* finds the coefficients that minimizes the $\\text{RSS}$. The ridge constraint is $L_2$ vector norm and its equation is:\n",
    "\n",
    "$$||\\beta||_2^2=\\sqrt{\\beta_{0}^2 + \\beta_{1}^2 + \\beta_{2}^2 + \\cdots +\\beta_{d}^2}$$\n",
    "\n",
    "The ridge loss function with $argmin$ and vector norm is as shown below. Notice that the first term of the equation is OLS loss function and second term is ridge penalty.\n",
    "\n",
    "$$\\boldsymbol{\\beta_{ridge}} = \\underset{\\boldsymbol{\\beta\\in\\mathbb{R}}}{\\arg\\min}||y-X\\beta||_2^2+\\lambda||\\beta||_2^2$$\n",
    "\n",
    "Ridge Regression is also called L2 Regularization as it uses $L_2$ penalty i.e squared of L2 norm of the parameter estimates. The $L_2$ norm is convex and derivative can be obtained at each point of the curve. The concept of $L_2$ norm is used in its geometrical visualization in forming the $L_2$ norm circle as shown in the figure below. As discussed above, the objective function of ridge regression is $(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})+ \\lambda \\boldsymbol{\\beta^T}\\boldsymbol{\\beta}$\n",
    "and it is a convex optimization problem.\n",
    "\n",
    "\n",
    "<figure align=\"center\">\n",
    "\n",
    "<img src=\"https://i.postimg.cc/zvpCkH9P/Geometrical-Intrepretation-of-Ridge-Regression.png\">\n",
    "<figcaption>Figure 1: Geometrical Intrepretation of Ridge Regression </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "If $d=2$, then the equation $$\\boldsymbol{\\sum_{j=1}^{d}}\\boldsymbol{\\beta_j^2 \\leq c}$$ equals $\\boldsymbol{\\beta_1^2+\\beta_2^2 \\leq c}$, which is the equation of circle as shown in the figure.\n",
    "Similarly, the term $(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$ is the OLS solution that generates an ellipse. The centre of the ellipse denotes the point in which the least square error(RSS) is minimum. The ellipse contour plot represents the residual sum of squares (RSS) increasing from inner ellipse to the outer ellipse quadratically. Likewise, the regularization curve is a circle which increases quadratically from origin towards the circumference.  \n",
    "\n",
    "The goal of this optimization problem is to find the point where the cost function is minimum. When $\\lambda=0$, the $L_2$ norm circle passes through the OLS estimate point where $\\beta_1$ and $\\beta_2$ gives the least RSS value. But, the parameter chosen by OLS is prone to overfitting. So, we need to find that combination of $\\beta_1$ and $\\beta_2$ where the penalized loss function is minimum. For a particular lambda, we try to find the minimum point that L2 intersects with OLS. This minimum point is called ridge estimate point and the $\\lambda$ at this point is the optimal value. This point is the intersection of the ellipse and the circle as shown in the figure. But a question arises, Is the value of $\\lambda$ at ridge estimate the optimal value of $\\lambda$? The answer is maybe yes, maybe no, because the circle changes with different value of $\\lambda$ and it may first intersect the ellipse at different point. Likewise, this point of intersection can fall anywhere in the circumference of the circle unlike L1 regularisation, which will be discussed in the next chapter. Here, we visualized an example with two variables so the ridge constraint is a circle. To visualize an example containing three variables, we need a three dimesional structure (i.e. a sphere) and hypersphere for more dimensions.\n",
    "\n",
    "Ridge Regression is unable to reduce to coefficients exactly equal to zero. So, Ridge cannot be used for feature selection process which will be discussed in next chapter. Ridge Regression is better than Lasso Regression by its ability to select groups of collinear features in a dataset containing large number of features. This capability of Ridge Regression is called \"grouping effect\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MVDLYVCg0Rc"
   },
   "source": [
    "## Ridge Regression with Gradient Descent\n",
    "\n",
    "We have learnt how Gradient Descent works with Linear Regression in the previous chapters. Here, we will implement Gradient Descent with Ridge Regression. Lets calculate the number of samples(rows) and the number of features(columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 2606,
     "status": "ok",
     "timestamp": 1603361499190,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "4wxBHyTVgF6d",
    "outputId": "b3e69fef-a4ec-4291-97e1-fc9bca441400"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of samples: 506 and No of features: 14\n"
     ]
    }
   ],
   "source": [
    "X = X\n",
    "\n",
    "n = X.shape[0] # number of samples (rows)\n",
    "d = X.shape[1] # number of features (columns)\n",
    "print('No of samples:',n,'and No of features:',d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOny2lyMVva5"
   },
   "source": [
    "### Random Initialization\n",
    "\n",
    "Let's initialize the values of parameter $\\beta$ randomly. The function initialize_beta uses the [numpy.random.randn](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randn.html) function to initialize the parameters using the random values sampled from standard normal distribution. It returns an array of the shape  d√ó1  (where  d  = no. of features) containing the initial values of the parameters. In our case  d=14  (including the ones column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 2557,
     "status": "ok",
     "timestamp": 1603361499191,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "LstXmiVNjwNV",
    "outputId": "57dc8351-9714-45e0-a98c-ec9b494e7e34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 1)\n"
     ]
    }
   ],
   "source": [
    "def initialize_betas(X, y):\n",
    "  np.random.seed(0)\n",
    "  betas = np.random.randn(d,1)\n",
    "  return betas\n",
    "\n",
    "betas = initialize_betas(X, y)\n",
    "print(betas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3ByZnLHkRIS"
   },
   "source": [
    "Here, the initial values for our parameters are:\n",
    "\n",
    "$$\\boldsymbol{\\beta} =\\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \\\\\n",
    "\\beta_3 \\\\\n",
    "\\beta_4 \\\\\n",
    "\\beta_5 \\\\\n",
    "\\beta_6 \\\\\n",
    "\\beta_7 \\\\\n",
    "\\beta_8 \\\\\n",
    "\\beta_9 \\\\\n",
    "\\beta_{10}\\\\\n",
    "\\beta_{11}\\\\\n",
    "\\beta_{12}\\\\\n",
    "\\beta_{13}\\\\\n",
    "\\end{bmatrix} =   \\begin{bmatrix}\n",
    "  1.7640\\\\\n",
    "  0.4001\\\\\n",
    "  0.9787\\\\\n",
    "  2.2408\\\\\n",
    "  1.8675\\\\\n",
    " -0.9772\\\\\n",
    "  0.9500\\\\\n",
    " -0.1513\\\\\n",
    " -0.1032\\\\\n",
    "  0.4105\\\\\n",
    "  0.1440\\\\\n",
    "  1.4542\\\\\n",
    "  0.7610\\\\\n",
    "  0.1216\\\\\n",
    " \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNDuyNR8lV98"
   },
   "source": [
    "### Cost Function\n",
    "\n",
    "Like Linear Regression, here we will be minimizing the cost function of ridge regression. The cost function $J$ is multiplied by $\\frac{1}{2}$ to make the derivation easier and divided by n to average the sum of parameters . You should know that multiplying the cost function with $\\frac{1}{2}$ only changes the value of the cost function but not the optimal parameters that minimize it.\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\beta_0, \\beta_1, \\beta_2,..., \\beta_{13}) &= \\frac{1}{2n}\\sum_{i=1}^{n}({y_{i}}-\\hat{y_{i}})^2 + \\frac{\\lambda}{2n} \\sum_{j=1}^{d} {\\beta_j^2} \\\\\n",
    "&=  \\frac{1}{2}\\sum_{i=1}^{n}(y_{i}-(\\beta_0x_{i0}+\\beta_1x_{i1} +\\beta_2x_{i2} + ,...,+\\beta_3x_{id}))^2+ \\frac{\\lambda}{2n} \\sum_{j=1}^{d} {\\beta_j^2}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The cost function can be written in matrix form as:\n",
    "\n",
    "$$J(\\boldsymbol{\\beta}) = \\frac{1}{2n}\\ \\sum(\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{y})^2+\\frac{\\lambda}{2n}\\ \\sum\\boldsymbol{\\beta^2}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Note: We call $J$ as a function of only parameters $\\boldsymbol{\\beta}$ but not of $X$ and $y$ because $X$ and $y$ are constants given by the dataset. So the value of $J$ depends only on the parameters.\n",
    "\n",
    "We want to find the parameters $\\beta$ that minimizes the cost function $J$ using Gradient Descent. Here we will use:\n",
    "\\begin{gather*}\n",
    "\\lambda=0.01 \\\\\n",
    "\\alpha=0.01 \\\\\n",
    "\\text{number of iterations}= 5000\n",
    "\\end{gather*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 2530,
     "status": "ok",
     "timestamp": 1603361499193,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "92hlcgspskYI",
    "outputId": "038e7802-98a5-4a19-ab95-59fa18299460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost of ridge regression with random betas:  201.5767128072047\n"
     ]
    }
   ],
   "source": [
    "def calculate_cost(betas):\n",
    "  cost=1/(2*n) * np.sum(np.square(y-np.dot(X, betas)))+ (lm/(2*n))*(np.sum(np.square(betas)))\n",
    "  return cost\n",
    "\n",
    "print('Cost of ridge regression with random betas: ', calculate_cost(betas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYp2CglEpOx1"
   },
   "source": [
    "### Gradients\n",
    "\n",
    "You will need to calculate the gradient of the cost function with respect to each of the parameters.\n",
    "\n",
    "Partial derivative(gradient) of the cost function with respect to $\\beta_1$,\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial \\beta_1} &= \\frac{\\partial}{\\partial \\beta_1}\\left( \\frac{1}{2n}\\sum_{i=1}^{n}({y_{i}}-\\hat{y_{i}})^2 + \\frac{\\lambda}{2n} \\sum_{j=1}^{d} {\\beta_j^2}\\right)\\\\\n",
    "&=\\frac{1}{2n}\\ \\sum_{i=1}^{n}\\frac{\\partial}{\\partial \\beta_1}({y_{i}}-\\hat{y_{i}})^2+ \\frac{\\lambda}{2n} \\sum_{j=1}^{d} \\frac{\\partial}{\\partial \\beta_1} {\\beta_j^2}\n",
    "\\end{align*}\n",
    "\n",
    "$\\hspace{5cm}$ Applying chain rule,\n",
    "\n",
    "\\begin{align*}\n",
    "\\hspace{5cm}&=\\frac{1}{2n}\\ \\sum_{i=1}^{n}\\frac{\\partial ({y_{i}}-\\hat{y_{i}})^2}{\\partial ({y_{i}}-\\hat{y_{i}})} \\times \\frac{\\partial ({y_{i}}-\\hat{y_{i}})}{\\partial \\beta_1}+\\frac{\\lambda}{2n}\\frac{\\partial (\\beta_0^2 + \\beta_1^2 + \\beta_2^2 +...+\\beta_{d}^2)}{\\partial \\beta_1}\\\\\n",
    "&=\\frac{1}{n}\\sum_{i=1}^{n}({y_i}-\\hat{y_i}) \\times \\frac{\\partial (y_i-(\\beta_0x_{i0} + \\beta_1x_{i1} + ...+ \\beta_3x_{id}))}{\\partial \\beta_1}+\\frac{\\lambda}{n}\\times\\beta_1\\\\\n",
    "&=\\frac{1}{n}\\sum_{i=1}^{n}({y_{i}}-\\hat{y_{i}}) \\times -x_{i1}+\\frac{\\lambda}{n}\\beta_1\\\\\n",
    "\\therefore \\frac{\\partial J}{\\partial \\beta_1}&=\\frac{1}{n}\\sum_{i=1}^{n}({\\hat{y_{i}}}-{y_{i}})\\times x_{i1}+ \\frac{\\lambda}{n}\\beta_1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_fApPbHCPhB"
   },
   "source": [
    "$\\hspace{8cm}$Applying chain rule,\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial \\beta_0}&=\\frac{1}{n}\\sum_{i=1}^{n}({y_{i}}-\\hat{y_{i}})\\times-x_{i0}+ \\frac{\\lambda}{n}\\beta_0\\\\\n",
    "&=\\frac{1}{n}\\sum_{i=1}^{n}({y_{i}}-\\hat{y_{i}})\\times-1+ \\frac{\\lambda}{n}\\beta_0\\\\\n",
    "&=\\frac{1}{n}\\sum_{i=1}^{n}({\\hat{y_{i}}}-{y_{i}})+ \\frac{\\lambda}{n}\\beta_0\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial \\beta_2}=\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y_{i}}-{y_{i}})x_{i2}+\\frac{\\lambda}{n}\\beta_2\\\\\n",
    "\\frac{\\partial J}{\\partial \\beta_3}=\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y_{i}}-{y_{i}})x_{i3}+\\frac{\\lambda}{n}\\beta_3\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1cwzbigE0Uh"
   },
   "source": [
    "In general, the formula for calculating the gradients with respect to a parameter $\\beta_j$ can be expressed as:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\beta_j}=\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y_i}-{y_i})x_{ij}+ \\frac{\\lambda}{n}\\beta_j$$\n",
    "\n",
    "\n",
    "\n",
    "We can write this generalized expression in matrix form to calculate the gradients wrt. all the parameters simultaneously as:\n",
    "\n",
    "$$\\frac{{\\partial J}}{{\\partial \\beta}}= {\\frac{1}{n}}\\mathbf{X^T}(\\mathbf{X\\boldsymbol{\\beta}-y})+\\frac{{\\lambda}}{{n}}{\\boldsymbol{\\beta}} = \\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial \\beta_0} \\\\\n",
    "\\frac{\\partial J}{\\partial \\beta_1}\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "\\frac{\\partial J}{\\partial \\beta_{d}}\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_ie4SV4LVma"
   },
   "source": [
    "The calculate_gradient function below calculates the gradients of cost function with respect to the parameters betas. It uses the matrix operations to compute the gradient of all the parameters simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 2496,
     "status": "ok",
     "timestamp": 1603361499195,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "VTnc0gYKLRN2",
    "outputId": "83180f54-b3e7-48da-c8f3-b0b74b00f90d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for random betas = \n",
      " [[-17.76799424]\n",
      " [ -0.35396529]\n",
      " [ -2.8144031 ]\n",
      " [ -5.72414928]\n",
      " [ -1.52344185]\n",
      " [ -5.21639823]\n",
      " [-10.14089749]\n",
      " [-10.95493409]\n",
      " [ -4.79003558]\n",
      " [ -5.13020129]\n",
      " [ -5.98593094]\n",
      " [ -9.90502901]\n",
      " [-16.67594573]\n",
      " [ -3.98356663]]\n"
     ]
    }
   ],
   "source": [
    "lm=0.01\n",
    "def calculate_gradients(betas,lm):\n",
    "  gradients=((1/n)*np.dot(X.T,(np.dot(X,betas)-y))+(lm/n)*betas)\n",
    "  return gradients\n",
    "\n",
    "print(\"Gradients for random betas = \\n\", calculate_gradients(betas,lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCmrWTVsMyLv"
   },
   "source": [
    "Here, the gradients of the cost function with respect to the initial parameters are:\n",
    "\n",
    "$$\\boldsymbol{\\frac{\\partial J}{\\partial \\beta}}  =\\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial \\beta_0} \\\\\n",
    "\\frac{\\partial J}{\\partial \\beta_1}\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "\\frac{\\partial J}{\\partial \\beta_{13}}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "-17.767\\\\\n",
    "-0.353\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "-3.983\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFByC-7zNawn"
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Now you need to update the parameters using their respective gradients until the cost function converges to its minimum value.\n",
    "\n",
    "\n",
    "\n",
    "${\\hspace{5cm}}\\text{Repeat until convergence }\\{$\n",
    "\n",
    "$$\\beta_0 :=\\beta_0-\\alpha\\frac{\\partial J}{\\partial \\beta_0}$$\n",
    "\n",
    "$$\\beta_1 :=\\beta_1-\\alpha\\frac{\\partial J}{\\partial \\beta_1}$$\n",
    "\n",
    "$$.$$\n",
    "$$.$$\n",
    "$$.$$\n",
    "\n",
    "$$\\beta_{d} :=\\beta_{d}-\\alpha\\frac{\\partial J}{\\partial \\beta_{d}}$$\n",
    "\n",
    "${\\hspace{8cm}}\\}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Since you already have a vector $\\beta$ called `beta` containing parameters and an another vector $\\frac{\\partial J}{\\partial \\beta}$ called `gradients` containing the gradients of cost function with respect to the parameters, this updation is a simple matrix operation:\n",
    "\n",
    "$$\\boldsymbol{\\beta_j} := \\boldsymbol{\\beta_j} - {\\alpha}\\boldsymbol{\\frac{\\partial J}{\\partial \\beta_j}}$$\n",
    "\n",
    "The gradient_descent function below applies the gradient descent algorithm to find the optimal parameters betas that minimize the cost function. Initially betas contain the random initial values of the parameters. It uses the gradients calculated by the calculate_gradients function to update the values of the parameters. We have used the learning rate of 0.01. The process of updating the parameters is repeated till the number of iterations(5000 here). Also the list costs contains the values of cost functions for different values of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJYZUm8XO1pE"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X,y):\n",
    "  np.random.seed(0)\n",
    "  no_of_iterations=5000\n",
    "  alpha=0.01\n",
    "  train_error=[]\n",
    "  betas=np.random.rand(d,1)\n",
    "\n",
    "  for i in range(1, no_of_iterations):\n",
    "    train_cost = calculate_cost(betas)\n",
    "    train_error.append(train_cost)\n",
    "\n",
    "    betas = betas - alpha * calculate_gradients(betas,lm)\n",
    "\n",
    "  return betas, train_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6nEI8_2Tm-V"
   },
   "source": [
    "Let's use the *gradient_descent*  function defined above to find the cost from gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "elapsed": 2455,
     "status": "ok",
     "timestamp": 1603361499202,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "fkWOchTCTlYz",
    "outputId": "6f66db93-b022-49e4-dade-c5450a5f24bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beta value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BIAS</th>\n",
       "      <td>13.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM</th>\n",
       "      <td>-2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN</th>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS</th>\n",
       "      <td>-1.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS</th>\n",
       "      <td>3.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>-1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM</th>\n",
       "      <td>21.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>1.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS</th>\n",
       "      <td>-2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD</th>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX</th>\n",
       "      <td>-2.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO</th>\n",
       "      <td>-6.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>7.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT</th>\n",
       "      <td>-14.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Beta value\n",
       "BIAS          13.33\n",
       "CRIM          -2.08\n",
       "ZN             3.61\n",
       "INDUS         -1.30\n",
       "CHAS           3.92\n",
       "NOX           -1.11\n",
       "RM            21.05\n",
       "AGE            1.74\n",
       "DIS           -2.43\n",
       "RAD            2.52\n",
       "TAX           -2.64\n",
       "PTRATIO       -6.64\n",
       "B              7.29\n",
       "LSTAT        -14.49"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas, train_error=gradient_descent(X,y)\n",
    "pd.DataFrame(betas,columns=['Beta value'],index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zl6U5HYST5LT"
   },
   "source": [
    "If we plot the ridge cost function *J* against the number of iteration, we get a plot as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "executionInfo": {
     "elapsed": 2425,
     "status": "ok",
     "timestamp": 1603361499205,
     "user": {
      "displayName": "Jasmin Karki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg3nFw3n9oncuMCv-Ekq2P2jY9FtYee9B2a_5I=s64",
      "userId": "13981372361842258837"
     },
     "user_tz": -345
    },
    "id": "Obhnz0tzQyTB",
    "outputId": "5e985c2c-8bc8-472a-a125-709f9e709155"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 12.92\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_5244501e-bbc0-40e8-8317-e80690f51544\", \"ridge_loss.png\", 23278)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Training Error: {0:.2f}'.format(train_error[-1]))\n",
    "plt.plot(np.arange(len(train_error)),train_error)\n",
    "plt.title('Cost Function Curve with Number of Iterations')\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Training Error\")\n",
    "plt.ylim(0, 200)\n",
    "\n",
    "from google.colab import files\n",
    "plt.savefig(\"ridge_loss.png\", dpi=100)\n",
    "\n",
    "plt.show()\n",
    "files.download(\"ridge_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6GDPRu2Ut7q"
   },
   "source": [
    "We implemented the gradient descent and found cost equals 12.92 for $\\lambda$=0.01, $\\alpha$=0.01 and 5000 iterations. We would get different cost for different set of $\\lambda$, $\\alpha$ and number of iterations. The optimal value for number of iterations, $\\lambda$ and $\\alpha$ can be found through hyper parameter tuning. We can see the effect of ridge regression on coefficient values. We can see that on using Ridge regression, the coefficients of features are reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56c8B9tetVH5"
   },
   "source": [
    "\n",
    "<img src=\"https://i.postimg.cc/zGDJMb4F/ols-vs-ridge-coefficients.png\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZD2gLH9MiIPR"
   },
   "source": [
    "### Choice of $\\lambda$\n",
    "\n",
    "Finding the optimal value of $\\lambda$ is considered an art than a science. The different methods that can be applied to find the value of $\\lambda$ hyperparameter is:\n",
    "1. Use k-fold cross validation to select the $\\lambda$ that gives the minimum error on cross validation dataset.\n",
    "2. Manually run the model with different values of $\\lambda$ and choose the value that gives the least error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTMkIkQ-kibp"
   },
   "source": [
    "## Takeaways\n",
    "1. Ridge Regression penalizes higher coefficients more due to the square of the coefficient values in penalty term.\n",
    "2. $L_2$ norm is Circle shaped in 2D space and Sphere shaped in 3D space.\n",
    "3. It performs coefficient shrinkage by reducing the coefficients of unknown features towards zero.\n",
    "4. It is capable of performing grouping effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJDokSuCd2A9"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JC0yDAD81XmD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ai_ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
